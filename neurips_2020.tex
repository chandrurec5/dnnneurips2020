\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[preprint]{neurips_2020}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bbm}
\input{pack.tex}
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage{caption}
\captionsetup{belowskip=0pt}
\usepackage{wrapfig,lipsum}
\usepackage[linewidth=1.2pt,linecolor=red]{mdframed}


\usepackage{comment}
\usepackage{cancel}
\usepackage{changepage}
\usepackage{bbm}
\usepackage{pgfplots}
\usepackage{filecontents}
\setcounter{MaxMatrixCols}{32}
\usepackage{placeins}


\title{Deep Learning: A study in paths and gates}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{Chandrashekar Lakshminarayanan and Amit Vikram Singh, \\ Indian Institute of Technology, Palakkad}

\begin{document}

\maketitle
\begin{abstract}
We study optimisation and generalisation in deep neural networks (DNNs) with ReLU activations. Central idea in the paper is to regard paths and gates as basic building blocks of a deep neural network. At time $t$, for an input $x\in\R^{d_{in}}$ to the DNN, we define a novel \emph{neural path feature} $\phi_{x,t}$ whose dimension is equal to total the number of paths, and whose co-ordinate corresponding to path is the product of the signal at the input node and the activity\footnote{A path is active if all the gates in the path are active.} of the path. We show that the associated \emph{neural path kernel} $H_t(x,x')=\phi^\top_{x,t}\phi_{x',t}$, plays an important role in optimisation and generalisation. In particular, $H_t(x,x')=(x^\top x')\lambda_t(x,x')$, where $\lambda_t(x,x')$ is a count of total number of paths that are active simultaneously for both inputs $x,x'\in \R^{d_{in}}$. 

When the activations and as a consequence the neural path features are frozen (i.e. do not change over time), and under symmetric Bernoulli initialisation, we show (in theory and experiments) that i) increasing depth till a point helps in training and ii) increasing depth beyond hurts training. We verify via experiments that the NPFs and NPK are learnt during training by showing that the norm of the labelling function measured with respect to the inverse of the trace normalised NPK reduces with time.

\end{abstract}
\section{Introduction}
Understanding optimisation and generalisation of deep neural networks (DNNs) trained using first-order method such as (stochastic) gradient descent is an important problem in machine learning. In this paper, we throw light on the following two questions:%Despite the fact that the optimisation of DNNs is a non-convex problem in general, simple (stochastic) gradient descent (SGD) type procedures are able to achieve zero training error when the DNN is sufficiently over-parameterised and when the parameters are initialised at random. Further, it has also been empirically observed that, practical DNNs can achieve zero training error on standard datasets even when the labels are flipped at random. 
\begin{comment}
Recent works based on the \emph{trajectory} analysis \cite{} have shown that, sufficiently over-parameterised DNNs can be optimised to achieve zero training error by a randomly initialised SGD procedure. The gist of the \emph{trajectory} based analysis is the following: consider the dataset given by $(x_s,y_s)_{i=1}^n\in \R^{d_{in}\times} \R$, and for an input $x_s\in \R^{d_{in}},i\in[n]$\footnote{We denote the set $\{1,\ldots, n\}$ by $[n]$.}, let $\hat{y}_{\Theta_t}(x_s)$ be predicted output of the DNN whose parameters/tunable weights at time $t$ is $\Theta_t\in \R^{d_{net}}$. Say one is interested in minimising the squared loss given by $L_{\Theta_t}=\frac{1}{2}\sum_{i=1}^n\left(\hat{y}_{\Theta_t}(x_s)-y_s\right)^2$ by a SGD procedure, then the idea behind the trajectory analysis is to look at the dynamics of the error term defined as $e_t(i)\stackrel{def}{=}\hat{y}_{\Theta_t}(x_s) -y_s$. Denoting $e_t=(e_t(i),i\in[n])\in \R^n$, one can study the following error recursion:
\begin{align}\label{eq:trajecbasic}
e_{t+1}=e_t-\alpha_t K_t e_t,
\end{align}
where $\alpha_t>0$ is a small stepsize, $K_t\in \R^{n\times n}$ is a Gram matrix. This gram matrix is in turn expressed as $K_t=\Psi_t^\top \Psi_t$, wherein, $\Psi_t$ is a $d_{net}\times n$ matrix known as the \emph{neural tangent feature} (NTF) matrix, which is the collection of the gradient of the network output with respect to the network parameters, and whose $s^{th}$ column is given by $\Psi_t(s)=(\frac{\partial \hat{y}_{\Theta_t}(x_s)}{\partial \theta},\theta \in \Theta)$.
\end{comment}

\textbf{Question I (Optimisation):} \emph{What is the role of depth in training of DNNs? Why increasing depth till a point helps in training? Why increasing the depth beyond hurts training?}\\
\begin{comment}We call the above  questions are the depth phenomena. \cite{dudnn} show that, when it comes to training, residual networks are better than simple FC-DNNs. However, the depth phenomena in the case of simple FC-DNNs is still unresolved.
\end{comment}
\textbf{Question II (Generalisation):} \emph{What are the hidden features in a DNN? Are these features learned? and if so, how?}
\begin{comment}The general consensus is that, the DNNs learn hidden representations progressively in each of the intermediate layers, and the final layer learns a linear model using features obtained in the penultimate layer.  This view, while conceptually simple, however, does not provide us any analytical insight regarding the above question.  A more analytically appealing candidate for the hidden representation (used in some of the recent works \cite{}) is the \emph{neural tangent random feature} (NTRF) which is the NTF evaluated at randomised initialisation of an infinitely wide DNN. \cite{} provides generalisation bounds in terms of the Rademacher complexity of the class of functions defined by the NTRF and also in terms of an associated neural tangent kernel (NTK). \cite{} uses NTK to set a significant new benchmark for pure-kernel based learning.  An issue with the NTRF/NTK approach is that the features do not change over the training of the DNN, thus implying no feature learning is happening, and yet experimental evidence (in \cite{} as well as \Cref{sec:generalisation-exp}) shows that DNNs perform significantly better than pure-kernel learning with NTK.
\end{comment}

The results in this paper fit within the the framework of the trajectory method, used in recent works \cite{} to show that SGD achieves zero training error in over-parameterised DNNs. In particular, the error trajectory can be given by $e_{t+1}=e_t-\alpha_tK_te_t$, where $e_t\in \R^n$ is the error in the DNN prediction (i.e., difference between predicted and true values), $\alpha_t$ is a small stepsize (of the SGD) and $K_t\in\R^{n\times n}$ is a Gram matrix. 


\textbf{Highlights Of Our Contributions:}
%In this paper, we are interested in fully connected (FC) deep gated networks (DGN), wherein, the output of a neuron can be expressed as a product of its pre-activation input and its gating value. DNNs with ReLU activations (in this paper, we refer to them as simply DNNs) are special cases of DGNs. 
%We consider networks with $d$ layers, and $w$ hidden neurons per layer.
 %We now describe in brief the conceptual novelties (CN), the corresponding conceptual gains/insights (CG) and the key results in the paper.

$\bullet$ \textbf{Paths:}  A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Let $[P]=\{1,\ldots,P\}$ be an enumerate of all the paths, the zeroth and first order quantities of a DNN can then be expressed as:
\begin{align*}
%\begin{split}
\text{(Zeroth Order)}\quad: \quad \hat{y}_t(x)&=\sum_{p\in [P]}x(\I(p))A_t(x,p)v_t(p)={\phi^\top_{x,t}} v_t\\
\text{(First Order)}\quad: \quad \partial \hat{y}_t(x)&=\sum_{p\in [P]}x(\I(p)) \partial(A_t(x,p)) v_t(p)+ \sum_{p\in [P]}x(\I(p)) A_t(x,p) \partial((v_t(p))\\
&=\phi^\top_{x_s,t} {\partial} v_t + {\partial} \phi^\top_{x_s,t} v_t,
%\end{split}
\end{align*}
where $\I(p)$ is the input node at which a path $p$ starts, $v_t(p)$ is its value given by the product of its weights and $A_t(x,p)$ is its activation level given by the product of the gating values in the path.

$\bullet$ \textbf{Path Feature and Kernel:}  $\phi_{x,t}=\left( x(\I(p))A_t(x,p),p\in[P]\right)\in \R^P$ is the \emph{neural path feature} (NPF), and the \emph{neural path kernel} (NPK) given by $H_t(x,x')=\phi^\top_{x,t}\phi_{x',t}$. We show that the NPK has a special structure, in that, $H_t(x,x')=(x^\top x')\lambda_t(x,x')$, where $\lambda_t(x,x')$ is a measure of similarity based on the path activation levels for inputs $x,x'\in\R^{d_{in}}$. %For instance, in the case of DNNs with ReLU, $\lambda_t(s,s')$ is a measure of the sub-networks that are simultaneously \emph{on} for both the inputs, capturing the intuition that if $s$ and $s'$ are similar, then their corresponding active sub-networks might also be similar. 

$\bullet$ \textbf{Optimisation:} We note that, in $\phi_{x,t}/H_t$ only $A_t/\lambda_t$ changes during training. %We first study the frozen gating regime, wherein, the activations and hence $\lambda_t$ do not change with time.
We first study \emph{gated linear unit} (GaLU) networks with frozen gates, wherein, the activations $A_t(\cdot,p)=A_0(\cdot,p),\forall t\geq 0,p\in[P]$ do not change with time. In GaLU networks, the gating values for each input are copied from a different network called gating network (whose weights are held constant during training). Thus the NPF as well as the NPK do not change with time, i.e., $\phi_{\cdot,t}=\phi_{\cdot,0}, H_t=H_0,\forall t\geq 0$ . 
For GaLU networks, when the weights are sampled $\stackrel{iid}\sim Ber(\frac{1}{2})$ over the set $\{-\sigma,+\sigma\}$, we have the following results that throw light on Question I: 

$1.$ $\E{K_0(s,s')}=d\sigma^{2(d-1)}H_0(s,s')=d\sigma^{2(d-1)}(x_s^\top x_{s'})\lambda_0(s,s')$. We argue that the non-diagonal entries of $\lambda_0$ decay at an exponential rate (in comparison with the diagonal entries), i.e., the Gram matrix whitens with depth.
%The ratio $\frac{{\lambda_0}(s,s')}{{\lambda_0}(s,s)}$ is the fractional overlap of active sub-networks, say at each layer the overlap of active gates is $\mu\in (0,1)$, then for a depth $d$, the fractional overlap decays at exponential rate, i.e., $\frac{{\lambda_0}(s,s')}{{\lambda_0}(s,s)}\leq \mu^d$, leading to whitening. Thus increasing depth till a point helps in training.

$2.$ $Var\left[K_0(s,s')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$ (for $\sigma^2=O(\frac{1}w)$). For a fixed $d$, increasing $w$ ensures $K_0$ converges to $\E{K_0}$. However, for a fixed $w$, increasing $d$ makes the entries of $K_0$ deviate from $\E{K_0}$, thus degrading the spectrum of $K_0$. Thus increasing depth beyond a point hurts training performance.

\begin{comment}

\textbf{ReLU artefact or NTF vs NPF:} Note that the left-hand side of \eqref{eq:npfgrad} is the neural tangent feature. We note that, in the case when $A_t(x,p)\in\{0,1\}$ (such as in DNN with ReLU activations)  $\partial_{\phi}=0$, and hence is not accounted for in the SGD update as well as the analysis. However, due to the SGD update, the gating value changes during training, and consequently the activations, the NPF, the NPK change during training (see the experiments in \Cref{sec:generalisation}). We believe the change of activations is key for generalisation, a fact which can explain the gap in generalisation performance \cite{} between the DNNs and the pure-kernel methods based on their static NTK counterparts.
\end{comment}
\begin{comment}
$\bullet$ \textbf{Separation of Gates:} In order to better understand the role of $A_t(\cdot,\cdot)$ and $\partial_{\phi}$ in the optimisation and generalisation of DNNs, we handle the gating values as independent variables. This way we consider networks with i) fixed random gates, wherein, the gating values for the $n$ input examples are generated at random at $t=0$, and held fixed throughout training, ii) gated linear units (GaLU), wherein, the gates are copied from a different network (called gating network) with identical architecture, and iii) soft-ReLU gates, wherein, the hard-max in the ReLU is replaced by a soft-max sigmoidal function.
%In our theory as well as experiments, we study the following useful gating regimes (GR): GR-(i)(\emph{frozen}), wherein, the activations $A_t(\cdot,\cdot)$ do not change with time, GR-(ii) (\emph{decoupled}), wherein, $A_t(\cdot,\cdot)$ is derived from a separate \emph{gating} network parameterised by $\Tg\in \R^{d_{net}}$,  GR-(iii) (\emph{adaptable}), wherein, $A_t(\cdot,\cdot)$ changes during training (this is the case of the standard DNNs with ReLU gates), and GR-(iv) (\emph{soft}), wherein, the hard-max indicator function of the ReLU is replaced by a soft-max sigmoidal function.

$\bullet$ \textbf{Optimisation (Key Results I):} For GaLU networks, when the weights are sampled $\stackrel{iid}\sim Ber(\frac{1}{2})$ over the set $\{-\sigma,+\sigma\}$, we have the following results that throw light on Question I: 

$1.$ $\E{K_0(s,s')}=d\sigma^{2(d-1)}H_0(s,s')=d\sigma^{2(d-1)}(x_s^\top x_{s'})\lambda_0(s,s')$. We argue that the non-diagonal entries of $\lambda_0$ decay at an exponential rate (in comparison with the diagonal entries), i.e., the Gram matrix whitens with depth.
%The ratio $\frac{{\lambda_0}(s,s')}{{\lambda_0}(s,s)}$ is the fractional overlap of active sub-networks, say at each layer the overlap of active gates is $\mu\in (0,1)$, then for a depth $d$, the fractional overlap decays at exponential rate, i.e., $\frac{{\lambda_0}(s,s')}{{\lambda_0}(s,s)}\leq \mu^d$, leading to whitening. Thus increasing depth till a point helps in training.

$2.$ $Var\left[K_0(s,s')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$ (for $\sigma^2=O(\frac{1}w)$). For a fixed $d$, increasing $w$ ensures $K_0$ converges to $\E{K_0}$. However, for a fixed $w$, increasing $d$ makes the entries of $K_0$ deviate from $\E{K_0}$, thus degrading the spectrum of $K_0$. Thus increasing depth beyond a point hurts training performance.

%The results on $\E{K_0}$ and $Var\left[K_0\right]$ cannot be applied to DNNs with ReLU gates, since the weights and the gates are not independent. However, the results apply to GaLU networks, where the gating pattern is generated by a different network, and hence the statistical independence of weights and the gating pattern can be ensured. We also present experimental results to support the theory.

\end{comment}
$\bullet$ \textbf{Generalisation:} We show via experiments on standard datasets such as MNIST and CIFAR that better generalisation happens when the activations $A_t$ change with time. In particular, for MNIST restricted to two labels, we experimentally verify that the quantity $y^\top (\widehat{H_t})^{-1}y$\footnote{For a matrix $H$, $\hat{H}=\frac{1}{trace(H)}H$ .} reduces as the training progresses. This shows that the eigenspaces of the NPK matrix align themselves in accordance to the labelling function. These experiments supports the fact that NPF and the NPK are indeed learnt during training in a manner to improve the generalisation performance. 
%sWe develop preliminary theory to throw light on feature learning, and argue that, at time $t$, and input example $x_s\in \R^{d_{in}},s\in[n]$, there are two sub-networks, namely i) an \emph{active} sub-network (for whose paths $\partial A_t(x_s,p)$ is close to zero) which holds the memory corresponding to that input example, and (ii) a \emph{sensitive} sub-network (for whose paths $\partial A_t(x_s,p)$ is significant), where the feature learning happens.


%In a DNN with ReLU activations, each input example can be associated with a gating pattern, i.e., the set of gates that are \emph{on} for that example. Note that, only the sub-network of those weights corresponding to these \emph{on} activations/gates are responsible for predicting the output. In this paper, we obtain several new insights related to optimisation and generalisation in DNNs via a formal study of the paths and gates. In what follows, we discuss the organisation of the paper and key contributions.

\begin{comment}
\textbf{Path-View:} Central to the contributions in the paper is the concept of \emph{path-view}, wherein, paths are regarded as basic building blocks of DNNs. A \emph{path} starts from an input node $i\in[d_{in}]$, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Using shorthand notation, say a path $p$ pass through input node $\I_0(p)\in [d_{in}]$, and weights $\Theta(l,\I_{l-1}(p),\I_l(p)),l\in[d-1]$, and gates $G(l,I_l(p))$

For an FC-DNN with $d$ layers (depth), and $w$ hidden units per-layer, the total number of paths (denoted by $P$) starting from a given input node $i\in[d_{in}]$ is $w^{(d-1)}$.

$\bullet$ At time $t$, for an input $x\in \R^{d_{in}}$, a path $p$ is associated with two quantities namely: (i) the path value denoted by $v_t(p)$, which is the product of the weights in the path, and (ii) the path activation level denoted by $A_t(x,p)$ which is the product of the gates\footnote{In the case of DNN with ReLU activations/gates, $A_t(x,p) \in \{0,1\}$.} in the path. Note that the activation level of a path is dependent on the input and the value of a path is not dependent on the input.
\end{comment}


\begin{comment}
In a DNN with ReLU activations, each input example can be associated with a gating pattern, i.e., the set of activations that are \emph{on} for that example. Note that, only the sub-network of those weights corresponding to these \emph{on} activations/gates are responsible for predicting the output. In this paper, we obtain several new insights related to Questions I and II by a formal study of the paths and gates.
\end{comment}

\input{pathgate}
\input{expressivity}
%\input{abstract.tex}
%\input{intro.tex}
\input{optimisation.tex}
\input{generalisation.tex}
%\input{convnet.tex}
%\input{related.tex}
%\input{future.tex}
\bibliographystyle{plainnat}
\bibliography{refs}
%\input{appendix}


\end{document}
