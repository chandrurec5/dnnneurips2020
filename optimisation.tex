\section{Neural Path Feature: Optimisation and Generalisation}
The neural path features capture the information stored in the gates of a deep network. To this end, we study DGNs with fixed gates, i.e., $\G_t=\G_0,\,\forall\,t\geq 0$, or in the case of parameterisation $\Tg_t=\Tg_0,\,\forall\,t\geq 0$. Since the gates (and hence the NPFs) are fixed, the output of the DGN is given by 
\begin{align}\label{eq:fixednpf}
\hat{y}_t=\Phi^\top_0 v_t
\end{align}
Thus, training the DGN with GD boils down to learning the path values $v_t$. 
\begin{lemma}\label{lm:disentangle}[Path Disentanglement] Let $I_P$ be the $P\times P$ identity matrix, and let $\nabla_\Theta v_t$ be a $P\times d_{net}$ matrix, with $\nabla_\Theta v_t(p,\theta)=\frac{\partial v_{\Theta}}{\partial \theta}|_{\Theta=\Theta_t}$. Then, 
under \Cref{assmp:main}-(ii), it follows that $\E{(\nabla_{\Theta}v_{\Theta_0})(\nabla_{\Theta}v_{\Theta_0})^\top }= d\sigma^{2(d-1)}I_{P}$.
\end{lemma}

$\bullet$ \textbf{Neural Tangent Kernel (NTK):} When the gates are fixed, it trivially follows that $A_t(\cdot,\cdot)=A_0(\cdot,\cdot),\,\forall\,t\geq 0$, and hence $\partial_{\tg} A_t(\cdot,\cdot)=0$. Thus, the feature gradient, $\psi^{\phi}_{x_s,t}=0$, and for the given dataset of $n$ points, it is enough to look the value gradient matrix $\Psi^v_t=(\nabla_{\Theta}v_{\Theta_t})^\top \Phi_{\Tg_t}$. The expression for the NTK matrix is given by:
\begin{align}\label{eq:kt}
K_t={\Psi^v}^\top_t\Psi^v_t= \Phi^\top_0(\nabla_{\Theta}v_{\Theta_t})(\nabla_{\Theta}v_{\Theta_t})^\top \Phi_0
\end{align}
$\bullet$ \textbf{Simplifying $K_t$:} Note that \Cref{assmp:main} does not directly hold for DNNs with ReLU activations. The reason is that conditioned on the fact that a particular ReLU is \emph{on}, we know that its incoming weights cannot all be simultaneously negative. However, this situation can be remedied by obtaining the NPFs from a gating network parameterised by $\Tg\inrdnet$.  \Cref{assmp:main} can be then satisfied by sampling $\Tg_0\inrdnet$ and $\Theta_0\inrdnet$ independently. Using \Cref{assmp:main}-(i), we have $\E{K_0}=\Phi^\top_0\E{(\nabla_{\Theta}v_{\Theta_0})(\nabla_{\Theta}v_{\Theta_0})^\top}\Phi_0$, and we need the following lemma to complete part of the result in \Cref{th:main}. Intuitively speaking, the active sub-networks corresponding to different examples have some overlap, and hence there is bound to be \emph{cross-talk} of the gradients flowing through them, and under \Cref{assmp:main}, this cross talk gets cancelled as in \Cref{lm:disentangle} resulting in \Cref{th:main}.
\begin{comment}
$\bullet$ \textbf{Active Sub-Network and Gradient Flow:} Each input example has its own associated active sub-network, and the gradient of  a particular example, the gradient flows through the weights of the corresponding active sub-network. Now, the active sub-networks corresponding to different examples have some overlap, and hence there is bound to be \emph{cross-talk} of the gradients flowing through them. Note that the \emph{cross-talk} of the paths appears as the $(\nabla_{\Theta}v_{\Theta_t})(\nabla_{\Theta}v_{\Theta_t})^\top$ in \eqref{eq:kt}, which got simplified due to the \Cref{assmp:main}, leading to \Cref{lm:disentangle} which in turn resulted in \Cref{th:main} (restricted to the case of fixed gates, wherein, $K^{\phi}_t=0$).\\
\end{comment}
$\bullet$ \textbf{NTK = NPK:} For $\sigma=O\left(\sqrt{\frac{1}{w}}\right)$, for a fixed depth $d$, as width $w$ increases, $K_0\ra\E{K_0}=d\sigma^{2(d-1)}H_0$. Thus, for random initialisation and large width, the NPK dictates the convergence properties of GD. In particular, the spectrum of $\lambda_0$ directly controls the spectral properties of the NTK matrix $K_0$. We now argue that when $\sigma=\sqrt{\frac{2}{w}}$, increasing depth causes whitening of $\lambda_0$ to resolve the optimisation question, and look at generalisation performance of DNGs trained with different NPFs to resolve the generalisation question.

\begin{comment}
\textbf{Proof of \Cref{th:main}-(i):} Let $\varphi_t=(\varphi_{p,t},p\in[P])\in \R^{d_{net}\times P}$ matrix, then since $K_t=\Psi^\top_t\Psi$, where $\Psi_t=\varphi_t \Phi_t$, we have $\E{K_t}=\E{\Phi^\top_t \varphi^\top_t \varphi_t \Phi_t}$. At initialisation, using the \Cref{assmp:main}-(i), we can pull out $\Phi^\top_t$ and $\Phi_t$ outside of the expectation to have \begin{align}\label{eq:pullout}\E{K_0}=\Phi^\top_0\E{ \varphi^\top_t \varphi_t }\Phi_0,\end{align} and from \Cref{lm:disentangle}, it follows that $\E{ \varphi^\top_t \varphi_t }=d\sigma^{2(d-1)}I$, and hence $\E{K_0}=d\sigma^{2(d-1)}\Phi^\top_0\Phi_0=d\sigma^{2(d-1)}H_0$.\\
\end{comment}
\subsection{Optimisation}
%This overlap is captured by $\lambda_t(s,s')$ which is the measure of overlap of the sub-networks that are active for both the inputs $x,x'\in\R^{d_{in}}$. Under. \Cref{assmp:main}, the inter-path interaction $\varphi^\top_t\varphi_t$ gets disentangled, result in the claim \Cref{th:main}-(i).\\
%However, in the later part of this section, we consider a special case, wherein, we give an explicit characterisation of the spectrum of $\E{K_0}$. Characterising $\lambda_0$ for the general case is left as future work.\\
\textbf{Why increasing depth till a point helps in training? } Let us first look at the diagonal terms of $\lambda_0$. It is reasonable to assume that, owing to the symmetric nature of the weights, roughly $\mu=\frac{1}{2}$ fraction of the gates are \emph{on} every layer. Thus $\lambda_0(s,s)\approx (w/2)^{d-1}$. Now, due our choice of $\sigma=\sqrt{\frac{2}{w}}$, the diagonal entries will be close to $1$. We now turn our attention towards the non-diagonal entries of $\lambda_0$. Define $\tau(s,s',l)\stackrel{def}=\sum_{i=1}^w G_{x_s,t}(l,i)G_{x_{s'},t}(l,i)$ be the overlap of the active gates in layer $l$ for input examples $s,s'\in[n]$, and  let $\eta\stackrel{def}=\max_s\left(\max_{s',l} \frac{\tau(s,s',l)}{\tau(s,s,l)}\right)$ be the maximum overlap between gates of a layer (maximum taken over over input pairs $s,s'\in[n]$ and layers $l\in [d]$).  Then it follows that $\max_{s,s'\in [n]} \frac{\bar{\lambda}_{cross}(s,s')}{\bar{\lambda}_{self}(s)}\leq \eta^{d-1}$. Thus, the non-diagonal entries decay an exponential rate in comparison to the diagonal entries.\hfill\\
\textbf{Why increasing the depth beyond hurts training?} The variance expression in \Cref{th:main}-$(ii)$ involves $d^2$ and $d^3$ terms, and hence for a fixed width as depth increases, the entries of $K_0$ deviates from $\E{K_0}$, and as a result the spectrum of $K_0$ degrades, thereby hurting training performance.\\
\textbf{Characterising $\lambda_0$:} Note that $\lambda_0$ is the matrix of correlation of the active sub-network for each input, and thus $\lambda_0$ depends on the inputs. However, what we have argued is that this input dependence of $\lambda_0$ \emph{fades} away as the depth increases. However, for a general dataset, it is not possible to further comment on the spectral properties of $\lambda_0$. In what follows, we look an experiment, wherein, we completely control $\lambda_0$ so as to gain further insights.
\begin{comment}$5.$ \Cref{assmp:main} is not satisfied by ReLU activations, i.e., conditioned on the fact that a ReLU is \emph{on}, the incoming weights cannot all be simultaneously negative. This implies that the $\Phi^\top_t$ and $\Phi_t$ terms cannot be pulled out of the expectation as in \eqref{eq:pullout}.
\end{comment}
\begin{comment}
\begin{wrapfigure}{h}{0.27\textwidth}
\includegraphics[scale=0.22]{figs/dgn-fra-ecdf-ideal.png}
\includegraphics[scale=0.22]{figs/dgn-fra-ecdfbyd-w25.png}
\includegraphics[scale=0.22]{figs/dgn-fra-ecdfbyd-w500.png}
\includegraphics[scale=0.21]{figs/dgn-fra-conv-w25.png}
\includegraphics[scale=0.21]{figs/dgn-fra-conv-w500.png}
\caption{Ideal spectrum}
\label{fig:dgn-frg-gram-ecdf}
\end{wrapfigure}
\end{comment}
\begin{comment}
\textbf{A:} In our experiment, we consider DGNs with the following three different gating schemes namely, i) fixed random (FR): for each input example in the dataset, we sample gating values from $Ber(\mu)$ taking values in $\{0,1\}$, and collect it in $\G_0$. DGN with FR gating is useful only for pure memorisation, ii) fixed explicit (FE): gating parameter $\Tg_t=\Tg_0,\forall\, t\geq 0, \Tg_0\neq \Theta_0$, iii) fixed implicit (FI): gating parameter $\Tg_t=\Tg_0,\forall\, t\geq 0, \Tg_0= \Theta_0$. The FI case does not satisfy \Cref{assmp:main}-(i).
\end{comment}
\begin{wrapfigure}{h}{0.20\textwidth}
\includegraphics[scale=0.2]{figs/dgn-fra-ecdf-ideal-small.png}
\caption{Ideal spectrum.}
\label{fig:ideal-spectrum}
\end{wrapfigure}
\textbf{Experiment with fixed random gates:} We consider the dataset given by $(x_s,y_s)_{s=1}^n\in \R\times \R$, where $x_s=1,\forall s\in [n]$, and $y_s\sim unif([-1,1])$, $n=200$. For each input example in the dataset, we sample gating values from $Ber(\mu)$ taking values in $\{0,1\}$, and collect it in $\G_0$, which we keep fixed throughout training of the DGN (i.e., $\G_t=\G_0,\,\forall\,t\geq 0$).  Even though the input is the same for all the $n$ examples, via the fixed random gating strategy, we ensure that there is a separate random sub-network for each input example.  In this case, it is easy to check that $\mathbb{E}_{\mu}\left[\lambda_0(s,s)\right]=(\mu w)^{(d-1)},\forall s\in[n]$ and $\mathbb{E}_{\mu}\left[\lambda_0(s,s')\right]=(\mu^2 w)^{(d-1)},\forall s,s'\in[n]$.\WFclear
$\bullet$ \textbf{Spectrum (Ideal):} The input Gram matrix $x^\top x$ is a $n\times n$ matrix with all entries equal to $1$ and its rank is equal to 1, and hence $H_0=\lambda_0$.  For $\sigma=\sqrt{\frac{1}{\mu w}}$, and by further averaging $\mathbb{E}_{\mu}\left[K_0(s,s)/d\right]=1$, and $\mathbb{E}_{\mu}\left[K_0(s,s')/d\right]=\mu^{(d-1)}$. Now, let $\rho_i\geq 0,i \in [n]$ be the eigenvalues of $\frac{\E{K_0}}{d}$, and let $\rho_{\max}$ and $\rho_{\min}$ be the largest and smallest eigenvalues.  One can easily show that $\rho_{\max}=1+(n-1)\mu^{d-1}$ and corresponds to the eigenvector with all entries as $1$, and $\rho_{\min}=(1-\mu^{d-1})$ repeats $(n-1)$ times,  which corresponds to eigenvectors given by $[0, 0, \ldots, \underbrace{1, -1}_{\text{$i$ and $i+1$}}, 0,0,\ldots, 0]^\top \in \R^n$ for $i=1,\ldots,n-1$. Note that as $d\ra\infty$, $\rho_{\max},\rho_{\min}\ra 1$.\\
$\bullet$ \textbf{Spectrum (numerical):} We look at the cumulative eigenvalue (e.c.d.f) obtained by first sorting the eigenvalues in ascending order then looking at their cumulative sum. The ideal behaviour (top plot of \Cref{fig:dgn-frg-gram-ecdf}) as predicted from theory is that for indices $k\in[n-1]$, the e.c.d.f should increase at a linear rate, i.e., the cumulative sum of the first $k$ indices is equal to $k(1-\mu^{d-1})$, and the difference between the last two indices is $1+(n-1)\mu^{d-1}$. In \Cref{fig:dgn-frg-gram-ecdf}, we plot the actual e.c.d.f for various depths $d=2,4,6,8,12,16,20$ and $w=25,500$ a (second and third from top in \Cref{fig:dgn-frg-gram-ecdf}). \hfill\\
$\bullet$ \textbf{Convergence (numerical):} In order to compare how the rate of convergence varies with the depth, we set the step-size $\alpha=\frac{0.1}{\rho_{\max}}$, $w=100$. We use the vanilla SGD-optimiser. Note the$ \frac{1}{\rho_{\max}}$ in the stepsize, ensures that the uniformity of maximum eigenvalue across all the instances, and the convergence should be limited by the smaller eigenvalues. We also look at the convergence rate of the ratio $\frac{\norm{e_t}^2_2}{\norm{e_0}^2_2}$. We notice that for $w=25$, increasing depth till $d=8$ improves the convergence, however increasing beyond $d=8$ worsens the convergence rate. For $w=500$, increasing the depth till $d=12$ improves convergence, and $d=16,20$ are worse than $d=12$.  This matches with the depth phenomena observed in practical DNNs and also matches our theory.
\input{dgnfig}
\begin{comment}
\begin{figure}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccc}
\includegraphics[scale=0.4]{figs/galu-ecdf-d.png}
&
\includegraphics[scale=0.4]{figs/galu-conv-d.png}
&
\includegraphics[scale=0.4]{figs/relu-ecdf.png}
&
\includegraphics[scale=0.4]{figs/relu-conv.png}
\end{tabular}
}
\caption{The left two plots shows the e.c.d.f and convergence rates for various depth in GaLU networks $w=100$. The third and fourth plot from the left show the e.c.d.f and convergence rates for various depth in ReLU networks $w=100$. The plots are averaged over $5$ runs. }
\label{fig:galu-d}
\end{figure}
\end{comment}