\section{Generalisation}\label{sec:generalisation}
In the previous section, we looked at frozen gating, i.e., $\G_t=\G_0,\forall t\geq 0$. The gating values were either sampled at random, as it was in the case of DGN-FRG, or generated by a separate network, as it was in the case of GaLU networks. In this section, we present experiments and analysis that provide insights about the role of gating in the generalisation performance of deep networks.


\textbf{DGNs with adaptable gates:} To investigate the role of gate adaptation,  we study parameterised DGNs of the general form given in \Cref{tb:dgn-parameterised}. %The specific variants we study in the experiments are in \Cref{tb:dgn-family}.
\FloatBarrier
\begin{table}[h]
\resizebox{\columnwidth}{!}{
\centering
\begin{tabular}{|c|c|c|}\hline
Gating Network & Weight Network\\\hline
$z_{x,\Tg_t}(0)=x$ & $z_{x,\Tw_t}(0)=x$ \\\hline
 $q_{x,\Tg_t}(l)={\Tg_t(l)}^\top z_{x,\Tg_t}(l-1)$ & $q_{x,\Tw_t}(l)={\Tw_t(l)}^\top z_{x,\Tw_t}(l-1)$\\\hline
 $z_{x,\Tg_t}(l)=q_{x,\Tg_t}(l)\odot G_{x,\Tg}(l)$ & $z_{x,\Tw_t}(l)=q_{x,\Tw_t}(l)\odot G_{x,\Tg_t}(l)$ \\\hline
 \multicolumn{2}{|c|}{$\hat{y}_{t}(x)={\Tw_t(d)}^\top z_{x,\Tw_t}(d-1)$}\\\hline 
 \multicolumn{2}{|c|}{$\begin{aligned}\beta >0: G_{x,\Tg_t}(l,i)&=\chi_{\epsilon}(-\beta q_{x,\Tg_t}(l,i)), \\ \beta=\infty: G_{x,\Tg_t}(l,i)&=\mathbbm{1}_{\{q_{x,\Tg_t}(l,i)>0\}}\end{aligned}$}\\\hline 
\end{tabular}
}
\caption{A DGN with parameterised gates. Here, for $\epsilon\geq 0$ is a hyper-parameter, and $\chi_{\epsilon}(v)=\frac{1+\epsilon}{1+\exp(v)}, \forall v\in \R$.}
\label{tb:dgn-parameterised}
\end{table}
In \Cref{tb:dgn-parameterised}, there are two networks namely i) the gating network, which generates the gating values, and is responsible for the path activation levels ii) the weight network, which is responsible for the path strengths. The gating network is parameterised by $\Tg\in \R^{d_{net}}$ and the weight network is parameterised by $\Tw\in \R^{d_{net}}$. A network is denoted by $\N(\Tg,\beta;\Tw_t)$. Note that a standard DNN with ReLU activations will be denoted by $\N(\Theta_t,\infty;\Theta_t)$, i.e., the gating network and the weight network share the same weights, i.e., $\Tg_t=\Tw_t=\Theta_t,\forall t\geq 0$. Note that, in our experiments we will also be considering convolution DGNs, wherein, in addition to the structure in \Cref{tb:dgn-parameterised}, weight sharing will be assumed implicitly.

\textbf{Different gating scenarios of interest:}  We wish to understand the role of gating in the generalisation of performance of deep networks. Towards this end, we study the following gating regimes:

$1$. \emph{Frozen Non-Learned} gating, wherein, the weights $\Tg\in\R^{d_{net}}$ of the gating network are initialised, but are kept frozen during training. 

$2$. \emph{Frozen Learned} gating, wherein, at initialisation, the weights $\Tg\in\R^{d_{net}}$ of the gating network are copied from a trained DNN with ReLU activations. However, the weights $\Tg\in\R^{d_{net}}$ are kept frozen during training. 

$3.$ \emph{Adaptable} gating, wherein, the weights $\Tg\in\R^{d_{net}}$ of the gating network are trainable.

$4.$ \emph{Hard/Soft} gating, wherein, we call the gating to be hard (denoted by $\beta=\infty$) when it takes values in $\{0,1\}$  and soft (denoted by $\beta>0$) when it takes values in $(0,1)$. 

As it turns out, the above $4$ different regimes of gating exhibit different generalisation behaviour, which will be the discussed shortly.


\textbf{Frozen Non-Learned gates generalise poorly:} Under this regime, we first compare DGNs of the form $\N(\Theta_t,\infty;\Theta_t)$ (which stands for the standard DNN with ReLU activation) and $\N(\Tg_{\dagger},\infty;\Tw_t)$ (which we call as GaLU networks). We trained both ReLU and GaLU networks on standard MNIST dataset to close to $100\%$ accuracy. We observed that the GaLU network trains a bit faster than the ReLU network (see \Cref{fig:galu-relu} ). However, the test performances were around $96\%$ and  $98\%$ for GaLU and ReLU networks respectively. We trained ReLU and GaLU networks on standard CIFAR-10 dataset close to $100\%$ training accuracy. In this case, the test performances were around $72\%$ and $64\%$ for ReLU and GaLU networks respectively.

Note that, in the case of $\N(\Tg_{\dagger},\infty;\Tw_t)$, we were initialising $\Tg_0\in \R^{d_{net}}$ and $\Tw_0\in \R^{d_{net}}$ in a statistically independent manner. Thus, in order to eliminate the effect of the initialisation, we also compared DGNs of the form  $\N(\Theta_t,\infty;\Theta_t)$ and $\N(\Tw_{\dagger},\infty;\Tw_t)$ (which we call as frozen ReLU networks). We trained both ReLU and frozen ReLU networks on standard MNIST dataset to close to $100\%$ accuracy. The test performances were around $96\%$ and  $98\%$ for frozen ReLU and ReLU networks respectively. We trained ReLU and GaLU networks on standard CIFAR-10 dataset close to $100\%$ training accuracy. In this case, the test performances were around $72\%$ and $64\%$ for ReLU and frozen ReLU networks respectively.



\textbf{Frozen Learned gates generalise better:} Under this regime, we first compare DGNs of the form $\N(\Theta_t,\infty;\Theta_t)$ and $\N(\Tg_{\dagger},\infty;\Tw_t)$, wherein, $\Tg_0$ is copied from the weights of a pre-trained ReLU network of the same architecture (this we call as GaLU networks with learned gates). In the case of MNIST, 
In the case of CIFAR-10, GaLU with learned gates achieved a test performance of $70\%$ (which is better than GaLU with non-learned gates with a performance of  $64\%$).


\input{mnistfig}

\textbf{Insights in the case of adaptable gates:} We consider ``Binary''-MNIST data set with two classes namely digits $4$ and $7$, with the labels taking values in $\{-1,+1\}$ and squared loss. We trained a standard DNN with ReLU activation ($w=100$, $d=5$). Recall from \Cref{sec:intro}, that $M_t=\Phi^\top_t\Phi_t$  (the Gram matrix of the features) and let $\widehat{M}_t=\frac{1}{trace(M_t)}M_t$ be its normalised counterpart. For a subset size, $n'=200$ ($100$ examples per class) we plot $\nu_t=y^\top (\widehat{M}_t)^{-1} y$, (where $y\in\{-1,1\}^{200}$ is the labeling function), and observe that $\nu_t$ reduces as training proceeds (see middle plot in \Cref{fig:path-norm}). Note that $\nu_t=\sum_{i=1}^{n'}(u_{i,t}^\top y)^2 (\hat{\rho}_{i,t})^{-1}$, where $u_{i,t}\in \R^{n'}$ are the orthonormal eigenvectors of $\widehat{M}_t$ and $\hat{\rho}_{i,t},i\in[n']$ are the corresponding eigenvalues. Since $\sum_{i=1}^{n'}\hat{\rho}_{i,t}=1$, the only way $\nu_t$ reduces is when more and more energy gets concentrated on $\hat{\rho}_{i,t}$s for which $(u_{i,t}^\top y)^2$s are also high. However, in $M_t=(x^\top x)\odot \lambda_t$, only $\lambda_t$ changes with time. Thus, $\lambda_t(s,s')$ which is a measure of overlap of sub-networks active for input examples $s,s'\in[n]$, changes in a manner to reduce $\nu_t$. We can thus infer that the \emph{right} active sub-networks are learned over the course of training.

\begin{figure}
\centering
\includegraphics[scale=0.5]{figs/path-gram.png}
\caption{Shows $\nu_t=y^\top (\widehat{M}_t)^{-1}y$, where $M_t=\Phi_t^\top \Phi_t$.}
\label{fig:path-norm}
\end{figure}

\begin{comment}
\FloatBarrier
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|}\hline
Terminology& Notation & Remarks\\\hline
ReLU & $\N(\Theta_t,\infty;\Theta_t)$ & $\Theta_t\in \R^{d_{net}}$\\\hline
GaLU (Frozen) &$\N(\Tg_{\dagger},\infty;\Tw_t)$ & $\Tw_t\in \R^{d_{net}}$, $\Tg_{\dagger}\in \R^{d_{net}}$\\\hline
Soft-ReLU &$\N(\Theta_t,\beta;\Theta_t)$ & $G\in(0,1)$ (not decoupled)\\\hline
Soft-GaLU &$\N(\Tg_t,\beta;\Tw_t)$ &  $G\in(0,1)$ (decoupled) \\\hline
\end{tabular}
}
\caption{Shows the variants of DGNs in the experiments. Here, $\dagger$ stands for frozen weights that are initialised by not trained.}% In all our experiments (expect one) we set $\epsilon=0$ in $\chi_{\epsilon}$.}
\label{tb:dgn-family}
\end{table}
\end{comment}

\begin{figure*}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccc}
\includegraphics[scale=0.1]{figs/allnet-train.png}
&
\includegraphics[scale=0.1]{figs/allnet-gen.png}
&
\includegraphics[scale=0.1]{figs/galu-learn-no-learn.png}
&
\includegraphics[scale=0.1]{figs/relu-froze-no-froze.png}

\end{tabular}
}
\caption{The left two plots show respectively the training and generalisation in the $4$ different networks with $w=100$, $d=6$. Generalisation performance (dotted lines) of learned gates vs unlearned gates ($3^{rd}$ from left), adaptable gates vs frozen gates (rightmost). The plots are averaged over $5$ runs. }
\label{fig:adapt}
\end{figure*}

\begin{figure*}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccc}
\includegraphics[scale=0.1]{figs/galu-learn-no-learn-knorm.png}
&
\includegraphics[scale=0.1]{figs/relu-froze-no-froze-knorm.png}
&
%\includegraphics[scale=0.18]{figs/path-gram.png}

\includegraphics[scale=0.18]{figs/activation-gram-unnorm.png}
&
\includegraphics[scale=0.18]{figs/activation-gram-norm.png}
\end{tabular}
}
\caption{Shows $\nu_t=y^\top (H_t)^{-1}y$ for (from the left) i) $H_t=\widehat{K}_t$, ii) $H_t=\widehat{K}_t$,  (iii) $H_t=K^a_t$ (iv) $H_t=\widehat{K^a_t}$. Here $K^a_t$ is the Gram matrix of activations in the soft-GaLU network.}
\label{fig:adapt-norm}
\end{figure*}

\begin{comment}

We now explain the idea behind the various gates (and some more) in \Cref{tb:dgn-family} as follows:

$1.$ The most general gate is called the \emph{soft-GaLU} gate denoted by $\N(\Tg,\beta;\Tw)$. Here, the gating values and hence the path activation levels are decided by $\Tg_t\in\R^{d_{net}}$, and the path strengths are decided by $\Tw_t\in \R^{d_{net}}$. This network has $2d_{net}$ parameters. % Both $\Tg_0$ and $\Tw_0$ are independent of each other and initialised according to Assumption~\ref{assmp:mainone}.

$2.$ The standard DNN with ReLU gating is denoted by $\N(\Theta_t,\infty;\Theta_t)$, where $\infty$ signifies that the outputs are $0/1$ (see \Cref{tb:dgn-parameterised}). Here, both the gating (and hence path activation levels) and the path strengths are decided by the same parameter namely $\Theta_t\in\R^{d_{net}}$.

$3.$ $\N(\Theta_t,\beta;\Theta_t)$ is a DNN with what we call the \emph{soft-ReLU} gates, where the gating values are in $(0,1+\epsilon)$ instead of $0/1$. Here too, like the standard ReLU networks, both the gating values and the path strengths are decided by $\Theta_t\in\R^{d_{net}}$.

$4.$ $\N(\Tg_{\dagger}, \infty;\Tw_t)$ is what we call a GaLU-frozen DGN, where the gating parameters $\Tg\in \R^{d_{net}}$ are initialised but not trained. 

$5.$ $\N(\Tg_t,\beta;\Tw_{\dagger})$ is a network where only the gating parameters $\Tg_t\in \R^{d_{net}}$ are trainable and the parameters which dictate the path strengths namely $\Tw$ are initialised by not trained.

\end{comment}

\subsection{Gate adaptation: Preliminary analysis }

 \textbf{Hard gate/ReLU artefact:} 
%We refer to the function $\chi_\epsilon(v)$ in \Cref{tb:dgn-parameterised} as the \emph{soft} gating function.
The NTF matrix has two components given by $\Psi_t(m,s)={\phi_{x_s,\G_t }^\top\frac{\partial w_{t}} {\partial \theta(m)}+ \frac{\partial \phi_{x_s,\G_t }^\top}{\partial \theta(m)} w_{t}}$. In the case of ReLU activation, the gating values are either $0/1$, the activation levels are also $0/1$ and hence their derivative is $0$. Thus, in the case of standard DNN with ReLU activations the change in the gating pattern with respect to time (captured by $\frac{\partial \phi_{x_s,\G_t }^\top}{\partial \theta(m)} w_{t}$) is unaccounted for both in analysis as well as in the gradient descent update rule, i.e., when a gradient step is taken, it is not known beforehand the effect of the gradient step on the gating pattern. 

%In contrast, the `soft-gating' is differentiable, and hence if follows that $\frac{\partial \phi_{x_s,\G_t }^\top}{\partial \theta(m)} w_{t}\neq 0$, which is also accounted in the analysis.
 \textbf{Soft-Gating}: We refer to the function $\chi_\epsilon(v)$ in \Cref{tb:dgn-parameterised} as the \emph{soft} gating function, which takes values in $(0,1+\epsilon)$.  An important feature of the soft-gate is that, the term $\frac{\partial \phi_{x_s,\G_t }^\top}{\partial \theta(m)} w_{t}\neq 0$, which is accounted in both analysis as well as the gradient descent update rule.
 
 
\textbf{Gram Matrices with gate adaptation term:} In the case of $\N(\Tg_t,\beta;\Tw_t)$ networks (which we call as soft-GaLU networks), since there are two set of parameters (total $2d_{net}$) $\Psi_t^\top=[{\Psi^w}^\top_t,{\Psi^a}^\top_t]$ is a $n\times 2d_{net}$, we have $K_t=K^w_t+K^a_t$,  where $K^w_t={\Psi^w_t}^\top \Psi^w_t$, and $K^a_t={\Psi^a_t}^\top \Psi^a_t$. In the case of $\N(\Theta_t,\beta;\Theta_t)$ (which we call as soft-ReLU)  we have $K_t={K^w_t}+{K^a_t}+{\Psi^w_t}^\top {\Psi^a_t}+{\Psi^a_t}^\top {\Psi^w_t}$.

\begin{definition} 
For a soft-GaLU DGN, using any $i\in[d_{in}]$, define $\delta(s,s')\stackrel{def}= \underset{{p\rsa i}}{\sum} \sum_{m=1}^{d_{net}}\frac{\partial A_{\Tg_0}(x_s,p)}{\partial \tg(m)} \frac{\partial A_{\Tg_0}(x_{s'},p)}{\partial \tg(m)}$.
\end{definition}

\begin{lemma} Under Assumptions~\ref{assmp:mainone},~\ref{assmp:maintwo}, in soft-GaLU networks we have: (i) $\E{K_0}=\E{K^w_0}+\E{K^a_0}$, 
 (ii) $\E{K^w_0}=\sigma^{2(d-1)} (x^\top x)\odot \lambda$,  (iii) $\E{K^a_0}=\sigma^{2d}  (x^\top x)\odot \delta$
\end{lemma}

Recall that $\G_t\stackrel{def}=\{G_{x_{s},t}(l,i) \in [0,1], \forall s\in[n],l\in[d-1],i\in[w]\}$. We now define the following:

$\bullet$ \textbf{Active Gates:} For an input $x_{s}\in \R^{d_{in}}$, and a threshold value $\tau_{\A}\in (0,1+\epsilon)$, define $\G_t^{\A}(x_s,\tau_{\A})\stackrel{def}=\left\{G_{x_s,t}(l,i)\colon G_{x_s,t}(l,i)> \tau_{\A}, l\in[d-1],i\in[w]\right\}$. These are the gates that are \emph{on} (i.e., more than threshold $\tau_{\A}$) for input $x_s\in\R^{d_{in}}$.

$\bullet$ \textbf{Sensitive Gates:} For an input $x_{s}\in \R^{d_{in}}$, and a threshold value $\tau_{\S}>0$, define $\G_{t}^{\S}(x_s,\tau_{\S})\stackrel{def}=\cup_{m=1}^{d_{net}}\left\{G_{x_s,t}(l,i)\colon \left|\frac{\partial G_{x_s,t}(l,i)}{\partial \tg(m)}\right| >\tau_{\S},l\in[d-1],i\in[w] \right\}$. These are set of gates that are sensitive to changes in anyone of the $\tg(m),m\in[d_{net}]$ tunable parameters that control the gates.

$\bullet$ \textbf{Relation between sensitive and active gates:} From the nature of the soft-gating function $\chi_{\epsilon}(v)$ it follows that for any given $\tau_{\A}\in(0,1+\epsilon)$, it follows that $\G_t^{\A}(x_s,\tau_{\A})\cap \G_t^{\S}(x_s,\tau_{\S})=\emptyset,\forall \tau_{\S}>\frac{d \chi_{\epsilon}(v)}{d v}|_{v=\chi^{-1}_{\epsilon}(\tau_{\A})}$. Also, note that as $\tau_{\A}\ra (1+\epsilon)$, $\tau_{\S}\ra 0$.

$\bullet$ \textbf{Sensitivity of Activations:} For a path $p$, and a gating parameter $\tg(m),m\in[d_{net}]$ we have $\frac{\partial A_{\Tg_t}(x_s,p)}{\partial \tg(m)}$ to be equal to
\begin{align}\label{eq:sensitivity}
\sum_{l=1}^{d-1} \Big(\frac{\partial G_{x_s,\Tg_t}(l,p(l))}{\partial \tg(m)} \Big)\Big(\Pi_{l'\neq l} G_{x_s,\Tg_t}(l',p(l'))\Big)
\end{align}

In what follows we assume that $\tau_{\S}>\frac{d \chi_{\epsilon}(v)}{d v}|_{v=\chi^{-1}_{\epsilon}(\tau_{\A})}$.

$\bullet$ \textbf{Active Sub-Network:} Which paths are active for input $x_s\in\R^{d_{in}}$?\quad Choose a threshold $\tau_{\A}$ close to $1$. The paths that pass through gates in $\G_t^{\A}(x_s,\tau_{\A})$ do not matter much in gate adaptation because they are already \emph{on}, and are responsible for holding the memory for input $x_s\in\R^{d_{in}}$. In particular, \eqref{eq:sensitivity} evaluates close to $0$ for such paths because $\left|\frac{\partial G_{x_s,\Tg_t}(l,p(l))}{\partial \tg(m)}\right|<\frac{d \chi_{\epsilon}(v)}{d v}|_{v=\chi^{-1}_{\epsilon}(\tau_{\A})}$.

$\bullet$ \textbf{Sensitive Sub-Network:} Which paths are learning for input $x_s\in\R^{d_{in}}$? \quad Those paths that have one gate from $\G^{\S}_t(x_s,\tau_{\S})$ and the rest of the $(d-2)$ gates from the set  $\G^{\A}_t(x_s,\tau_{\A})$. For such paths, the magnitude of at least one of the $(d-1)$ terms in \eqref{eq:sensitivity} will be greater than $\tau_{\S}(\tau_{A})^{(d-2)}$, and the rest of the $(d-2)$ terms will contain a term whose magnitude is less than $\frac{d \chi_{\epsilon}(v)}{d v}|_{v=\chi^{-1}_{\epsilon}(\tau_{\A})}$ component and hence will contribute less to the summation.

$\bullet$ \textbf{Role of $\beta$} for now is limited to an analytical convenience, especially to address the non-differentiability artefact in ReLU gates. The ideas is that as $\beta\ra\infty$, the analysis applies more sharply to networks with ReLU gates.



\textbf{Adaptable gates generalise better:} In all experiments below, we use step-size $\alpha=1e^{-4}$, $w=100,d=6$ and we use \emph{RMSprop} to train.
\begin{comment}
\FloatBarrier
\begin{figure}[h]
\resizebox{\columnwidth}{!}{
\begin{tabular}{c}
\includegraphics[scale=0.1]{figs/allnet-train.png}
\end{tabular}
}
\caption{Shows the training performance of the $4$ different networks in \textbf{Experiment $3$} with $w=100$, $d=6$.} %Generalisation performance (dotted lines) of learned gates vs `not'-learned gates ($3^{rd}$ from left), adaptable gates vs frozen gates (rightmost). The plots are averaged over $5$ runs. }
\label{fig:adapt-4net-train}
\end{figure}
\end{comment}

$\bullet$ \textbf{Experiment $3$:} On `Binary'-MNIST, we train four different networks ($w=100$, $d=6$), namely, $\N(\Tg_t,\beta=4;\Tw_t)$ (soft-GaLU), $\N(\Theta_t,\beta=4;\Theta_t)$ (soft-ReLU), $\N(\Theta_t,\infty;\Theta_t)$ (ReLU), and $\N(\Tg_{\dagger}, \infty;\Tw_t)$ (GaLU with frozen gates). We observe that the $3$ networks with adaptable gates generalise better the GaLU network with frozen gates as shown in \Cref{fig:adapt}.
\begin{comment}
\FloatBarrier
\begin{figure}[h]
\resizebox{\columnwidth}{!}{
\begin{tabular}{c}
\includegraphics[scale=0.1]{figs/allnet-gen.png}
%\includegraphics[scale=0.1]{figs/galu-learn-no-learn.png}
%\includegraphics[scale=0.1]{figs/relu-froze-no-froze.png}

\end{tabular}
}
\caption{Shows the generalisation performance of the $4$ different networks in \textbf{Experiment $3$} with $w=100$, $d=6$.} %Generalisation performance (dotted lines) of learned gates vs `not'-learned gates ($3^{rd}$ from left), adaptable gates vs frozen gates (rightmost). The plots are averaged over $5$ runs. }
\label{fig:adapt-4net-gen}
\end{figure}
\end{comment}






$\bullet$\textbf{Experiment $4$:} We train a frozen ReLU network $\N(\Tg_{\dagger},\infty;\Tw_t)$, where $\Tg_0=\Tw_0$ (weights chosen according to Assumption~\ref{assmp:maintwo}), and a standard ReLU network $\N(\Theta_t,\infty;\Theta_t )$ in which the gates adapt. We observe that generalisation is better when the gates adapt (see right most plot in \Cref{fig:adapt}).  
\begin{comment}
\FloatBarrier
\begin{figure}[h]
\resizebox{\columnwidth}{!}{
\begin{tabular}{c}
%\includegraphics[scale=0.1]{figs/galu-learn-no-learn.png}
\includegraphics[scale=0.1]{figs/relu-froze-no-froze.png}
\end{tabular}
}
\caption{ \textbf{Experiment $4$} with $w=100$, $d=6$. Generalisation performance (dotted lines) adaptable gates is better than frozen gates. The training performance is shown in bold lines. The plots are averaged over $5$ runs. } %Generalisation performance (dotted lines) of learned gates vs `not'-learned gates ($3^{rd}$ from left), adaptable gates vs frozen gates (rightmost). The plots are averaged over $5$ runs. }
\label{fig:adapt-relu-frozen}
\end{figure}
\end{comment}

$\bullet$ \textbf{Experiment $5$ (Lottery Ticket):} We train $\N(\Theta_t,\infty;\Theta_t)$ (a standard DNN with ReLU gates) till time $T=100$ epochs. We then use the weights to initialise $\Tg_0=\Theta_T$ for the network $\N(\Tg_{\dagger},\infty;\Tw_t)$ and train $\Tw_t$ (initialised independently) and we call this GaLU with learned gates. We also consider the case of GaLU network with non-learned gates, where we train $\N(\Tg_{\dagger},\infty;\Tw_t)$ where $\Tg_0$ and $\Tw_0$ is initialised according to Assumptions~\ref{assmp:mainone},~\ref{assmp:maintwo}. We observe that the learned gates generalise better than the non-learned case (see third plot from the left in \Cref{fig:adapt}). This shows that the real lottery is in the gates.
\begin{comment}
\FloatBarrier
\begin{figure}[h]
\resizebox{\columnwidth}{!}{
\begin{tabular}{c}
%\includegraphics[scale=0.1]{figs/galu-learn-no-learn.png}
\includegraphics[scale=0.1]{figs/relu-froze-no-froze.png}
\end{tabular}
}
\caption{ \textbf{Experiment $5$} with $w=100$, $d=6$. Generalisation performance (dotted lines) of learned gates is better than `non'-learned gates. The training performance is shown in bold lines. The plots are averaged over $5$ runs. } 
\label{fig:adapt-galu-relu}
\end{figure}
\end{comment}


$\bullet$\textbf{Experiment $6$ (Lottery Ticket):} We consider $\N(\Tg_t,\infty;\Tw_{\dagger})$, where the weights corresponding to the strengths $\Tw$ are frozen, but the weights $\Tg$ that parameterise the gates are trained. We observed  a $56\%$ test performance in CIFAR-10 just by tuning the gates.  For this experiment, we used a \emph{convolutional} neural network, of the following architecture: input layer is $(32,32,3)$, followed by $conv(3, 3) : {64, 64, 128, 128}$, followed by $Flatten(), 256, 256, 10$.


$\bullet$ \textbf{Measure for generalisation:} In order to look for a possible explanation for the better generalisation performance (in the case of learned/adaptable gates), we plot $\nu_t=y^\top (H_t)^{-1}y$ (see \Cref{fig:adapt-norm}), for the following choices of $H_t$, namely  i) $H_t=\widehat{K}_t$,  $H_t=K^a_t$ and $H_t=\widehat{K^a_t}$,  Experiment $3$. We observe that, $\nu_t$ is smaller for the learned/adaptable gates when compared respectively to the case of non-learned/frozen gates. The behaviour of $\nu_t$ points to the fact that, when gates adapt, the underlying Gram matrices align their eigen-space in accordance with the labeling function.


\begin{comment}
\begin{figure}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c}
\includegraphics[scale=0.1]{figs/galu-learn-no-learn-knorm.png}
\end{tabular}
}
\caption{Shows $\nu_t=y^\top (H_t)^{-1}y$ for $H_t=K_t$.}
\label{fig:adapt-norm-1}
\end{figure}


\begin{figure}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c}
\includegraphics[scale=0.1]{figs/relu-froze-no-froze-knorm.png}
\end{tabular}
}
\caption{Shows $\nu_t=y^\top (H_t)^{-1}y$ for $H_t=\widehat{K}_t$.}
\label{fig:adapt-norm-2}
\end{figure}


\begin{figure}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c}
\includegraphics[scale=0.18]{figs/activation-gram-unnorm.png}
\end{tabular}
}
\caption{Shows $\nu_t=y^\top (H_t)^{-1}y$ for $H_t=K^a_t$. Here, $K^a_t$ is the Gram matrix of activations in the soft-GaLU network.}
\label{fig:adapt-norm-3}
\end{figure}


\begin{figure}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c}
\includegraphics[scale=0.18]{figs/activation-gram-norm.png}
\end{tabular}
}
\caption{Shows $\nu_t=y^\top (H_t)^{-1}y$ for $H_t=\widehat{K^a_t}$. Here, $K^a_t$ is the Gram matrix of activations in the soft-GaLU network.}
\label{fig:adapt-norm-4}
\end{figure}
\end{comment}


$\bullet$ \textbf{Experiment $7$:} We train convolution networks with soft-ReLU gates (i.e., $\chi_{\epsilon}(v)$) for various values of $\beta$ and $\epsilon$ on CIFAR-10 dataset (the architecture is same as the one described in \textbf{Experiment $6$}).  For moderately high values of $\beta$ we obtain generalisation performance of $72\%$ which is comparable to what we obtain for DNNs (identical architecture, and hyper-parameters) with standard ReLU activation.
\FloatBarrier
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|}\hline
Network
&Training Accuracy
&Test Accuracy\\\hline
Deep Linear Network
&0.5042
&0.3939\\\hline
Relu
&0.99
&0.71\\\hline

$\beta=$1, $\epsilon=$0.1
&0.99
&0.59\\\hline
$\beta=$2, $\epsilon=$0.1
&0.99
&0.64\\\hline
$\beta=$4, $\epsilon=$0.1
&0.99
&0.68\\\hline
$\beta=$8, $\epsilon=$0.1
&0.99
&0.71\\\hline
$\beta=$12, $\epsilon=$0.1
&0.99
&0.71\\\hline
$\beta=$16, $\epsilon=$0.1
&0.99
&0.72\\\hline
$\beta=$20, $\epsilon=$0.1
&0.99
&0.72\\\hline
$\beta=$24, $\epsilon=$0.1
&0.99
&0.72\\\hline
$\beta=$1, $\epsilon=$0.4
&0.99
&0.56\\\hline
$\beta=$2, $\epsilon=$0.4
&0.99
&0.63\\\hline
$\beta=$4, $\epsilon=$0.4
&0.99
&0.68\\\hline
$\beta=$8, $\epsilon=$0.4
&0.99
&0.71\\\hline
$\beta=$12, $\epsilon=$0.4
&0.99
&0.71\\\hline
$\beta=$16, $\epsilon=$0.4
&0.99
&0.71\\\hline
$\beta=$20, $\epsilon=$0.4
&0.99
&0.71\\\hline
$\beta=$24, $\epsilon=$0.4
&0.99
&0.71\\\hline
\end{tabular}
}
\caption{Shows performance of various networks for CIFAR 10 dataset. The results are averaged over $5$ runs. The reported results are mean of the best performance obtained in each run. The optimiser used was \emph{Adam} with step-size equal to $3e^{-4}$.}
\label{tb:cifar}
\end{table}




