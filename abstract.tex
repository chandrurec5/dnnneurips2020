\begin{abstract}
Understanding optimisation and generalisation in deep neural networks (DNNs) (with ReLU) trained using first-order method such as gradient descent (GD) is an important problem in machine learning. Recent works on this problem have made use of the \emph{neural tangent feature} (NTF) which is equal to the gradient of the DNN output with respect to its weights, i.e., the feature is based on first-order information. In this paper, we deconstruct the computation in DNNs into paths and gates. This deconstruction yields a novel zeroth-order feature namely the \emph{neural path feature} (NPF), whose dimension is equal to total the number of paths, and whose co-ordinate corresponding to a path is the product of the signal at its input node and its \emph{on/off} state\footnote{A path is \emph{on} if all the gates in the path are \emph{on}.}. We show that when optimising with fixed NPFs using GD i) increasing depth till a point helps in training and ii) increasing depth beyond a point hurts training. While in NTFs based works, DNNs are considered as linear learners with fixed NTF, we show via theory and experiments, that, in DNNs, NPFs are learnt during training, and such learning is key for generalisation. By bringing in zeroth-order information into the picture via NPFs, our work is complementary to prior work based on NTFs.
\end{abstract}
\begin{comment}
We study optimisation and generalisation in deep gated networks (DGNs), wherein, the output of a single hidden neuron is obtained as a product of its pre-activation input and a gating value. DNNs with ReLU activations are special cases of DGNs.  The DGN framework enables us to dismantle deep networks into paths and gates. This dismantling yields a novel (zeroth-order) \emph{neural path feature} (NPF) whose dimension is equal to total the number of paths, and whose co-ordinate corresponding to a path is the product of the signal at its input node and its \emph{on/off} state\footnote{A path is \emph{on} if all the gates in the path are \emph{on}.}. We show that the associated \emph{neural path kernel} (NPK) plays an important role in optimisation and generalisation of DGNs. %In particular, $H_t(x,x')=(x^\top x')\lambda_t(x,x')$, where $\lambda_t(x,x')$ is a count of total number of paths that are active simultaneously for both inputs $x,x'\in \R^{d_{in}}$. \hfill\\
For randomly initialised DGNs optimising with fixed NPFs, we show that i) increasing depth till a point helps in training and ii) increasing depth beyond hurts training. We verify via experiments that the NPFs are learned during training by showing that the norm of the labelling function measured with respect to the inverse of the trace normalised NPK reduces with time.
\end{comment}