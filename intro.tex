\section{Introduction}
Understanding optimisation and generalisation in deep neural networks (DNNs) trained using first-order method such as gradient descent (GD) is an important problem in machine learning. Despite having a non-convex loss surface, GD achieves zero training error in over-parameterised DNNs where the number of parameters exceeds the size of the dataset. Interestingly, \cite{ben} demonstrated that practical DNNs have enough capacity to achieve zero training loss with even random labelling of standard datasets such as MNIST and CIFAR. However, when trained with true labels, such networks achieve zero training error and also exhibit good performance on test data. 

In this paper, we consider fully connected DNNs with ReLU activations, with $d$ layers and $w$ hidden units per layer. In what follows ,we denote the dataset by $(x_s,y_s)_{s=1}^n\in \R^{d_{in}}\times \R$, and the parameters of the network by $\Theta\in \R^{d_{net}}$, the network output for an input $x\in \R^{d_{in}}$ by $\hat{y}_{\Theta}(x)$.

Some of the recent works (\citenum{dudnn,du2018,arora2019exact,dudln}) on this problem have made use of the \emph{neural tangent features} (NTFs) and the \emph{trajectory based analysis}, which we describe in brief. The NTF of an input $x\in \R^{d_{in}}$ is defined as $\psi_{x,\Theta}=\left(\partial_{\theta}\hat{y}_{\Theta}(x),\theta\in\Theta\right)\in\R^{d_{net}}$\footnote{Here $\partial_{\theta}(\cdot)$ stands for $\frac{\partial (\cdot)}{\partial \theta}$}, i.e., the gradient of the network output with respect to the weights. By collecting the NTFs of all the inputs examples in the dataset, we can form the NTF matrix $\Psi=(\psi_{x_s,\Theta},s\in[n])\in\R^{d_{net}\times n}$\footnote{$[n]=\{1,\ldots,n\}$}. The trajectory based analysis looks at the dynamics of the error $e_t=(\hat{y}_{\Theta_t}-y_s,s\in[n])\in\R^n$. For a small enough step-size $\alpha_t>0$ of the GD procedure, the error dynamics is given by:
\begin{align}
e_{t+1}=e_t-\alpha_t K_{\Theta_t} e_t,
\end{align}
where $K_{\Theta_t}\in\R^{n\times n}$ is the \emph{neural tangent kernel} (NTK) matrix give by $K_{\Theta}=\Psi^\top_{\Theta} \Psi_{\Theta}$. Thus, the spectral properties, and in particular, $\rho_{\min}(K_{\Theta_t})$ the minimum eigenvalue of $K_{\Theta_t}$ dictates the rate of convergence. Under randomised initialisation, and in the limit of infinite width an interesting property emerges: the parameters of the DNN deviate very little during training, i.e. $\Theta_t\approx \Theta_0$. In particular, $K_{\Theta_t}\approx K_{\Theta_0}$, $K_{\Theta_0}\ra K^{(d)}$, i.e., the NTK stays almost constant through training and the NTK matrix at initialisation $K_{\Theta_0}$ converges to a deterministic matrix $K^{(d)}$ (see \Cref{sec:exact} for exact expression of $K^{(d)}$. Thus in the `large-width' regime, zero training error can be achieved if $\rho_{\min}(K^{(d)})>0$ which holds as long as the training data is not degenerate (\citenum{dudnn,arora2019exact}). In the `large-width' regime, \cite{arora2019exact} show that the fully trained DNN is equivalent to kernel regression with $K^{(d)}$. Hence, a trained DNN enjoys the generalisation ability of its corresponding $K^{(d)}$ matrix in the `large-width' regime. \cite{cao2019generalization} show that in the `large-width' regime, the DNN is almost a linear learner with the random NTFs, and showed a generalisation bound in the form of $\tilde{\mathcal{O}}\left(d\cdot\sqrt{y^\top K^{(d)} y/n}\right)$\footnote{$a_t=\mathcal{O}(b_t)$ if $\lim\sup_{t\ra\infty}|a_t/b_t|<\infty$, and $\tilde{\mathcal{O}}(\cdot)$ is used to hide logarithmic factors in $\mathcal{O}(\cdot)$.}, where $y=(y_s,s\in[n])\in\R^n$ is the labelling function.

\textbf{Research Gap I (Feature Learning):} In the `large-width' i.e., fixed NTF/NTK regime, the DNNs are linear learner using the random NTFs at random initialisation. This implies that there is little or no feature learning. Is this true?\\
\textbf{Research Gap II (Finite vs Infinite):} \cite{arora2019exact} note that, while pure-kernel methods based on the limiting NTK (i.e., $K^{(d)}$) outperform other state-of-the-art kernel methods, the finite width DNNs (CNNs) still outperform their NTK (CNTK)\footnote{CNTK: Convolutional Neural Tangent Kernel, the NTK for CNNs} counterpart. Can we explain this gap?
\begin{comment}
\subsection{Contributions}
$\bullet$ (\Cref{sec:pathgate}) Central idea in this paper is the 'path-view', wherein, we decompose the computation in such DNNs into paths and gate. Intuitively speaking, when a particular example is presented to the network only a sub-set of the activations \emph{on} and the output is obtained as the sum of the contribution of the various paths in the sub-network formed by such \emph{on} gates. We explicitly encode the state of the gates in a novel \emph{neural path feature} (NPFs), which is zeroth-order feature. 
Using the `path-view', we  encode the state of the gates in a novel \emph{neural path feature} (NPFs), and the weights in a novel \emph{neural path value} (NPV), and express the output as an inner product of the NPV and NPF. This reveals the presence to learning problems namely i) learning NPVs for fixed NPFs and ii) learning NPFs themselves.\\
Using the `path-view' we characterise the information flow via two different sub-networks, i) of active gates which are on and ii) of sensitive gates which are on the verge of turning on or off. 
$\bullet$ (\Cref{sec:kernel}) We also show the NTK can be decomposed into $K_{\Theta}=K^v_{\Theta}+K^{\phi}_{\Theta}+K^{cross}_{\Theta}$, where $K^v_{\Theta}$ is the NTK of values responsible for optimisation with fixed features, and $K^{\phi}_{\Theta}$ is the NTK of features responsible for feature learning, and $K^{cross}_{\Theta}$ is a symmetric matrix of cross terms. \\
$\bullet$ (\Cref{sec:optimisation})We study a novel fixed NPF regime. Here, we show that as width increases to infinity the optimisation and generalisation properties are captured by the \emph{neural path kernel} (NPK), the kernel associated with the NPFs. In the case of finite width,  we show that when optimising  using GD i) increasing depth till a point helps in training and ii) increasing depth beyond a point hurts training.\\
$\bullet$ (\Cref{sec:generalisation}) We show via experiments, that NPFs are learnt during training, and such learning improves generalisation performance.
\end{comment}