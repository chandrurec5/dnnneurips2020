\section{Introduction}
Understanding optimisation and generalisation in deep neural networks (DNNs) trained using first-order method such as gradient descent (GD) is an important problem in machine learning. Some of the recent works on this problem have made use of the \emph{neural tangent features} (NTFs) and the \emph{trajectory} based analysis. The NTF of an input $x\in \R^{d_{in}}$ is defined as $\psi_{x,t}=\left(\frac{\partial \hat{y}_{\Theta}(x)}{\partial \theta}|_{\Theta=\Theta_t},\theta \in \Theta\right)\in \R^{d_{net}}$, i.e., as the gradient of the network output $\hat{y}_{\Theta}(x)$ with respect to its weights $\Theta\inrdnet$. Since the NTF is a first-order term, it can be used to linearise the DNN output about $\Theta_t$. Associated with the NTF is a \emph{neural tangent kernel} (NTK), which is given by $K_t(x,x')=\ip{\psi_{x,t},\psi_{x',t}}$\footnote{For $x,y\in\R^d \ip{x,y}=x^\top y$.}, where $x,x'\in\R^{d_{in}}$ are inputs to the DNN. In the \emph{trajectory} based analysis, one looks at the dynamics of error $e_t$  (i.e., difference between predicted and true values) at time $t$. For a small step-size $\alpha_t>0$ of the GD procedure, the error dynamics follows a recursion given by: $e_{t+1}=e_t-\alpha_tK_te_t$, where $K_t$ is the NTK matrix obtained on the the dataset. It is known from \cite{dudnn}, that, gradient descent achieves zero training loss in over-parameterised DNNs, and from \cite{cao2019generalization}, that, the NTK at initialisation can be used to bound the generalisation performance.\\
\begin{comment}
 the following questions still remain unresolved \hfill\\
\textbf{Question I (Optimisation):} \emph{What is the role of width and depth in training of DNNs? Why increasing depth till a point helps in training? Why increasing the depth beyond hurts training?}\\
The results by \cite{dudnn} only throw light on why ResNets could be better than fully connected DNNs. The above question in relation to fully connected networks is still unresolved.\\
\textbf{Question II (Generalisation):} \emph{What are the hidden features in a DNN? Are these features learned? and if so, how?} \hfill\\
There are couple of unresolved issues in prior work by \cite{arora2019exact,cao2019generalization}. Firstly, if the DNNs are only linear learners with random NTFs, then it means that, no feature learning happens in DNNs. Secondly, it was observed in prior experiments, that, the DNNs perform better than their corresponding NTK counterparts \cite{arora2019exact,lee2019wide}. \hfill\\
\end{comment}
\textbf{Our Contributions:} 


 In this paper, we study fully connected deep gated networks (DGNs), with $d$ layers, and $w$ hidden units per layer. In a DGN,  the output of a single hidden unit is given by the product of its pre-activation input and its gating value. DNNs with ReLU activations are special cases of DGNs. Central idea in this paper is the `path-view': to regard paths and gates as basic building blocks of a DGN. A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Let $[P]=\{1,\ldots,P\}$ be an enumeration of all the paths, the zeroth and first order quantities of a DNN can then be expressed as:
\begin{align}
\label{eq:zero}&\text{(Zeroth-Order/Neural Path Feature)}&\quad:& \quad \hat{y}_t(x)={\phi^\top_{x,t}} v_t=\sum_{p\in [P]}x(\I(p))A_t(x,p)v_t(p)\\
%\text{(First Order)}\quad: \quad \partial \hat{y}_t(x)&= \sum_{p\in [P]}x(\I(p)) A_t(x,p) \partial((v_t(p))+\sum_{p\in [P]}x(\I(p)) \partial(A_t(x,p)) v_t(p)\nn\\
\label{eq:first}&\text{(First- Order/Neural Tangent Feature)}&\quad:&\quad   \nabla \hat{y}_t(x)=\psi_{x,t}=(\nabla v_t )^\top \phi_{x,t} + (\nabla \phi_{x,t})^\top v_t,
\end{align}
where, for a path $p$, $\I(p)$ is the input node at which it starts, $v_t(p)$ is its value given by the product of its weights and for input $x\in\R^{d_{in}}$, $A_t(x,p)$ is its activation level given by the product of the gating values in the path. In order to understand the role $A_t$, we handle the gating values as independent variables\footnote{In a DNN with ReLU activations, the gates are implicit: a gate is \emph{on} only if pre-activation input is positive.}. We now list the major highlights in the paper.\hfill\\
$1.$ \textbf{Information Flow:} We introduce a host of novel terms/expressions and concepts related to information flow in DGNs. This includes i) the zeroth-order (see \eqref{eq:zero}) \emph{neural path feature} (NPF) and the \emph{neural path kernel} (NPK) denoted by $H_t$, which are defined using the gating information, ii) the first-order (see \eqref{eq:first}) terms namely, the \emph{value gradient} given by $(\nabla v_t )^\top \phi_{x,t}$, and the \emph{feature gradient} given by $(\nabla \phi_{x,t})^\top v_t$, iii)  two sub-network of paths namely, an \emph{active} sub-network, which is the set of active paths that hold the memory for an input, through which the value gradient flows and a \emph{sensitive} sub-network, which is the set of sensitive paths, through which the feature gradient flows. We also show that for random initialisation, the paths exhibit a \emph{disentanglement} property, which is very key in our results.\\
%$1.$ \textbf{Representation and Generalisation:} By fixing the activation (i.e., $A_t=A_0,\forall t\geq 0$), we train DGNs with different fixed \emph{neural path features} (NPFs), and demonstrate that the NPFs are key for generalisation. Further, we show that the NPFs are learnt during training. This shows that NPFs can be regarded as the true hidden features in a deep network.\\
$2.$ \textbf{Neural Path Features} are the information stored in the gates of a deep network. Our main result connects the NPK to the NTK, i.e., we show that, when the weights are randomly initialised (with an appropriate scale) in a manner statistically independent of the gates, i)  $\E{K_0}=H_0$, and ii) $Var\left[K_0(x,x')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$.\\
$\bullet$ \textbf{Optimisation:}   We argue that  $H_0$ whitens with depth, and hence, increasing depth helps in training. Increasing depth further causes $K_0$ to deviate from $H_0$, and hence, degrades the spectrum of $K_0$, thereby affecting training performance.\\
$\bullet$ \textbf{Generalisation:} By fixing the activation (i.e., $A_t=A_0,\forall t\geq 0$), we train DGNs with different fixed NPFs, and demonstrate that the NPFs are key for generalisation. Further, we show that the NPFs are learnt during training. This shows that NPFs can be regarded as the true hidden features in a deep network.\\
$3.$ \textbf{NPF Learning:} We present preliminary results connecting NPF learning and the trajectory method.

\begin{comment}
Understanding optimisation and generalisation of deep neural networks (DNNs) trained using first-order method such as (stochastic) gradient descent is an important problem in machine learning. In this paper, we throw light on the following two questions:\hfill\\
\textbf{Question I (Optimisation):} \emph{What is the role of width and depth in training of DNNs? Why increasing depth till a point helps in training? Why increasing the depth beyond hurts training?}\\
\textbf{Question II (Generalisation):} \emph{What are the hidden features in a DNN? Are these features learned? and if so, how?} \hfill\\

\textbf {Background:} We consider a fully connected deep gated network (DGN) of depth $d$, and width $w$, weights $\Theta\in\R^{d_{net}}$, which, accepts an input $x\in \R^{d_{in}}$ and produces an output $\hat{y}_{\Theta_t}(x)\in \R$. In a DGN, the output of a hidden neuron is obtained as a product of its pre-activation input and a gating value. DNNs with ReLU activations are special cases DGNs. Some of the recent works to understand SGD in relation to the optimisation and generalisation in DNNs make use of the \emph{neural tangent features} (NTFs) and the \emph{trajectory} based analysis, which we describe in brief.\hfill\\
$1.$The NTF for an input $x\in \R^{d_{in}}$ is defined as $\psi_{x,t}=(\frac{\partial \hat{y}_{\Theta}}{\partial \theta}|_{\Theta=\Theta_t},\theta \in \Theta)\in \R^{d_{net}}$, i.e., collection of the gradients of the network output with respect to its weights. Since the NTF is a first-order term, it can be used to linearise the DNN output about the point $\Theta_t$. Associated with the NTF is a \emph{neural tangent kernel} (NTK), which is given by $K_t(x,x')=\psi^\top_{x,t}\psi_{x',t}$. 
$2.$ In the \emph{trajectory} based analysis, one looks at the dynamics of error $e_t$  (i.e., difference between predicted and true values) at time $t$. For a small step-size $\alpha_t>0$, the error dynamics follows a linear recursion given by: $e_{t+1}=e_t-\alpha_tK_te_t$, where $K_t$ is the NTK matrix obtained on the the dataset. Thus, the spectral properties of $K_t$ is key to achieve zero training error. \hfill\\
\textbf{Related Work and Gaps:} \hfill\\
\emph{Optimisation:} \cite{ntk} were the first to point out the role of NTK in DNNs. Using the trajectory based analysis, \cite{dudnn} show that in fully connected DNNs with $w=\Omega(poly(n)2^{O(d)})$, and in residual neural networks (ResNets) with $w=\Omega(poly(n,d))$ gradient descent converges to zero training loss.  However, Question I, i.e., `why depth helps in training?' is unresolved.\hfill\\%This shows that ResNets are better than fully connected DNNs, based on the fact that the dependence on the number of layers improves exponentially for ResNets. 
\emph{Generalisation:} \cite{arora2019exact,arora,cao2019generalization} use NTK to provide generalisation bounds as well as propose pure-kernel methods. However, couple of issues remain unresolved: firstly, if the DNNs are only linear learners with random NTFs, then it suggests that no feature learning happens in DNNs, and secondly, it was observed in prior experiments that the DNNs perform better than their corresponding NTK counterparts \cite{arora2019exact,lee2019wide}. \hfill\\

\textbf{Our Results:} We now highlight the contributions in this paper.\\
$1.$ \textbf{Neural Path Feature:} Central idea in this paper is the `path-view': to regard paths and gates as basic building blocks of a DGN.  A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Let $[P]=\{1,\ldots,P\}$ be an enumeration of all the paths, then the \emph{neural path feature} (NPF) is defined $\phi_{x,t}=(x(\I(p))A_t(x,p),p\in[P])\in\R^P$, wherein, for a path $p$, $\I(p)$ is the input node at which it starts, and  $A_t(x,p)$ is its activation level is equal to the product of the gating values in the path. Using the NPF, the output is expressed as $\hat{y}_t(x)=\phi^\top_{x,t}v_t$, where, $v_t=(v_t(p),p\in[P])\in \R^P$, with $v_t(p)$ is the value of a path $p$ and is equal to the product of its weights.\hfill\\
$2.$ \textbf{Neural Path Kernel (NPK)}  is given by $H_t(x,x')=\phi^\top_{x,t}\phi_{x',t}$, where $x,x'\in\R^{d_in}$ are the inputs to the DGN. We show that the NPK whitens as depth increases.\hfill\\
$3.$ \textbf{Optimisation:}  When weights are initialised at random (with variance $\sigma$) and statistically independent of the activations, we show that $\E{K_0}= CH_0$, (for some constant $C>0$) and  $Var\left[K_0(x,x')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$ (for $\sigma^2=O(\frac{1}w)$). Thus,\\
(i) For a fixed $d$, increasing $w$ ensures $K_0$ converges to $\E{K_0}$, and since NPK matrix whitens as depth increases, increasing depth till a point helps in training performance.\hfill\\
(ii) For a fixed $w$, increasing $d$ makes the entries of $K_0$ deviate from $\E{K_0}$, thus degrading the spectrum of $K_0$. Thus increasing depth beyond a point hurts training performance. \hfill\\
$4.$ \textbf{Feature Learning:} Since $\hat{y}_t(x)=\phi^\top_{x,t}v_t$, the gradient flow has two components: an $(\nabla v_t)^\top\phi_{x,t}$ term, which we call the \emph{value gradient}, learns the path values (which are akin to weight vector in a standard linear approximation) keeping the NPFs fixed, and a $(\nabla \phi_{x,t})^\top v_t $ term, which we call the \emph{feature gradient} learns the NPFs keeping the path values fixed. We present preliminary theory connecting the trajectory method and feature learning. \hfill\\
$5.$ \textbf{Generalisation:} We experiment with two gating regimes namely the static, wherein, $A_t=A_0,\forall t \geq 0$, and the dynamic, wherein $A_t$ changes during training. The experiments on MNIST and CIFAR show that, better generalisation happens when the activations $A_t$ change with time. In particular, for binary classification, we verify that the quantity $y^\top (\widehat{H_t})^{-1}y$\footnote{For a matrix $H$, $\hat{H}=\frac{1}{trace(H)}H$ .} reduces as the training progresses. This shows that the eigenspaces of the NPK matrix align themselves in accordance to the labelling function. These experiments supports the fact that NPF and the NPK are indeed learnt during training in a manner to improve the generalisation performance. \hfill\\
\end{comment}
\begin{comment}
\textbf{Paths:}  A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Let $[P]=\{1,\ldots,P\}$ be an enumeration of all the paths, the zeroth and first order quantities of a DNN can then be expressed as:
\begin{align}
\label{eq:zero}\text{(Zeroth Order)}\quad: \quad \hat{y}_t(x)&=\sum_{p\in [P]}x(\I(p))A_t(x,p)v_t(p)={\phi^\top_{x,t}} v_t\\
\text{(First Order)}\quad: \quad \partial \hat{y}_t(x)&= \sum_{p\in [P]}x(\I(p)) A_t(x,p) \partial((v_t(p))+\sum_{p\in [P]}x(\I(p)) \partial(A_t(x,p)) v_t(p)\nn\\
\label{eq:first}\psi_{x,t}=(\partial v_t)^\top\phi_{x,t}+ (\partial \phi_{x,t})^\top v_t,
\end{align}
where, for a path $p$, $\I(p)$ is the input node at which it starts, $v_t(p)$ is its value given by the product of its weights and for input $x\in\R^{d_{in}}$, $A_t(x,p)$ is its activation level given by the product of the gating values in the path. \hfill\\
\textbf{Independent Gates and Activations:} In order to understand the role $A_t$, we handle the gating values as independent variables\footnote{In DNN with ReLU, the gates are implicit: a gate is \emph{on} only if pre-activation input is positive.}. All the theoretical results are under the assumption that $A_0$ is statistically independent of $\Theta_0$. \hfill\\
\textbf{Neural Path Feature and Kernel:} The \emph{neural path feature} (NPF) is given by $\phi_{x,t}=(x(\I(p))A_t(x,p))\in \R^P$ (see \eqref{eq:zero}) , and an associated \emph{neural path kernel} (NPK) to $H_t(x,x')=\phi^\top_{x,t}\phi_{x',t}$. The NPK has special structure: $H_t(x,x')=(x^\top x')\lambda_t(x,x')$, where $\lambda_t(x,x')$ is a measure of similarity based on the path activation levels for inputs $x,x'\in\R^{d_{in}}$. \hfill\\
\textbf{NPK vs NTK: } Under symmetric Bernoulli initialisation, we have $\E{K_0} = C H_0$, ($C>0$ is a constant), where $K_0$ is the NTK matrix at $t=0$. \hfill\\
\textbf{Optimisation I:} We argue that $\frac{\lambda_0(x,x')}{\lambda_0(x,x)}$ decays at an exponential rate with depth, i.e., the Gram matrix $K_0$ whitens with depth. Thus increasing depth helps in training. \hfill\\
\textbf{Optimisation II:} We show that $Var\left[K_0(x,x')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$ (for $\sigma^2=O(\frac{1}w)$). For a fixed $d$, increasing $w$ ensures $K_0$ converges to $\E{K_0}$. However, for a fixed $w$, increasing $d$ makes the entries of $K_0$ deviate from $\E{K_0}$, thus degrading the spectrum of $K_0$. Thus increasing depth beyond a point hurts training performance. \hfill\\
\textbf{Generalisation:} We experiment with two gating regimes namely the static, wherein, $A_t=A_0,\forall t \geq 0$, and the dynamic, wherein $A_t$ changes during training. The experiments on MNIST and CIFAR show that, better generalisation happens when the activations $A_t$ change with time. In particular, for binary classification, we verify that the quantity $y^\top (\widehat{H_t})^{-1}y$\footnote{For a matrix $H$, $\hat{H}=\frac{1}{trace(H)}H$ .} reduces as the training progresses. This shows that the eigenspaces of the NPK matrix align themselves in accordance to the labelling function. These experiments supports the fact that NPF and the NPK are indeed learnt during training in a manner to improve the generalisation performance. \hfill\\
\end{comment}