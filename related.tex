\section{Related Work}
\cite{dudnn} show that in fully connected DNNs with $w=\Omega(poly(n)2^{O(d)})$, and in residual neural networks (ResNets) with $w=\Omega(poly(n,d))$ gradient descent converges to zero training loss. \cite{dudnn} claim to demystify the second part of what we called the depth phenomena (``why deeper networks are harder to train"), since, the dependence on the number of layers improves exponentially for ResNets. Our optimisation results are weaker than \cite{dudnn} in the sense that we consider only DGNs with decoupling assumptions. However, we show both parts of the depth phenomena, in particular why increasing depth till a point helps training. 

\begin{comment}Further, the \emph{algebraic} nicety due to \emph{Hadamard} product decomposition of the Gram matrix is a useful take away. In addition, the connection to how the sub-network overlap is a conceptual gain. 
In comparison to \cite{dudln} the gain in our work is that, thanks to the path-view, we obtain a single expression for $\E{K_0}$ which can be applied to deep linear networks, GaLU networks and any networks whose gating values are known and fixed. Both \cite{dnn,dln} are analytically more involved, in that they provide guaranteed rates of converges with high probability, and in comparison, our work has stopped with the variance calculation.
\end{comment}
In comparison to \cite{sss} who were the first to initiate the study on GaLU networks, we believe, our work has made significant progress. We introduced adaptable gates, and showed via experiments, that, gate adaptation is key in learning, thereby showing a clear separation between GaLU and ReLU networks. To support the claim, we have used idea from \cite{arora}, in that, we measure $\nu_t=y^\top {K_t}^{-1}y$ to show that the eigen spaces indeed align with respect to the labelling function.

In comparison to \cite{lottery}, we also show in our experiments that the winning lottery is in the gating pattern, which, in the case of ReLU networks is inseparable from the weights. However, our experiments show that the weights can be reinitialised if we have the learned gating pattern.

\textbf{Neural tangent feature} (NTF) is the collection of the output gradients with respect to the network parameters, and is given by $\psi_{x_s,t}=(\nabla_{\Theta|_{\Theta=\Theta_t}} \hat{y}_{\Theta}(x_s))\in\R^{d_{net}}$.  The NTF matrix can be used to linearise the output of a DNN about the point $\Theta_t\inrdnet$. To see this, let the NTF matrix be $\Psi_t=[\psi_{x_1,t},\ldots, \psi_{x_n,t}]\in\R^{d_{net}\times n}$, then a linearisation of the DNN output about $\Theta_t$ is given by: $\hat{y}_{\Theta_t+\Theta}=\hat{y}_{\Theta_t} + \Psi^\top_t (\Theta-\Theta_t)$. An associated \emph{neural tangent kernel} is then given by $K_t=\Psi^\top_t\Psi_t$.\\
\textbf{Trajectory Method:} Recent works have made use of the trajectory method to show that gradient descent achieves zero training error in over-parameterised DNNs. In the \emph{trajectory} based analysis, one looks at the dynamics of error $e_t$  (i.e., difference between predicted and true values) at time $t$. For a small step-size $\alpha_t>0$, the error dynamics follows a linear recursion given by: $e_{t+1}=e_t-\alpha_tK_te_t$, where $K_t$ is the NTK matrix obtained on the the dataset. Thus, the spectral properties of $K_t$ is key to achieve zero training error. \hfill\\
\textbf{Optimsation:} \cite{ntk} were the first to point out the role of NTK in DNNs. Using the trajectory based analysis, \cite{dudnn} show that in fully connected DNNs with $w=\Omega(poly(n)2^{O(d)})$, and in residual neural networks (ResNets) with $w=\Omega(poly(n,d))$ gradient descent converges to zero training loss.\\
\textbf{Research Gap I:} The result by \cite{dudnn} shows that ResNets are better than fully connected DNNs, based on the fact that the dependence on the number of layers improves exponentially for ResNets. However, Question I (see \Cref{sec:intro}), i.e., `why depth helps in training?' is unresolved.\\
\textbf{Generalisation:} Prior works by \cite{arora2019exact,cao2019generalization} suggest that, for randomised initialisation, DNNs can be thought of as learning with the linear features given by the random NTFs, and provide generalisation bounds with the corresponding NTK. They use NTK at initialisation to provide generalisation bounds as well as propose pure-kernel methods. However, couple of issues remain unresolved: firstly, if the DNNs are only linear learners with random NTFs, then it suggests that no feature learning happens in DNNs, and secondly, it was observed in prior experiments that the DNNs perform better than their corresponding NTK counterparts \cite{arora2019exact,lee2017deep}. \hfill\\
\textbf{Research Gap II:} However, couple of issues remain unresolved: firstly, if the DNNs are only linear learners with random NTFs, then it suggests that no feature learning happens in DNNs, and secondly, it was observed in prior experiments that the DNNs perform better than their corresponding NTK counterparts \cite{arora2019exact,lee2017deep}.

