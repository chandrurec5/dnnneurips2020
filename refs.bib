


@article{dln,
  title={A convergence theory for deep learning via over-parameterization},
 author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1811.03962},
  year={2018}
}
@article{shamir,
  title={Exponential convergence time of gradient descent for one-dimensional deep linear neural networks},
  author={Shamir, Ohad},
  journal={arXiv preprint arXiv:1809.08587},
  year={2018}
}


@article{ando,
  title={Majorization relations for Hadamard products},
  author={Ando, T},
  journal={Linear algebra and its applications},
  volume={223},
  pages={57--64},
  year={1995},
  publisher={Elsevier Science Publishing Company, Inc.}
}


@article{ben,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}



@article{sss,
  author    = {Jonathan Fiat and
               Eran Malach and
               Shai Shalev{-}Shwartz},
  title     = {Decoupling Gating from Linearity},
  journal   = {CoRR},
  volume    = {abs/1906.05032},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.05032},
  archivePrefix = {arXiv},
  eprint    = {1906.05032},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1906-05032},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{arora,
  author    = {Sanjeev Arora and
               Simon S. Du and
               Wei Hu and
               Zhiyuan Li and
               Ruosong Wang},
  title     = {Fine-Grained Analysis of Optimization and Generalization for Overparameterized
               Two-Layer Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1901.08584},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.08584},
  archivePrefix = {arXiv},
  eprint    = {1901.08584},
  timestamp = {Mon, 25 Nov 2019 14:34:49 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-08584},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{ganguli,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}


@article{shamir,
  title={Exponential convergence time of gradient descent for one-dimensional deep linear neural networks},
  author={Shamir, Ohad},
  journal={arXiv preprint arXiv:1809.08587},
  year={2018}
}

@article{dudnn,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal={arXiv preprint arXiv:1811.03804},
  year={2018}
}


@article{dudln,
  title={Width provably matters in optimization for deep linear neural networks},
  author={Du, Simon S and Hu, Wei},
  journal={arXiv preprint arXiv:1901.08572},
  year={2019}
}


@article{arora,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  journal={arXiv preprint arXiv:1901.08584},
  year={2019}
}


@inproceedings{ntk,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}


@article{lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}


@article{edgepop,
  title={What's Hidden in a Randomly Weighted Neural Network?},
  author={Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  journal={arXiv preprint arXiv:1911.13299},
  year={2019}
}