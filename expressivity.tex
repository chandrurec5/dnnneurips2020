\section{Features and Kernels: Zeroth- vs First-Order}
\textbf{Zeroth-Order:} We first discuss zeroth-order feature and kernel, and then connect it to the first-order features and kernel.\hfill\\
The \emph{neural path feature} of an input $x\in \R^{d_{in}}$ is given by $\phi_{x,t}=(x(p(0))A_t(x,p) ,p\in[P])\in\R^P$. By arranging the NPF of the $n$ input examples in a matrix $\Phi_t=\left[\phi_{x_1,t},\ldots, \Phi_{x_n,t}\right]$, we can express the predicted output of a DGN as: \begin{align}\label{eq:npfbasic}\hat{y}_t=\Phi_t^\top v_t,\end{align}
where, the value of the path $v_t$ is the equivalent of the so called \emph{weight-vector} in a standard linear approximation. The significance of the NPF are:\hfill\\
$1.$ \textbf{Signal-Wire Separation}: Note that $\Phi$ encodes the signal: say for a DNN with ReLU activations, the co-ordinate corresponding to path $p$ is either $x(p(0))$ if the path is active for that input (i.e., $A_t(x,p)=1$) or $0$ if the path is inactive for that input  (i.e., $A_t(x,p)=0$). The value of the path thus encodes the \emph{wire}, i.e., the information contained in the weights of the network. \hfill\\
$2.$ \textbf{Deep Information Propagation:} The path view provides a novel way of looking at information propagation in DNNs, eschewing the conventional `layer-by-layer' expression for information flow.\hfill\\
\textbf{Sub-network Overlap:} For any $i\in [d_{in}]$ define $\lambda_t(s,s')\stackrel{def}{=}\sum_{p\rsa i} A_t(x_s,p) A_t(x_{s'},p)$ to be the measure of activation similarity of the paths for inputs $s,s'\in[n]$. Here, $\lambda_t$ counts the total number of paths that are simultaneously \emph{on/active} for both inputs $s,s'\in[n]$. Note that the definition of $\lambda_t$ is independent of $i\in [d_{in}]$: owing to symmetry the same number of paths start from any given input node, and see exactly the same gates in the subsequent layers.

\begin{lemma}[Neural Path Kernel] Let $x=(x_s,s\in [n])\in\R^{d_{in}\times n}$ be the data matrix and let the neural path kernel be defined as $H_t\stackrel{def}=\Phi^\top_t\Phi_t$. It follows that $H_t= (x^\top x)\odot(\lambda_t)$ \end{lemma}
\textbf{First-Order:} The \emph{neural tangent feature} $\psi_{x,t}\in \R^{d_{net}}$ is given by the gradient of the output with respect to the network parameters. From \eqref{eq:npfbasic} it follows that $\psi_{x,t}=(\partial \phi_{x,t})^\top v_t +\phi^\top_{x,t}\partial v_t$. In what follows (in the current section and next section), we assume that the gates are frozen, i.e., $\G_t=\G_0, A_t(\cdot,\cdot)=A_0(\cdot,\cdot),\forall t\geq 0$, and hence $\partial \phi_{x,t}=0$.\hfill\\
\textbf{Value Tangent Feature:} For a path $p$, the gradient  $\partial v_t(p)$ is given by: \begin{align}\label{eq:vft}{\partial v_t(p)}/{\partial \Theta\left(l,\I_{l'-1}(p),\I_{l'}(p)\right)}|_{\Theta=\Theta_t}= \underset{l=1}{\underset{l\neq l'}{\overset{d}{\Pi}}} \Theta_t\left(l,\I_{l-1}(p),\I_{l}(p)\right)\end{align} The gradient of the value of a path $p$ with respect to the $d_{net}$ parameters can be collected in a \emph{value tangent feature} (VTF) denoted by $\varphi_{p,t}\in \R^{d_{net}}$. Note that, from \eqref{eq:vft} it follows that the VTF entry corresponding to a weight is $0$ if the path $p$ does not pass through that weight, and hence $\varphi_{p,t}$ has a maximum of $d$ non-zero entries.\hfill\\
\textbf{Inter-path interaction} term is given by $\kappa_t(x,x',i)\stackrel{def}=\underset{p,p'\rsa i}{\underset{{p,p'\in [P]:}}\sum} A_{\G_t}(x,p) A_{\G_t}(x',p') \ip{\varphi_{p,t}, \varphi_{p',t}}$. 

