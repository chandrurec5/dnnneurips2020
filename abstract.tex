\begin{abstract}
Understanding optimisation and generalisation in deep neural networks (DNNs) with ReLU activations trained using first-order method such as gradient descent (GD) is an important problem in machine learning. To tackle this problem, we introduce a novel `path-view': we decompose the computation in such DNNs into paths and gates. Here, we capture the state of the gates in a novel zeroth-order feature namely the \emph{neural path feature} (NPF), whose dimension is equal to the total number of paths from the input to the output. Our work is related and complementary to prior work based on \emph{neural tangent features and kernels} which are based on the first-order gradient information. 

Using the `path-view', we present a description of GD dynamics that incorporates NPF learning. We introduce a novel fixed NPF regime, in which, in the limit of infinite width, optimisation and generalisation properties can be tied down to an associated \emph{neural path kernel}. In the fixed NPF regime, in the finite width case, we show that when optimising  using GD i) increasing depth till a point helps in training and ii) increasing depth beyond a point hurts training.  We show via experiments, that NPFs are learnt during training, and such learning improves generalisation performance.
\end{abstract}
\begin{comment}
\begin{abstract}
Understanding optimisation and generalisation in deep neural networks (DNNs) trained using first-order method such as gradient descent (GD) is an important problem in machine learning. Recent works have studied this problem under the `extra-width' regime, wherein, the width of the DNN far exceeds the number of data points. A counter-intuitive property of this regime is that the parameters of the DNN deviate very little during training, and as a result of which feature learning is not accounted. In this paper, we consider DNNs with ReLU activations, and decompose the computation in such DNNs into paths and gates. This decomposition yields a novel zeroth-order feature namely the \emph{neural path feature} (NPF), whose dimension is equal to total the number of paths, and whose co-ordinate corresponding to a path is the product of the signal at its input node and its \emph{on/off} state\footnote{A path is \emph{on} if all the gates in the path are \emph{on}.}.  We present a description of GD dynamics that incorporates NPF learning. We introduce a novel fixed NPF regime, in which, in the limit of infinite width, optimisation and generalisation properties can be tied down to an associated \emph{neural path kernel} (NPK). In the fixed NPF regime, in the finite width case, we show that when optimising  using GD i) increasing depth till a point helps in training and ii) increasing depth beyond a point hurts training.  We show via experiments, that NPFs are learnt during training, and such learning improves generalisation performance. %B
\end{abstract}
\begin{abstract}
Understanding optimisation and generalisation in deep neural networks (DNNs) with ReLU activations trained using first-order method such as gradient descent (GD) is an important problem in machine learning. In this paper, we decompose the computation in such DNNs into paths and gates. This decomposition yields a novel zeroth-order feature namely the \emph{neural path feature} (NPF), whose dimension is equal to total the number of paths, and whose co-ordinate corresponding to a path is the product of the signal at its input node and its \emph{on/off} state\footnote{A path is \emph{on} if all the gates in the path are \emph{on}.}.  We show via theory and experiments, that, in DNNs with finite width, NPFs are learnt during training, and such learning improves generalisation performance. %B
\end{abstract}
\end{comment}
\begin{comment}
Understanding optimisation and generalisation in deep neural networks (DNNs) with ReLU activations trained using first-order method such as gradient descent (GD) is an important problem in machine learning. In this paper, we deconstruct the computation in such DNNs into paths and gates. This deconstruction yields a novel zeroth-order feature namely the \emph{neural path feature} (NPF), whose dimension is equal to total the number of paths, and whose co-ordinate corresponding to a path is the product of the signal at its input node and its \emph{on/off} state\footnote{A path is \emph{on} if all the gates in the path are \emph{on}.}. We show that when optimising with fixed NPFs using GD i) increasing depth till a point helps in training and ii) increasing depth beyond a point hurts training. We show via theory and experiments, that, in DNNs, NPFs are learnt during training, and such learning improves generalisation performance. %By bringing in zeroth-order information into the picture via NPFs, our work is complementary to prior work based on NTFs.
\en{comment}
\begin{comment}
Understanding optimisation and generalisation in deep neural networks (DNNs) (with ReLU) trained using first-order method such as gradient descent (GD) is an important problem in machine learning. Recent works on this problem have made use of the \emph{neural tangent feature} (NTF) which is equal to the gradient of the DNN output with respect to its weights, i.e., the feature is based on first-order information. In this paper, we deconstruct the computation in DNNs into paths and gates. This deconstruction yields a novel zeroth-order feature namely the \emph{neural path feature} (NPF), whose dimension is equal to total the number of paths, and whose co-ordinate corresponding to a path is the product of the signal at its input node and its \emph{on/off} state\footnote{A path is \emph{on} if all the gates in the path are \emph{on}.}. We show that when optimising with fixed NPFs using GD i) increasing depth till a point helps in training and ii) increasing depth beyond a point hurts training. While in NTFs based works, DNNs are considered as linear learners with fixed NTF, we show via theory and experiments, that, in DNNs, NPFs are learnt during training, and such learning improves generalisation performance. By bringing in zeroth-order information into the picture via NPFs, our work is complementary to prior work based on NTFs.
\end{comment}
\begin{comment}
We study optimisation and generalisation in deep gated networks (DGNs), wherein, the output of a single hidden neuron is obtained as a product of its pre-activation input and a gating value. DNNs with ReLU activations are special cases of DGNs.  The DGN framework enables us to dismantle deep networks into paths and gates. This dismantling yields a novel (zeroth-order) \emph{neural path feature} (NPF) whose dimension is equal to total the number of paths, and whose co-ordinate corresponding to a path is the product of the signal at its input node and its \emph{on/off} state\footnote{A path is \emph{on} if all the gates in the path are \emph{on}.}. We show that the associated \emph{neural path kernel} (NPK) plays an important role in optimisation and generalisation of DGNs. %In particular, $H_t(x,x')=(x^\top x')\lambda_t(x,x')$, where $\lambda_t(x,x')$ is a count of total number of paths that are active simultaneously for both inputs $x,x'\in \R^{d_{in}}$. \hfill\\
For randomly initialised DGNs optimising with fixed NPFs, we show that i) increasing depth till a point helps in training and ii) increasing depth beyond hurts training. We verify via experiments that the NPFs are learned during training by showing that the norm of the labelling function measured with respect to the inverse of the trace normalised NPK reduces with time.
\end{comment}