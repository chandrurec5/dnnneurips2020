\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2020}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bbm}
\input{pack.tex}
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage{caption}
\captionsetup{belowskip=0pt}
\usepackage{wrapfig,lipsum}
\usepackage[linewidth=1.2pt,linecolor=red]{mdframed}


\usepackage{comment}
\usepackage{cancel}
\usepackage{changepage}
\usepackage{bbm}
\usepackage{pgfplots}
\usepackage{filecontents}
\setcounter{MaxMatrixCols}{32}
\usepackage{placeins}


\title{Deep Learning: A study in paths and gates}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

%\author{}

\begin{document}

\maketitle
\section{Introduction}
Understanding optimisation and generalisation of deep neural networks (DNNs) is an important problem in machine learning. Despite the fact that the optimisation of DNNs is a non-convex problem in general, simple (stochastic) gradient descent (SGD) type procedures are able to achieve zero training error when the DNN is sufficiently over-parameterised and when the parameters are initialised at random. Further, it has also been empirically observed that, practical DNNs can achieve zero training error on standard datasets even when the labels are flipped at random. 

Recent works based on the \emph{trajectory} analysis \cite{} have shown that, sufficiently over-parameterised DNNs can be optimised to achieve zero training error by a randomly initialised SGD procedure. The gist of the \emph{trajectory} based analysis is the following: consider the dataset given by $(x_s,y_s)_{i=1}^n\in \R^{d_{in}\times} \R$, and for an input $x_s\in \R^{d_{in}},i\in[n]$\footnote{We denote the set $\{1,\ldots, n\}$ by $[n]$.}, let $\hat{y}_{\Theta_t}(x_s)$ be predicted output of the DNN whose parameters/tunable weights at time $t$ is $\Theta_t\in \R^{d_{net}}$. Say one is interested in minimising the squared loss given by $L_{\Theta_t}=\frac{1}{2}\sum_{i=1}^n\left(\hat{y}_{\Theta_t}(x_s)-y_s\right)^2$ by a SGD procedure, then the idea behind the trajectory analysis is to look at the dynamics of the error term defined as $e_t(i)\stackrel{def}{=}\hat{y}_{\Theta_t}(x_s) -y_s$. Denoting $e_t=(e_t(i),i\in[n])\in \R^n$, one can study the following error recursion:
\begin{align}\label{eq:trajecbasic}
e_{t+1}=e_t-\alpha_t K_t e_t,
\end{align}
where $\alpha_t>0$ is a small stepsize, $K_t\in \R^{n\times n}$ is a Gram matrix. This gram matrix is in turn expressed as $K_t=\Psi_t^\top \Psi_t$, wherein, $\Psi_t$ is a $d_{net}\times n$ matrix known as the \emph{neural tangent feature} (NTF) matrix, which is the collection of the gradient of the network output with respect to the network parameters, and whose $s^{th}$ column is given by $\Psi_t(s)=(\frac{\partial \hat{y}_{\Theta_t}(x_s)}{\partial \theta},\theta \in \Theta)$.


\textbf{Question I (Optimisation):} \emph{What is the role of depth in training of DNNs? Why increasing depth till a point helps in training? Why increasing the depth beyond hurts training?}\\
\begin{comment}We call the above  questions are the depth phenomena. \cite{dudnn} show that, when it comes to training, residual networks are better than simple FC-DNNs. However, the depth phenomena in the case of simple FC-DNNs is still unresolved.
\end{comment}
\textbf{Question II (Generalisation):} \emph{What are the hidden features in a DNN? Are these features learned? and if so, how?}
\begin{comment}The general consensus is that, the DNNs learn hidden representations progressively in each of the intermediate layers, and the final layer learns a linear model using features obtained in the penultimate layer.  This view, while conceptually simple, however, does not provide us any analytical insight regarding the above question.  A more analytically appealing candidate for the hidden representation (used in some of the recent works \cite{}) is the \emph{neural tangent random feature} (NTRF) which is the NTF evaluated at randomised initialisation of an infinitely wide DNN. \cite{} provides generalisation bounds in terms of the Rademacher complexity of the class of functions defined by the NTRF and also in terms of an associated neural tangent kernel (NTK). \cite{} uses NTK to set a significant new benchmark for pure-kernel based learning.  An issue with the NTRF/NTK approach is that the features do not change over the training of the DNN, thus implying no feature learning is happening, and yet experimental evidence (in \cite{} as well as \Cref{sec:generalisation-exp}) shows that DNNs perform significantly better than pure-kernel learning with NTK.
\end{comment}
\subsection{Contributions}
In this paper, we are interested in fully connected (FC) deep gated networks (DGN), wherein, the output of a neuron can be expressed as a product of its pre-activation input and its gating value. DNNs with ReLU activations (in this paper, we refer to them as simply DNNs) are special cases of DGNs.
Central to the contributions in this paper is the conceptual novelty of regarding activations/gates and paths as basic building blocks of a DGN. We now describe in brief the conceptual novelties (CN), the corresponding conceptual gains/insights (CG) and the key results in the paper.

\textbf{Paths (CN I ):}  A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Say for a FC-DNN with $d$ layers, and $w$ hidden gates per layer, there are $w(d-1)$ gates and a total of $P=d_{in}w^{(d-1)}$ paths. At time $t$, each path is associated with three important quantities namely i) its input node denoted by $\I(p)$, ii) its value $v_t(p)$ given by the products of the weights in the path, and  iii) its activation level for an input $x\in\R$, denoted by $A_t(x,p)$, which is given by the product of the activations of the gates in the path.

\textbf{Neural Path Features (CG I):}  For an input example $x\in \R^{d_{in}}$, the \emph{neural path feature} (NPF) denote by $\phi_{x,t}\in \R^P$,  whose $p^{th}$ co-ordinate corresponding to path $p$ is given by $\phi_{x,t}(p)=x(\I(p))A_t(x,p)$. The predicted output of the DNN can then be expressed as:
\begin{align}\label{eq:npf}
\text{(Zeroth Order)}\quad: \quad \hat{y}_t(x)=\sum_{p}x(\I(p))A_t(x,p)v_t(p)={\phi_{x,t}}^\top v_t, 
\end{align}
where, $v_t=(v_t(p),p\in[P])\in \R^P$ is vector values of all the paths.

\textbf{Neural Path Kernel (CG II ):} Associated with the NPF is a novel \emph{neural path kernel} (NPK) given by  $H_t(s,s')=(x_s^\top x_{s'})\lambda_t(s,s')$, where $\lambda_t(s,s')$ is a measure of similarity based on the path activation levels for inputs $s,s'$. For instance, in the case of DNNs with ReLU, $\lambda_t(s,s')$ is a measure of the sub-networks that are simultaneously \emph{on} for both the inputs, capturing the intuition that if $s$ and $s'$ are similar, then their corresponding active sub-networks might also be similar. Note that, in $H_t$, only $\lambda_t$ changes with training. 

\textbf{Feature Learning and Twin Gradients (CG III ):} From \eqref{eq:npf}, it follows that
\begin{align}\label{eq:npfgrad}
\text{(First Order)}\quad: \quad \partial \hat{y}_t(x)&=\sum_{p}x(\I(p)) \partial(A_t(x,p)) v_t(p)+ \sum_{p}x(\I(p)) A_t(x,p) \partial((v_t(p))\nonumber\\
&=\phi^\top_{x_s,t} {\partial} v_t + {\partial} \phi^\top_{x_s,t} v_t,
\end{align}
from which we gather that the gradient flow has two components i) $\partial_{v}= \phi^\top_{x_s,t} {\partial} v_t$, responsible for learning the weights while keeping the NPF fixed, and ii) $ \partial_{\phi}={\partial} \phi^\top_{x_s,t} v_t$, responsible for learning the NPF while keeping the weights fixed. 

\textbf{ReLU artefact or NTF vs NPF (CG IV):} Note that the left-hand side of \eqref{eq:npfgrad} is the neural tangent feature. We note that, in the case when $A_t(x,p)\in\{0,1\}$ (such as in DNN with ReLU activations)  $\partial_{\phi}=0$, and hence is not accounted for in the SGD update as well as the analysis. However, due to the SGD update, the gating value changes during training, and consequently the activations, the NPF, the NPK change during training (see the experiments in \Cref{sec:generalisation}). We believe the change of activations is key for generalisation, a fact which can explain the gap in generalisation performance \cite{} between the DNNs and the pure-kernel methods based on their static NTK counterparts.

\textbf{Separation of Gates (CN II):} In order to better understand the role of $A_t(\cdot,\cdot)$ and $\partial_{\phi}$ in the optimisation and generalisation of DNNs, we handle the gating values as independent variables. This enables us to set up various experiments such as freezing the gates (i.e., gating values do not change over time), copy the gating values from a different network, introduce soft-max type gating functions etc.
%In our theory as well as experiments, we study the following useful gating regimes (GR): GR-(i)(\emph{frozen}), wherein, the activations $A_t(\cdot,\cdot)$ do not change with time, GR-(ii) (\emph{decoupled}), wherein, $A_t(\cdot,\cdot)$ is derived from a separate \emph{gating} network parameterised by $\Tg\in \R^{d_{net}}$,  GR-(iii) (\emph{adaptable}), wherein, $A_t(\cdot,\cdot)$ changes during training (this is the case of the standard DNNs with ReLU gates), and GR-(iv) (\emph{soft}), wherein, the hard-max indicator function of the ReLU is replaced by a soft-max sigmoidal function.

\textbf{Optimisation (Key Results I):} In GR-(ii), when the weights are sampled from symmetric Bernoulli distribution taking values in $\{-\sigma,+\sigma\}$, we have the following results that throw light on Question I: 

$1.$ $\E{K_0(s,s')}=d\sigma^{2(d-1)}H_0(s,s')=d\sigma^{2(d-1)}(x_s^\top x_{s'})\lambda_0(s,s')$. The ratio $\frac{{\lambda_0}(s,s')}{{\lambda_0}(s,s)}$ is the fractional overlap of active sub-networks, say at each layer the overlap of active gates is $\mu\in (0,1)$, then for a depth $d$, the fractional overlap decays at exponential rate, i.e., $\frac{{\lambda_0}(s,s')}{{\lambda_0}(s,s)}\leq \mu^d$, leading to whitening. Thus increasing depth till a point helps in training.

$2.$ $Var\left[K_0(s,s')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$ (for $\sigma^2=O(\frac{1}w)$). For large width $K_0$ converges to its expected value. However, for a fixed width, increasing depth makes the entries of $K_0$ deviate from $\E{K_0}$, thus degrading the spectrum of $K_0$. Thus increasing depth beyond a point hurts training performance.

%The results on $\E{K_0}$ and $Var\left[K_0\right]$ cannot be applied to DNNs with ReLU gates, since the weights and the gates are not independent. However, the results apply to GaLU networks, where the gating pattern is generated by a different network, and hence the statistical independence of weights and the gating pattern can be ensured. We also present experimental results to support the theory.


\textbf{Generalisation (Key Results II):} We show via experiments on standard datasets such as MNIST and CIFAR that adaptable gates generalise better than frozen gates. In particular, for MNIST restricted to two labels, we experimentally verify that the quantity $y^\top (\widehat{H_t})^{-1}y$\footnote{For a matrix $H$, $\hat{H}=\frac{1}{trace(H)}H$ .} reduces as the training progresses. This shows that the eigenspaces of the NPK matrix align themselves in accordance to the labelling function. These experiments supports the fact that NPF and the NPK are indeed learnt during training. We develop preliminary theory to throw light on feature learning, and argue that, at time $t$, and input example $x_s\in \R^{d_{in}},s\in[n]$, there are two sub-networks, namely i) an \emph{active} sub-network (for whose paths $\partial A_t(x_s,p)$ is close to zero) which holds the memory corresponding to that input example, and (ii) a \emph{sensitive} sub-network (for whose paths $\partial A_t(x_s,p)$ is significant), where the feature learning happens.


%In a DNN with ReLU activations, each input example can be associated with a gating pattern, i.e., the set of gates that are \emph{on} for that example. Note that, only the sub-network of those weights corresponding to these \emph{on} activations/gates are responsible for predicting the output. In this paper, we obtain several new insights related to optimisation and generalisation in DNNs via a formal study of the paths and gates. In what follows, we discuss the organisation of the paper and key contributions.

\begin{comment}
\textbf{Path-View:} Central to the contributions in the paper is the concept of \emph{path-view}, wherein, paths are regarded as basic building blocks of DNNs. A \emph{path} starts from an input node $i\in[d_{in}]$, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Using shorthand notation, say a path $p$ pass through input node $\I_0(p)\in [d_{in}]$, and weights $\Theta(l,\I_{l-1}(p),\I_l(p)),l\in[d-1]$, and gates $G(l,I_l(p))$

For an FC-DNN with $d$ layers (depth), and $w$ hidden units per-layer, the total number of paths (denoted by $P$) starting from a given input node $i\in[d_{in}]$ is $w^{(d-1)}$.

$\bullet$ At time $t$, for an input $x\in \R^{d_{in}}$, a path $p$ is associated with two quantities namely: (i) the path value denoted by $v_t(p)$, which is the product of the weights in the path, and (ii) the path activation level denoted by $A_t(x,p)$ which is the product of the gates\footnote{In the case of DNN with ReLU activations/gates, $A_t(x,p) \in \{0,1\}$.} in the path. Note that the activation level of a path is dependent on the input and the value of a path is not dependent on the input.
\end{comment}


\begin{comment}
In a DNN with ReLU activations, each input example can be associated with a gating pattern, i.e., the set of activations that are \emph{on} for that example. Note that, only the sub-network of those weights corresponding to these \emph{on} activations/gates are responsible for predicting the output. In this paper, we obtain several new insights related to Questions I and II by a formal study of the paths and gates.
\end{comment}

\input{pathgate}
\input{expressivity}
%\input{abstract.tex}
%\input{intro.tex}
\input{optimisation.tex}
\input{generalisation.tex}
%\input{convnet.tex}
%\input{related.tex}
%\input{future.tex}
\bibliographystyle{plainnat}
\bibliography{refs}
%\input{appendix}


\end{document}
