\begin{abstract}
We study optimisation and generalisation in deep gates networks (DGNs), wherein, the output of a single hidden neuron is obtained as a product of its pre-activation input and a gating value. DNNs with ReLU activations are special cases of DGNs. Central idea in this paper is to regard paths and gates as basic building blocks of a DGN. At time $t$, for an input $x\in\R^{d_{in}}$ to the DGN, we define a novel \emph{neural path feature} (NPF) whose dimension is equal to total the number of paths, and whose co-ordinate corresponding to a path is the product of the signal at its input node  and the activity\footnote{A path is active if all the gates in the path are active.} of the path. We show that the associated \emph{neural path kernel} (NPK) plays an important role in optimisation and generalisation of DGNs. %In particular, $H_t(x,x')=(x^\top x')\lambda_t(x,x')$, where $\lambda_t(x,x')$ is a count of total number of paths that are active simultaneously for both inputs $x,x'\in \R^{d_{in}}$. \hfill\\
For randomly initialised DGNs optimising with fixed NPFs, we show that i) increasing depth till a point helps in training and ii) increasing depth beyond hurts training. We verify via experiments that the NPFs are learned during training by showing that the norm of the labelling function measured with respect to the inverse of the trace normalised NPK reduces with time.
\end{abstract}
