\section{Expressivity: Neural Path Feature and Neural Path Kernel}
\textbf{Deep Information propagation and Signal-Wire separation:} The \emph{neural path feature} of an input $x\in \R^{d_{in}}$ is given by $\phi_{x,t}=(x(p(0))A_t(x,p) ,p\in[P])\in\R^P$. By arranging the NPF of the $n$ input examples in a matrix $\Phi_t=\left[\phi_{x_1,t},\ldots, \Phi_{x_n,t}\right]$, we can express the predicted output of a DGN as \begin{align}\hat{y}_t=\Phi_t^\top v_t\end{align} Note that $\Phi$ encodes the signal: say for a DNN with ReLU activations, the co-ordinate corresponding to path $p$ is either $x(p(0))$ if the path is active for that input (i.e., $A_t(x,p)=1$) or $0$ if the path is inactive for that input  (i.e., $A_t(x,p)=0$). The value of the path $v_t$ is the equivalent of the so called \emph{weight-vector} in a standard linear approximation. The value of the path thus encodes the \emph{wire}, i.e., the information contained in the weights of the network. Thus the path view provides a novel way of looking at information propagation in deep neural networks eschewing the conventional layer after layer way of expressing information flow in almost all the literature in the past.

\textbf{Similarity Metric:} For any $i\in [d_{in}]$ define $\lambda_t(s,s')\stackrel{def}{=}\sum_{p\rsa i} A_t(x_s,p) A_t(x_{s'},p)$ to be the measure of activation similarity of the paths for inputs $s,s'\in[n]$. In the case of DNNs, $\lambda_t$ counts the total number of paths that are simultaneously \emph{on/active} for both inputs $s,s'\in[n]$. Note that the definition of $\lambda_t$ is independent of $i\in [d_{in}]$: owing to symmetry the same number of paths start from any given input node, and see exactly the same gates in the subsequent layers.

\begin{lemma}[Neural Path Kernel] Let $x=(x_s,s\in [n])\in\R^{d_{in}\times n}$ be the data matrix and let the neural path kernel be defined as $H_t\stackrel{def}=\Phi^\top_t\Phi_t$. It follows that $H_t= (x^\top x)\odot(\lambda_t)$ \end{lemma}





