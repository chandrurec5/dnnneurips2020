\section{Kernels}\label{sec:kernels}
In this section, we look at the relationship between the \emph{neural path kernel} (NPK) obtained from the NPFs, and the \emph{neural tangent kernel} (NTK) obtained from the \emph{neural tangent features}. In what follows, let $(x_s,y_s)_{s=1}^n\in\R^{d_{in}}\times\R$ be the dataset. 
\begin{comment}
\subsection{Neural Tangent Kernel in Prior Works}
The \emph{neural tangent feature} (NTF) matrix, denoted by $\Psi_{\Theta}$ is a $d_{net}\times n$ matrix, whose entries are given by $\Psi(\theta,s)=\partial_{\theta} \hat{y}_{\Theta}(x_s),\theta\in\Theta, s\in[n]$\footnote{Here $\partial_{\theta}(\cdot)$ stands for $\frac{\partial (\cdot)}{\partial \theta}$}. The \emph{neural tangent kernel} (NTK) matrix is given by $K_{\Theta}=\Psi_{\Theta}^\top\Psi_{\Theta}$. The NTK is useful in the following two ways:\\
$1.$ Recent works have used the \emph{trajectory} based analysis to show that GD achieves zero training loss in the over-parameterised regime. Suppose we are interested in minimising the squared loss, using GD, and then the trajectory analysis looks at the evolution of the error term $e_t=(\hat{y}_{\Theta_t}(x_s)-y_s,s\in[n])\in\R^n$. The dynamics of the error can given by:
\begin{align}
e_{t+1}=e_t-\alpha_t K_t e_t,
\end{align}
where $\alpha_t>0$ is the stepsize of the GD procedure. Thus, the eigenvalues of $K_t$ play an important role in convergence of GD.\\
$2.$ The NTK matrix has also been used to design pure-kernel based methods \cite{arora2019exact} and provide generalisation bounds \cite{cao2019generalization}.\\
\textbf{NTK in prior works:} Before we connect the NPK and the NTK using the `path-view', we will first look at prior works based on NTK. Prior works have used the following recursive definition or its variants, wherein, for layers $l=1,\ldots, d-1$, we define matrices:
\begin{align}\label{eq:ntkold}
&\tilde{K}^{(1)}(s,s')=\Sigma^{(1)}(s,s')=\Sigma(s,s'), M^{(l)}_{ss'}=\left[\begin{matrix}\Sigma^{(l)}(s,s) & \Sigma^{(l)}(s,s')\\ \Sigma^{(l)}(s',s) & \Sigma^{(l)}(s',s')\end{matrix}\right]\in \R^2,\\
&\Sigma^{(l+1)}(s,s')= 2\cdot\mathbb{E}_{(q,q')\sim N(0,M_{ss'}^{(l)})} \left[\chi(q)\chi(q')\right], \dot{\Sigma}^{(l+1)}(s,s')= 2\cdot\mathbb{E}_{(q,q')\sim N(0,M_{ss'}^{(l)})}\left[\dot{\chi}(q)\dot{\chi}(q')\right],\nn\\
&\tilde{K}^{(l+1)}=\tilde{K}^{(l)}\odot \dot{\Sigma}^{(l+1)}+\Sigma^{(l+1)},\nn
\end{align}
where $s,s'\in[n]$ are two input examples in the dataset, $\Sigma$ is the data Gram matrix, $\dot{\chi}$ stands for the derivative of the activation function with respect to the pre-activation input, $N(0,M)$ stands for the mean-zero Gaussian distribution with co-variance matrix $M$. The final limiting matrix is given by $K^{(d)}=\left(\tilde{K}^{(d)}+\Sigma^{(d)}\right)/2$. Prior works show that at randomised initialisation $\Theta_0$, $K_{\Theta_0}\ra K^{(d)}$ as $w\ra \infty$.
\end{comment}
\section{Neural Tangent Kernel in `Path-View'}
Using the `path-view', we obtain a different decomposition for the NTK matrix $K{\Theta}$, and compare our expression to the previously obtained expression in \Cref{eq:ntkold}. To this end, let us by define $d_{net}\times n$ matrices $\Psi^v_t$, and $\Psi^{\phi}_t$, whose entries are given by $\Psi^v_t(\theta,s)=\ip{\phi_{x_s,\Theta},\partial_{\theta}v_{\Theta}}$, and  $\Psi^{\phi}_t(\theta,s)=\ip{\partial_{\theta}\phi_{x_s,\Theta}, v_{\Theta}}$. Now, we have the following NTK decomposition (for a soft-ReLU DNN):
\begin{align}\label{eq:ntknew}
K_{\Theta}=K^v_{\Theta}+K^{\phi}_{\Theta}+(\Psi^v_\Theta)^\top \Psi^{\phi}_{\Theta} +(\Psi^{\phi}_\Theta)^\top \Psi^{v}_{\Theta},\,\text{where},
\end{align}
$K^v_{\Theta}$ is the NTK of path values due to the value gradient, $K^{\phi}_{\Theta}$ is the NTK of the features due to the feature gradient, and $(\Psi^v_\Theta)^\top \Psi^{\phi}_{\Theta} +(\Psi^{\phi}_\Theta)^\top \Psi^{v}_{\Theta}$, is a symmetric matrix which is the cross-term obtained as an interaction of the value and the feature gradients. We now list the salient points about the decomposition in \eqref{eq:ntknew}. The recursive relationship in \eqref{eq:ntkold} is due to the fact that both zeroth-order (forward propagation) and first-order (backward propagation) information use a layer after layer expression for information flow. Also, \eqref{eq:ntkold} derives a limiting matrix ($w\ra\infty$) that occurs at a random $\Theta_0$. On the contrary, \eqref{eq:ntknew} holds for any parameter setting and for finite $w$ and $d$.\\
%\subsection{Neural Path Kernel and Optimisation}
%\subsection{Gate Dynamics and Feature Learning}
%$\bullet$ $(\Psi^v_\Theta)^\top \Psi^{\phi}_{\Theta} +(\Psi^{\phi}_\Theta)^\top \Psi^{v}_{\Theta}$, is a symmetric matrix which is the cross-term obtained as an interaction of the value and the feature gradients.
\begin{comment}
$K^v_{\Theta}$ is the NTK of path values due to the value gradient, $K^{\phi}_{\Theta}$ is the NTK of the features due to the feature gradient, and $(\Psi^v_\Theta)^\top \Psi^{\phi}_{\Theta} +(\Psi^{\phi}_\Theta)^\top \Psi^{v}_{\Theta}$, is a symmetric matrix which is the cross-term obtained as an interaction of the value and the feature gradients. We now list the salient points about the decomposition in \eqref{eq:ntknew}.\\
$1.$ \textbf{Non-recursive:} The recursive relationship in \eqref{eq:ntkold} is due to the fact that both zeroth-order (forward propagation) and first-order (backward propagation) information use a layer after layer expression for information flow. Also, \eqref{eq:ntkold} derives a limiting matrix ($w\ra\infty$) that occurs at a random $\Theta_0$. On the contrary, \eqref{eq:ntknew} holds for any parameter setting and for finite $w$ and $d$.\\
$2.$ \textbf{Gate Dynamics and Feature Learning:} 
\end{comment}
%\subsection{Neural Path Kernel and Optimisation}
\begin{comment}$\bullet$ \textbf{Role of Active Sub-Network:} Input Gram matrix and 
\begin{theorem}[\citet{ando}]\label{th:ando}
For two Hermitian matrices $A\in \C^{d\times d}$ and $B\in \C^{d\times d}$, $\lambda_{\min}(A\odot\B)\geq \lambda_{\min} (AB)$. 
\end{theorem}
\begin{lemma}\label{lm:suf}
For two positive definite symmetric matrices $A$ and $B$, $\lambda_{\min}(AB)\geq \lambda_{\min} (A)\lambda_{\min}(B)$. 
\end{lemma}
From \Cref{lm:suf}, a sufficient condition to ensue that $\rho_{\min}(H_{\Theta})$ is bounded away from zero is to ensure that $\rho_{\min}(\Lambda_{\Theta})$ and $\rho_{\min}(\Sigma)$ bounded away from $0$.\\
\end{comment}
%\subsection{Gating Dynamics and Feature Learning} 

\begin{comment}
\begin{proof}
Matrix $AB$ is similar to the matrix $B^{\frac12}ABB^{-\frac12}$. Thus $\lmin{AB}=\lmin{B^{\frac12}AB^{\frac12}}$.
\begin{align*}
\min_{x: \norm{x}^2=1} x^\top B^{\frac12}AB^{\frac12} x &=  \left(\min_{y: y=B^{\frac12} x, x: \norm{x}^2=1 }y^\top A y \right)\\
%&\geq  \left(\min_{y: \norm{y}^2= \min_{x\in \R^d: \norm{x}^2=1} x^\top B x }y^\top A y \right)\\
&\geq \left(\min_{x: \norm{x}^2=1}x^\top B x \right) \left(\min_{y: \norm{y}^2=1 }y^\top A y \right)
\end{align*}
\end{proof}
\end{comment}