\section{Introduction}
Understanding optimisation and generalisation in deep neural networks (DNNs) trained using first-order method such as gradient descent (GD) is an important problem in machine learning. Some of the recent works on this problem have made use of the \emph{neural tangent features} (NTFs) and the \emph{trajectory} based analysis. The NTF of an input $x\in \R^{d_{in}}$ is defined as $\psi_{x,t}=\left(\frac{\partial \hat{y}_{\Theta}(x)}{\partial \theta}|_{\Theta=\Theta_t},\theta \in \Theta\right)\in \R^{d_{net}}$, i.e., as the gradient of the network output $\hat{y}_{\Theta}(x)$ with respect to its weights $\Theta\inrdnet$. Since the NTF is a first-order term, it can be used to linearise the DNN output about $\Theta_t$. Associated with the NTF is a \emph{neural tangent kernel} (NTK), which is given by $K_t(x,x')=\ip{\psi_{x,t},\psi_{x',t}}$\footnote{For $x,y\in\R^d \ip{x,y}=x^\top y$.}, where $x,x'\in\R^{d_{in}}$ are inputs to the DNN. In the \emph{trajectory} based analysis, one looks at the dynamics of error $e_t$  (i.e., difference between predicted and true values) at time $t$. For a small step-size $\alpha_t>0$ of the GD procedure, the error dynamics follows a recursion given by: $e_{t+1}=e_t-\alpha_tK_te_t$, where $K_t$ is the NTK matrix obtained on the the dataset. It is known from \cite{dudnn}, that, gradient descent achieves zero training loss in over-parameterised DNNs, and from \cite{cao2019generalization}, that, the NTK at initialisation can be used to bound the generalisation performance.\\
\textbf{Question:} What are the hidden features in a DNN? Are these features learned? and if so, how?\\
Based on the prior works using NTFs and NTK, the answer to the above question is negative, because, the DNNs are only linear learners with the NTFs at random initialisation, and hence no feature learning happens. In this paper, we answer the question in the affirmative by identifying novel \emph{neural path features} (NPFs), which are zeroth-order features, and showing via theory and experiments that NPF are learnt during training, and such learning improves generalisation performance. The key step are:\\
$1.$ \textbf{Path-View:}(\Cref{sec:pathgate}) We introduce the `path-view', wherein, we decompose the computation in a DNN in terms of the paths and the gates. Using the `path-view', we define a novel \emph{neural path feature} (NPF) and a novel \emph{neural path value} (NPV), and express the output as an inner product of the NPV and NPF. We argue that the gradient flow has two components: i) the value gradient (of the NPVs)  and i) the feature gradient (of the NPFs).\\
$2.$ \textbf{Kernels:} (\Cref{sec:kernels}) The \emph{neural path kernel} (NPK) $H_t$ associated with the NPFs has a special structure, i.e., $H_t=\Sigma\odot\lambda_t$, where, $\Sigma$ is the input Gram matrix, $\lambda_0$ is the correlation matrix of the active sub-networks, and $\odot$ stands of \emph{Hadamard} product.  We show the NTK can be decomposed into $K_t=K^v_t+K^{\phi}_t+M$, where $K^v_t$ is the NTK of values responsible for optimisation with fixed features, and $K^{\phi}_t$ is the NTK of features responsible for feature learning, and $M$ is a symmetric matrix. We discuss the connection between $H_t$ and $K^v_t$.\\
\begin{comment}$3.$ \textbf{Deep Gated Networks:} (\Cref{sec:optimisation}) We introduce the deep gated networks (DGN) framework, wherein, the gating values and hence the NPFs are held in a gating network and the NPVs are held in a value network (which is different from the gating network). The DGN framework enables us to study optimisation and generalisation properties of NPFs.\\
\end{comment}
$3.$ \textbf{Optimisation:} (\Cref{sec:optimisation}-\Cref{th:main}) We show that for fixed NPFs, and appropriate random initialisation of weights of the value network, in the limit of infinite width $K_0\ra CH_0$ ($C>0$ is a scaling constant). Using the property of $H_0$ we conclude that for infinitely large width, increasing depth helps in training performance, and for finite widths, increasing depth beyond a point hurts training performance due to the variance in entries of $K_0$.\\
$4.$ \textbf{Generalisation and NPF learning:} (\Cref{sec:generalisation}) We show that NPFs obtained from a pre-trained network generalises better than NPFs from a randomly initialised network. This implies that the NPFs are learned during training. Since \Cref{th:main} allows us to use $H_0$ in the place of $K_0$ in the generalisation bounds \cite{cao2019generalization}, we verify that the norm of the labelling function measured with respect to the inverse of the trace normalised NPK reduces with time.\\
\textbf{Lesson Learnt:} Gradient is a first-order information, and learning with GD is essentially a process of integration, and naturally its outcome is a zeroth-order term. It turns out that splitting this zeroth-order term into NPFs and NPVs is quite useful, in particular, this enables us to identify the role of NPFs. \emph{Understanding deep learning requires understanding the dynamics of the gates.}
\begin{comment}
$1.$ \textbf{Deep Gated Networks (DGNs):} Here, the output of a single hidden neuron is obtained as a product of its pre-activation input ($\in \R$) and a gating value ($\in [0,1]$ ). The DGNs we consider consists of two networks i) a value network which holds the pre-activation input, layer output and layer weights $\Theta\in \R^{d_{net}}$, and the final output, and ii) a separate gating network (with its own internal pre-activation inputs, layer output, and layer weights $\Tg\in \R^{d_{net}}$), which provides the gating values. 
DNN with ReLU activation is a special case, wherein, the value network and gating network are \emph{coupled} i.e., $\Theta=\Tg$.\\
$2.$ \textbf{Neural Path Feature and Kernel (NPF and NPK):} We express the output as the sum total of the contributions from various paths. A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Let $[P]=\{1,\ldots,P\}$ be an enumeration of all the paths, then the contribution of a path in a DGN is the product of the signal at the input node, all the gating values of the hidden neurons and all the weight through which it passes. At time $t$, and for an input $x\in\R^{d_{in}}$,  the output of a DGN is recovered as $\hat{y}_t(x)=\ip{\phi_{x,t},v_t}$, where $\phi_{x,t}\in \R^P$ is the \emph{neural path feature} (NPF). Here, for a path $p\in[P]$, $\phi_{x,t}(p)$ holds the product of the signal at its input node and all its gating values, and  $v_t=(v_t(p),p\in[P])\in\R^P$, with $v_t(p)$ holding the product of the weights in path $p$. The associated \emph{neural path kernel} (NPK) matrix $H_t$ has a special \emph{Hadamard} structure, i.e., $H_t=\Sigma \odot \lambda_t$, where $\Sigma$ is the data \emph{Gram matrix}, $\lambda_t(x,x')$ measure the overlap of active sub-networks for inputs $x,x'\in\R^{d_{in}}$, and $\odot$ is the Hadamard product.\\
$3.$ \textbf{Gate Copying:} Contrary to NTK based works \cite{jacot18,jacot19,cao2019generalization,arora2019exact}, wherein, the DNNs are linear learners using fixed NTFs at random initialisation, the DGN framework enables the study the optimisation and generalisation with different NPFs. Two interesting cases arise when NPFs are obtained from a gating network which is a i)  pre-trained DNN, and ii) randomly initialised DNN.\\
$4.$ \textbf{Optimisation:} We consider DGN training in the fixed gating regime, where, the NPFs in the gating network are kept fixed through training, and only the path values are trained. When the weights are initialised statistically independent of the NPFs, we show, in \Cref{th:main}, that as width approaches $\infty$, $K_0\ra C H_0$ (for a constant $C>0$). This result implies that in the limit of infinite width, increasing depth leads to whitening of $\lambda_0$ and thus helps in training, and for finite width increasing depth beyond a point hurts training variance in entries of $K_0$ which depend on $d$. \\
$5.$ \textbf{Generalisation:}  We show (on MNIST and CIFAR datasets) that DGN trained with fixed NPFs obtained from a pre-trained gating network generalise better than DGNs trained with fixed NPFs obtained from a randomly initialised gating network. We verify that (experimentally with a `Binary'-MNIST dataset) that the norm of the label function measured with respect to the normalised NPK matrix reduces during training. We note that using \Cref{th:main} to replace $K_0$ with $H_0$ in the generalisation bounds of \cite{cao2019generalization}, would provide the theoretical justification of why NPFs from pre-trained gating networks generalise better.\\
$6.$ \textbf{Feature Learning:} The gradient $\partial \hat{y}$ has two components i) a \emph{value gradient} term $(\partial v)^\top \phi$ which learns path values with fixed NPFs and ii) a \emph{feature gradient} term $(\partial \phi)^\top v$ which learns the features themselves. The value gradient flows through the sub-network of \emph{active} paths, and the feature gradient flows the sub-network of \emph{sensitive} paths which contain gates that are in the verge of switching \emph{on/off}. In particular, when $\Tg$ and $\Theta$ are initialised statistically independent of each other, we show that $\E{K_0}=(C\Sigma\odot\lambda_0+C'\Sigma\odot\delta_0)$, where $\delta_0(x,x')$ is a measure of overlap of sensitive sub-networks for inputs $x,x'\in\R^{d_{in}}$.
\end{comment}
\begin{comment}
Say $[P]=\{1,\ldots,P\}$ be an enumeration of the paths, 
$1.$ At time $t$, for an input $x\in\R^{d_{in}}$, we define a novel (zeroth-order) \emph{neural path feature} (NPF) $\phi_{x,t}\in \R^P$, where $P$ is the total number of paths, and for a path $p$, $\phi_{x,t}(p),\p\in[P]$\footnote{$[P]=\{1,\ldots,P\}$} is equal to the product of the signal at its input node and its activity. The output can be written as $\hat{y}_t(x)={\phi^\top_{x,t}} v_t$, where $v_t=(v_t(p),p\in[P])\in\R^P$ is the vector of path values with $v_t(p)$ given by the product of the weights in the path.\\
%\text{(First Order)}\quad: \quad \partial \hat{y}_t(x)&= \sum_{p\in [P]}x(\I(p)) A_t(x,p) \partial((v_t(p))+\sum_{p\in [P]}x(\I(p)) \partial(A_t(x,p)) v_t(p)\nn\\
%\label{eq:first}&\text{(First- Order/Neural Tangent Feature)}&\quad:&\quad   \nabla \hat{y}_t(x)=\psi_{x,t}=(\nabla v_t )^\top \phi_{x,t} + (\nabla \phi_{x,t})^\top v_t,
$2.$ The associated \emph{neural path kernel} (NPK) matrix $H_t$ has a special \emph{Hadamard} structure, i.e., $H_t=\Sigma \odot \lambda_t$, where $\Sigma$ is the data \emph{Gram matrix}, $\lambda_t(x,x')$ measure the overlap of active sub-networks for inputs $x,x'\in\R^{d_{in}}$, and $\odot$ is the Hadamard product.\\
$3.$ Say $\Theta$ are the network weights, now, since the NPFs are solely based on the gating information, the NPFs can be obtained from a separate gating network, whose weights are $\Tg\neq\Theta$. The NPFs could be derived from i) a randomly initialised gating network or ii) a pre-trained gating network. This way we can study optimisation and generalisation of different kinds of NPFs, which is in sharp contrast with the NTK based works \cite{jacot18,jacot19,cao2019generalization,arora2019exact}, wherein, the DNNs are linear learners using fixed NTFs at random initialisation.\\
$4$. The gradient $\partial \hat{y}$ has two components i) a $(\partial v)^\top \phi$ which learns path values with fixed NPFs and ii) a $(\partial \phi)^\top v$ which learns the features themselves.\\
\textbf{Main Results:} We present the analysis and experiments in two regimes namely the fixed gating regime, wherein, the NPFs are fixed and the adaptable gating regime, wherein, the NPFs are learnt.\\
$\bullet$ \textbf{Optimisation:} When the gates are fixed, under appropriate statistical assumptions, we show that (\Cref{th:main}) as width approaches $\infty$, $K_0\ra C H_0$ (for a constant $C>0$). This result implies that in the limit of infinite width, increasing depth leads to whitening of $\lambda_0$ and thus helps in training, and for finite width increasing depth beyond a point hurts training variance in entries of $K_0$ which depend on $d$. \\
$\bullet$ \textbf{Generalisation:} We show via experiments (on standard MNIST and CIFAR datasets) that DGNs with fixed NPFs derived from a pre-trained gating network generalise better than fixed NPFs obtained from a randomly initialised gating network. \\
We show on an experiment in `Binary'-MNIST dataset that the norm of the label function measured with respect to the normalised NPK matrix reduces during training.
$\bullet$ 
\end{comment}
\begin{comment}
$1.$ \textbf{Information Flow:} We introduce a host of novel terms/expressions and concepts related to information flow in DGNs. This includes i) the zeroth-order (see \eqref{eq:zero}) \emph{neural path feature} (NPF) and the \emph{neural path kernel} (NPK) denoted by $H_t$, which are defined using the gating information, ii) the first-order (see \eqref{eq:first}) terms namely, the \emph{value gradient} given by $(\nabla v_t )^\top \phi_{x,t}$, and the \emph{feature gradient} given by $(\nabla \phi_{x,t})^\top v_t$, iii)  two sub-network of paths namely, an \emph{active} sub-network, which is the set of active paths that hold the memory for an input, through which the value gradient flows and a \emph{sensitive} sub-network, which is the set of sensitive paths, through which the feature gradient flows. We also show that for random initialisation, the paths exhibit a \emph{disentanglement} property, which is very key in our results.\\
%$1.$ \textbf{Representation and Generalisation:} By fixing the activation (i.e., $A_t=A_0,\forall t\geq 0$), we train DGNs with different fixed \emph{neural path features} (NPFs), and demonstrate that the NPFs are key for generalisation. Further, we show that the NPFs are learnt during training. This shows that NPFs can be regarded as the true hidden features in a deep network.\\
\end{comment}
\begin{comment}
$3.$ \textbf{Neural Path Features} are the information stored in the gates of a deep network. Our main result connects the NPK to the NTK, i.e., we show that, when the weights are randomly initialised (with an appropriate scale) in a manner statistically independent of the gates, i)  $\E{K_0}=H_0$, and ii) $Var\left[K_0(x,x')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$.\\
$\quad\bullet$ \textbf{Optimisation:}   We argue that  $H_0$ whitens with depth, and hence, increasing depth helps in training. Increasing depth further causes $K_0$ to deviate from $H_0$, and hence, degrades the spectrum of $K_0$, thereby affecting training performance.\\
$\quad\bullet$ \textbf{Generalisation:} By fixing the activation (i.e., $A_t=A_0,\forall t\geq 0$), we train DGNs with different fixed NPFs, and demonstrate that the NPFs are key for generalisation. Further, we show that the NPFs are learnt during training. This shows that NPFs can be regarded as the true hidden features in a deep network.\\
$3.$ \textbf{NPF Learning:} We present preliminary results connecting NPF learning and the trajectory method.
\end{comment}
\begin{comment}
Understanding optimisation and generalisation of deep neural networks (DNNs) trained using first-order method such as (stochastic) gradient descent is an important problem in machine learning. In this paper, we throw light on the following two questions:\hfill\\
\textbf{Question I (Optimisation):} \emph{What is the role of width and depth in training of DNNs? Why increasing depth till a point helps in training? Why increasing the depth beyond hurts training?}\\
\textbf{Question II (Generalisation):} \emph{What are the hidden features in a DNN? Are these features learned? and if so, how?} \hfill\\

\textbf {Background:} We consider a fully connected deep gated network (DGN) of depth $d$, and width $w$, weights $\Theta\in\R^{d_{net}}$, which, accepts an input $x\in \R^{d_{in}}$ and produces an output $\hat{y}_{\Theta_t}(x)\in \R$. In a DGN, the output of a hidden neuron is obtained as a product of its pre-activation input and a gating value. DNNs with ReLU activations are special cases DGNs. Some of the recent works to understand SGD in relation to the optimisation and generalisation in DNNs make use of the \emph{neural tangent features} (NTFs) and the \emph{trajectory} based analysis, which we describe in brief.\hfill\\
$1.$The NTF for an input $x\in \R^{d_{in}}$ is defined as $\psi_{x,t}=(\frac{\partial \hat{y}_{\Theta}}{\partial \theta}|_{\Theta=\Theta_t},\theta \in \Theta)\in \R^{d_{net}}$, i.e., collection of the gradients of the network output with respect to its weights. Since the NTF is a first-order term, it can be used to linearise the DNN output about the point $\Theta_t$. Associated with the NTF is a \emph{neural tangent kernel} (NTK), which is given by $K_t(x,x')=\psi^\top_{x,t}\psi_{x',t}$. 
$2.$ In the \emph{trajectory} based analysis, one looks at the dynamics of error $e_t$  (i.e., difference between predicted and true values) at time $t$. For a small step-size $\alpha_t>0$, the error dynamics follows a linear recursion given by: $e_{t+1}=e_t-\alpha_tK_te_t$, where $K_t$ is the NTK matrix obtained on the the dataset. Thus, the spectral properties of $K_t$ is key to achieve zero training error. \hfill\\
\textbf{Related Work and Gaps:} \hfill\\
\emph{Optimisation:} \cite{ntk} were the first to point out the role of NTK in DNNs. Using the trajectory based analysis, \cite{dudnn} show that in fully connected DNNs with $w=\Omega(poly(n)2^{O(d)})$, and in residual neural networks (ResNets) with $w=\Omega(poly(n,d))$ gradient descent converges to zero training loss.  However, Question I, i.e., `why depth helps in training?' is unresolved.\hfill\\%This shows that ResNets are better than fully connected DNNs, based on the fact that the dependence on the number of layers improves exponentially for ResNets. 
\emph{Generalisation:} \cite{arora2019exact,arora,cao2019generalization} use NTK to provide generalisation bounds as well as propose pure-kernel methods. However, couple of issues remain unresolved: firstly, if the DNNs are only linear learners with random NTFs, then it suggests that no feature learning happens in DNNs, and secondly, it was observed in prior experiments that the DNNs perform better than their corresponding NTK counterparts \cite{arora2019exact,lee2019wide}. \hfill\\

\textbf{Our Results:} We now highlight the contributions in this paper.\\
$1.$ \textbf{Neural Path Feature:} Central idea in this paper is the `path-view': to regard paths and gates as basic building blocks of a DGN.  A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Let $[P]=\{1,\ldots,P\}$ be an enumeration of all the paths, then the \emph{neural path feature} (NPF) is defined $\phi_{x,t}=(x(\I(p))A_t(x,p),p\in[P])\in\R^P$, wherein, for a path $p$, $\I(p)$ is the input node at which it starts, and  $A_t(x,p)$ is its activation level is equal to the product of the gating values in the path. Using the NPF, the output is expressed as $\hat{y}_t(x)=\phi^\top_{x,t}v_t$, where, $v_t=(v_t(p),p\in[P])\in \R^P$, with $v_t(p)$ is the value of a path $p$ and is equal to the product of its weights.\hfill\\
$2.$ \textbf{Neural Path Kernel (NPK)}  is given by $H_t(x,x')=\phi^\top_{x,t}\phi_{x',t}$, where $x,x'\in\R^{d_in}$ are the inputs to the DGN. We show that the NPK whitens as depth increases.\hfill\\
$3.$ \textbf{Optimisation:}  When weights are initialised at random (with variance $\sigma$) and statistically independent of the activations, we show that $\E{K_0}= CH_0$, (for some constant $C>0$) and  $Var\left[K_0(x,x')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$ (for $\sigma^2=O(\frac{1}w)$). Thus,\\
(i) For a fixed $d$, increasing $w$ ensures $K_0$ converges to $\E{K_0}$, and since NPK matrix whitens as depth increases, increasing depth till a point helps in training performance.\hfill\\
(ii) For a fixed $w$, increasing $d$ makes the entries of $K_0$ deviate from $\E{K_0}$, thus degrading the spectrum of $K_0$. Thus increasing depth beyond a point hurts training performance. \hfill\\
$4.$ \textbf{Feature Learning:} Since $\hat{y}_t(x)=\phi^\top_{x,t}v_t$, the gradient flow has two components: an $(\nabla v_t)^\top\phi_{x,t}$ term, which we call the \emph{value gradient}, learns the path values (which are akin to weight vector in a standard linear approximation) keeping the NPFs fixed, and a $(\nabla \phi_{x,t})^\top v_t $ term, which we call the \emph{feature gradient} learns the NPFs keeping the path values fixed. We present preliminary theory connecting the trajectory method and feature learning. \hfill\\
$5.$ \textbf{Generalisation:} We experiment with two gating regimes namely the static, wherein, $A_t=A_0,\forall t \geq 0$, and the dynamic, wherein $A_t$ changes during training. The experiments on MNIST and CIFAR show that, better generalisation happens when the activations $A_t$ change with time. In particular, for binary classification, we verify that the quantity $y^\top (\widehat{H_t})^{-1}y$\footnote{For a matrix $H$, $\hat{H}=\frac{1}{trace(H)}H$ .} reduces as the training progresses. This shows that the eigenspaces of the NPK matrix align themselves in accordance to the labelling function. These experiments supports the fact that NPF and the NPK are indeed learnt during training in a manner to improve the generalisation performance. \hfill\\
\end{comment}
\begin{comment}
\textbf{Paths:}  A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Let $[P]=\{1,\ldots,P\}$ be an enumeration of all the paths, the zeroth and first order quantities of a DNN can then be expressed as:
\begin{align}
\label{eq:zero}\text{(Zeroth Order)}\quad: \quad \hat{y}_t(x)&=\sum_{p\in [P]}x(\I(p))A_t(x,p)v_t(p)={\phi^\top_{x,t}} v_t\\
\text{(First Order)}\quad: \quad \partial \hat{y}_t(x)&= \sum_{p\in [P]}x(\I(p)) A_t(x,p) \partial((v_t(p))+\sum_{p\in [P]}x(\I(p)) \partial(A_t(x,p)) v_t(p)\nn\\
\label{eq:first}\psi_{x,t}=(\partial v_t)^\top\phi_{x,t}+ (\partial \phi_{x,t})^\top v_t,
\end{align}
where, for a path $p$, $\I(p)$ is the input node at which it starts, $v_t(p)$ is its value given by the product of its weights and for input $x\in\R^{d_{in}}$, $A_t(x,p)$ is its activation level given by the product of the gating values in the path. \hfill\\
\textbf{Independent Gates and Activations:} In order to understand the role $A_t$, we handle the gating values as independent variables\footnote{In DNN with ReLU, the gates are implicit: a gate is \emph{on} only if pre-activation input is positive.}. All the theoretical results are under the assumption that $A_0$ is statistically independent of $\Theta_0$. \hfill\\
\textbf{Neural Path Feature and Kernel:} The \emph{neural path feature} (NPF) is given by $\phi_{x,t}=(x(\I(p))A_t(x,p))\in \R^P$ (see \eqref{eq:zero}) , and an associated \emph{neural path kernel} (NPK) to $H_t(x,x')=\phi^\top_{x,t}\phi_{x',t}$. The NPK has special structure: $H_t(x,x')=(x^\top x')\lambda_t(x,x')$, where $\lambda_t(x,x')$ is a measure of similarity based on the path activation levels for inputs $x,x'\in\R^{d_{in}}$. \hfill\\
\textbf{NPK vs NTK: } Under symmetric Bernoulli initialisation, we have $\E{K_0} = C H_0$, ($C>0$ is a constant), where $K_0$ is the NTK matrix at $t=0$. \hfill\\
\textbf{Optimisation I:} We argue that $\frac{\lambda_0(x,x')}{\lambda_0(x,x)}$ decays at an exponential rate with depth, i.e., the Gram matrix $K_0$ whitens with depth. Thus increasing depth helps in training. \hfill\\
\textbf{Optimisation II:} We show that $Var\left[K_0(x,x')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$ (for $\sigma^2=O(\frac{1}w)$). For a fixed $d$, increasing $w$ ensures $K_0$ converges to $\E{K_0}$. However, for a fixed $w$, increasing $d$ makes the entries of $K_0$ deviate from $\E{K_0}$, thus degrading the spectrum of $K_0$. Thus increasing depth beyond a point hurts training performance. \hfill\\
\textbf{Generalisation:} We experiment with two gating regimes namely the static, wherein, $A_t=A_0,\forall t \geq 0$, and the dynamic, wherein $A_t$ changes during training. The experiments on MNIST and CIFAR show that, better generalisation happens when the activations $A_t$ change with time. In particular, for binary classification, we verify that the quantity $y^\top (\widehat{H_t})^{-1}y$\footnote{For a matrix $H$, $\hat{H}=\frac{1}{trace(H)}H$ .} reduces as the training progresses. This shows that the eigenspaces of the NPK matrix align themselves in accordance to the labelling function. These experiments supports the fact that NPF and the NPK are indeed learnt during training in a manner to improve the generalisation performance. \hfill\\
\end{comment}