\section{Introduction}
Understanding optimisation and generalisation in deep neural networks (DNNs) trained using first-order method such as gradient descent (GD) is an important problem in machine learning. Despite having a non-convex loss surface, GD achieves zero training error in over-parameterised DNNs where the number of parameters exceeds the size of the dataset. Interestingly, \cite{ben} demonstrated that practical DNNs have enough capacity to achieve zero training loss with even random labelling of standard datasets such as MNIST and CIFAR. However, when trained with true labels, such networks achieve zero training error and also exhibit good performance on test data. 

In this paper, we consider fully connected DNNs with ReLU activations, with $d$ layers and $w$ hidden units per layer. In what follows ,we denote the dataset by $(x_s,y_s)_{s=1}^n\in \R^{d_{in}}\times \R$, and the parameters of the network by $\Theta\in \R^{d_{net}}$, the network output for an input $x\in \R^{d_{in}}$ by $\hat{y}_{\Theta}(x)$.

Some of the recent works (\citenum{dudnn,du2018,arora2019exact,dudln}) on this problem have made use of the \emph{neural tangent features} (NTFs) and the \emph{trajectory based analysis}, which we describe in brief. The NTF of an input $x\in \R^{d_{in}}$ is defined as $\psi_{x,\Theta}=\left(\partial_{\theta}\hat{y}_{\Theta}(x),\theta\in\Theta\right)\in\R^{d_{net}}$\footnote{Here $\partial_{\theta}(\cdot)$ stands for $\frac{\partial (\cdot)}{\partial \theta}$}, i.e., the gradient of the network output with respect to the weights. By collecting the NTFs of all the inputs examples in the dataset, we can form the NTF matrix $\Psi=(\psi_{x_s,\Theta},s\in[n])\in\R^{d_{net}\times n}$\footnote{$[n]=\{1,\ldots,n\}$}. The trajectory based analysis looks at the dynamics of the error $e_t=(\hat{y}_{\Theta_t}-y_s,s\in[n])\in\R^n$. For a small enough step-size $\alpha_t>0$ of the GD procedure, the error dynamics is given by:
\begin{align}
e_{t+1}=e_t-\alpha_t K_{\Theta_t} e_t,
\end{align}
where $K_{\Theta_t}\in\R^{n\times n}$ is the \emph{neural tangent kernel} (NTK) matrix give by $K_{\Theta}=\Psi^\top_{\Theta} \Psi_{\Theta}$. Thus, the spectral properties, and in particular, $\rho_{\min}(K_{\Theta_t})$ the minimum eigenvalue of $K_{\Theta_t}$ dictates the rate of convergence. Under randomised initialisation, and in the limit of infinite width an interesting property emerges: the parameters of the DNN deviate very little during training, i.e. $\Theta_t\approx \Theta_0$. In particular, $K_{\Theta_t}\approx K_{\Theta_0}$, $K_{\Theta_0}\ra K^{(d)}$, i.e., the NTK stays almost constant through training and the NTK matrix at initialisation $K_{\Theta_0}$ converges to a deterministic matrix $K^{(d)}$ (see \Cref{sec:exact} for exact expression of $K^{(d)}$. Thus in the `large-width' regime, zero training error can be achieved if $\rho_{\min}(K^{(d)})>0$ which holds as long as the training data is not degenerate (\citenum{dudnn,arora2019exact}). In the `large-width' regime, \cite{arora2019exact} show that the fully trained DNN is equivalent to kernel regression with $K^{(d)}$. Hence, a trained DNN enjoys the generalisation ability of its corresponding $K^{(d)}$ matrix in the `large-width' regime. \cite{cao2019generalization} show that in the `large-width' regime, the DNN is almost a linear learner with the random NTFs, and showed a generalisation bound in the form of $\tilde{\mathcal{O}}\left(d\cdot\sqrt{y^\top K^{(d)} y/n}\right)$\footnote{$a_t=\mathcal{O}(b_t)$ if $\lim\sup_{t\ra\infty}|a_t/b_t|<\infty$, and $\tilde{\mathcal{O}}(\cdot)$ is used to hide logarithmic factors in $\mathcal{O}(\cdot)$.}, where $y=(y_s,s\in[n])\in\R^n$ is the labelling function.

\textbf{Research Gap I (Feature Learning):} In the `large-width' i.e., fixed NTF/NTK regime, the DNNs are linear learner using the random NTFs at random initialisation. This implies that there is little or no feature learning. Is this true?\\
\textbf{Research Gap II (Finite vs Infinite):} \cite{arora2019exact} note that, while pure-kernel methods based on the limiting NTK (i.e., $K^{(d)}$) outperform other state-of-the-art kernel methods, the finite width DNNs (CNNs) still outperform their NTK (CNTK)\footnote{CNTK: Convolutional Neural Tangent Kernel, the NTK for CNNs} counterpart. Can we explain this gap?
\subsection{Contributions}
$\bullet$ (\Cref{sec:pathgate}) Central idea in this paper is the 'path-view', wherein, we decompose the computation in such DNNs into paths and gate. Intuitively speaking, when a particular example is presented to the network only a sub-set of the activations \emph{on} and the output is obtained as the sum of the contribution of the various paths in the sub-network formed by such \emph{on} gates. We explicitly encode the state of the gates in a novel \emph{neural path feature} (NPFs), which is zeroth-order feature. Using the `path-view' we characterise the information flow via two different sub-networks, i) of active gates which are on and ii) of sensitive gates which are on the verge of turning on or off. We also show the NTK can be decomposed into $K_{\Theta}=K^v_{\Theta]+K^{\phi}_{\Theta}+M$, where $K^v_{\Theta}$ is the NTK of values responsible for optimisation with fixed features, and $K^{\phi}_{\Theta}$ is the NTK of features responsible for feature learning, and $M$ is a symmetric matrix. \\
$\bullet$ We study a novel fixed NPF regime. Here, we show that as width increases to infinity the optimisation and generalisation properties are captured by the \emph{neural path kernel} (NPK), the kernel associated with the NPFs. In the case of finite width,  we show that when optimising  using GD i) increasing depth till a point helps in training and ii) increasing depth beyond a point hurts training.\\
$\bullet$ We show via experiments, that NPFs are learnt during training, and such learning improves generalisation performance.



\textbf{Organisation:} In \Cref{sec:pathgate}, we introduce the `path-view', wherein, we decompose the computation in a DNN in terms of the paths and the gates. Using the `path-view', we define a novel \emph{neural path feature} (NPF) and a novel \emph{neural path value} (NPV), and express the output as an inner product of the NPV and NPF. In \Cref{sec:our} we  discuss how `path-view' helps in capturing feature learning, and in particular, show the NTK can be decomposed into $K_t=K^v_t+K^{\phi}_t+M$, where $K^v_t$ is the NTK of values responsible for optimisation with fixed features, and $K^{\phi}_t$ is the NTK of features responsible for feature learning, and $M$ is a symmetric matrix. 
In \Cref{sec:optimisation}-\Cref{th:main}, we consider optimisation and generalisation with fixed NPFs. In \Cref{sec:generalisation},  we show that NPFs obtained from a pre-trained network generalises better than NPFs from a randomly initialised network. This implies that the NPFs are learned during training. Since \Cref{th:main} allows us to use $H_0$ in the place of $K_0$ in the generalisation bounds \cite{cao2019generalization}, we verify that the norm of the labelling function measured with respect to the inverse of the trace normalised NPK reduces with time.\\
\begin{comment}\textbf{Lesson Learnt:} The `path-view' enables us to model and study the dynamics of the gates, and hence the learning of NPFs. Based on the theory and experiments, we can say that \emph{``understanding deep learning requires understanding the dynamics of the gates".}\end{comment}
\begin{comment}
\textbf{Lesson Learnt:} Gradient is a first-order information, and learning with GD is essentially a process of integration, and naturally its outcome is a zeroth-order term. It turns out that splitting this zeroth-order term into NPFs and NPVs is quite useful, in particular, this enables us to identify the role of NPFs. \emph{Understanding deep learning requires understanding the dynamics of the gates.}
\end{comment}
\begin{comment}
$1.$ \textbf{Deep Gated Networks (DGNs):} Here, the output of a single hidden neuron is obtained as a product of its pre-activation input ($\in \R$) and a gating value ($\in [0,1]$ ). The DGNs we consider consists of two networks i) a value network which holds the pre-activation input, layer output and layer weights $\Theta\in \R^{d_{net}}$, and the final output, and ii) a separate gating network (with its own internal pre-activation inputs, layer output, and layer weights $\Tg\in \R^{d_{net}}$), which provides the gating values. 
DNN with ReLU activation is a special case, wherein, the value network and gating network are \emph{coupled} i.e., $\Theta=\Tg$.\\
$2.$ \textbf{Neural Path Feature and Kernel (NPF and NPK):} We express the output as the sum total of the contributions from various paths. A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Let $[P]=\{1,\ldots,P\}$ be an enumeration of all the paths, then the contribution of a path in a DGN is the product of the signal at the input node, all the gating values of the hidden neurons and all the weight through which it passes. At time $t$, and for an input $x\in\R^{d_{in}}$,  the output of a DGN is recovered as $\hat{y}_t(x)=\ip{\phi_{x,t},v_t}$, where $\phi_{x,t}\in \R^P$ is the \emph{neural path feature} (NPF). Here, for a path $p\in[P]$, $\phi_{x,t}(p)$ holds the product of the signal at its input node and all its gating values, and  $v_t=(v_t(p),p\in[P])\in\R^P$, with $v_t(p)$ holding the product of the weights in path $p$. The associated \emph{neural path kernel} (NPK) matrix $H_t$ has a special \emph{Hadamard} structure, i.e., $H_t=\Sigma \odot \lambda_t$, where $\Sigma$ is the data \emph{Gram matrix}, $\lambda_t(x,x')$ measure the overlap of active sub-networks for inputs $x,x'\in\R^{d_{in}}$, and $\odot$ is the Hadamard product.\\
$3.$ \textbf{Gate Copying:} Contrary to NTK based works \cite{jacot18,jacot19,cao2019generalization,arora2019exact}, wherein, the DNNs are linear learners using fixed NTFs at random initialisation, the DGN framework enables the study the optimisation and generalisation with different NPFs. Two interesting cases arise when NPFs are obtained from a gating network which is a i)  pre-trained DNN, and ii) randomly initialised DNN.\\
$4.$ \textbf{Optimisation:} We consider DGN training in the fixed gating regime, where, the NPFs in the gating network are kept fixed through training, and only the path values are trained. When the weights are initialised statistically independent of the NPFs, we show, in \Cref{th:main}, that as width approaches $\infty$, $K_0\ra C H_0$ (for a constant $C>0$). This result implies that in the limit of infinite width, increasing depth leads to whitening of $\lambda_0$ and thus helps in training, and for finite width increasing depth beyond a point hurts training variance in entries of $K_0$ which depend on $d$. \\
$5.$ \textbf{Generalisation:}  We show (on MNIST and CIFAR datasets) that DGN trained with fixed NPFs obtained from a pre-trained gating network generalise better than DGNs trained with fixed NPFs obtained from a randomly initialised gating network. We verify that (experimentally with a `Binary'-MNIST dataset) that the norm of the label function measured with respect to the normalised NPK matrix reduces during training. We note that using \Cref{th:main} to replace $K_0$ with $H_0$ in the generalisation bounds of \cite{cao2019generalization}, would provide the theoretical justification of why NPFs from pre-trained gating networks generalise better.\\
$6.$ \textbf{Feature Learning:} The gradient $\partial \hat{y}$ has two components i) a \emph{value gradient} term $(\partial v)^\top \phi$ which learns path values with fixed NPFs and ii) a \emph{feature gradient} term $(\partial \phi)^\top v$ which learns the features themselves. The value gradient flows through the sub-network of \emph{active} paths, and the feature gradient flows the sub-network of \emph{sensitive} paths which contain gates that are in the verge of switching \emph{on/off}. In particular, when $\Tg$ and $\Theta$ are initialised statistically independent of each other, we show that $\E{K_0}=(C\Sigma\odot\lambda_0+C'\Sigma\odot\delta_0)$, where $\delta_0(x,x')$ is a measure of overlap of sensitive sub-networks for inputs $x,x'\in\R^{d_{in}}$.
\end{comment}
\begin{comment}
Say $[P]=\{1,\ldots,P\}$ be an enumeration of the paths, 
$1.$ At time $t$, for an input $x\in\R^{d_{in}}$, we define a novel (zeroth-order) \emph{neural path feature} (NPF) $\phi_{x,t}\in \R^P$, where $P$ is the total number of paths, and for a path $p$, $\phi_{x,t}(p),\p\in[P]$\footnote{$[P]=\{1,\ldots,P\}$} is equal to the product of the signal at its input node and its activity. The output can be written as $\hat{y}_t(x)={\phi^\top_{x,t}} v_t$, where $v_t=(v_t(p),p\in[P])\in\R^P$ is the vector of path values with $v_t(p)$ given by the product of the weights in the path.\\
%\text{(First Order)}\quad: \quad \partial \hat{y}_t(x)&= \sum_{p\in [P]}x(\I(p)) A_t(x,p) \partial((v_t(p))+\sum_{p\in [P]}x(\I(p)) \partial(A_t(x,p)) v_t(p)\nn\\
%\label{eq:first}&\text{(First- Order/Neural Tangent Feature)}&\quad:&\quad   \nabla \hat{y}_t(x)=\psi_{x,t}=(\nabla v_t )^\top \phi_{x,t} + (\nabla \phi_{x,t})^\top v_t,
$2.$ The associated \emph{neural path kernel} (NPK) matrix $H_t$ has a special \emph{Hadamard} structure, i.e., $H_t=\Sigma \odot \lambda_t$, where $\Sigma$ is the data \emph{Gram matrix}, $\lambda_t(x,x')$ measure the overlap of active sub-networks for inputs $x,x'\in\R^{d_{in}}$, and $\odot$ is the Hadamard product.\\
$3.$ Say $\Theta$ are the network weights, now, since the NPFs are solely based on the gating information, the NPFs can be obtained from a separate gating network, whose weights are $\Tg\neq\Theta$. The NPFs could be derived from i) a randomly initialised gating network or ii) a pre-trained gating network. This way we can study optimisation and generalisation of different kinds of NPFs, which is in sharp contrast with the NTK based works \cite{jacot18,jacot19,cao2019generalization,arora2019exact}, wherein, the DNNs are linear learners using fixed NTFs at random initialisation.\\
$4$. The gradient $\partial \hat{y}$ has two components i) a $(\partial v)^\top \phi$ which learns path values with fixed NPFs and ii) a $(\partial \phi)^\top v$ which learns the features themselves.\\
\textbf{Main Results:} We present the analysis and experiments in two regimes namely the fixed gating regime, wherein, the NPFs are fixed and the adaptable gating regime, wherein, the NPFs are learnt.\\
$\bullet$ \textbf{Optimisation:} When the gates are fixed, under appropriate statistical assumptions, we show that (\Cref{th:main}) as width approaches $\infty$, $K_0\ra C H_0$ (for a constant $C>0$). This result implies that in the limit of infinite width, increasing depth leads to whitening of $\lambda_0$ and thus helps in training, and for finite width increasing depth beyond a point hurts training variance in entries of $K_0$ which depend on $d$. \\
$\bullet$ \textbf{Generalisation:} We show via experiments (on standard MNIST and CIFAR datasets) that DGNs with fixed NPFs derived from a pre-trained gating network generalise better than fixed NPFs obtained from a randomly initialised gating network. \\
We show on an experiment in `Binary'-MNIST dataset that the norm of the label function measured with respect to the normalised NPK matrix reduces during training.
$\bullet$ 
\end{comment}
\begin{comment}
$1.$ \textbf{Information Flow:} We introduce a host of novel terms/expressions and concepts related to information flow in DGNs. This includes i) the zeroth-order (see \eqref{eq:zero}) \emph{neural path feature} (NPF) and the \emph{neural path kernel} (NPK) denoted by $H_t$, which are defined using the gating information, ii) the first-order (see \eqref{eq:first}) terms namely, the \emph{value gradient} given by $(\nabla v_t )^\top \phi_{x,t}$, and the \emph{feature gradient} given by $(\nabla \phi_{x,t})^\top v_t$, iii)  two sub-network of paths namely, an \emph{active} sub-network, which is the set of active paths that hold the memory for an input, through which the value gradient flows and a \emph{sensitive} sub-network, which is the set of sensitive paths, through which the feature gradient flows. We also show that for random initialisation, the paths exhibit a \emph{disentanglement} property, which is very key in our results.\\
%$1.$ \textbf{Representation and Generalisation:} By fixing the activation (i.e., $A_t=A_0,\forall t\geq 0$), we train DGNs with different fixed \emph{neural path features} (NPFs), and demonstrate that the NPFs are key for generalisation. Further, we show that the NPFs are learnt during training. This shows that NPFs can be regarded as the true hidden features in a deep network.\\
\end{comment}
\begin{comment}
$3.$ \textbf{Neural Path Features} are the information stored in the gates of a deep network. Our main result connects the NPK to the NTK, i.e., we show that, when the weights are randomly initialised (with an appropriate scale) in a manner statistically independent of the gates, i)  $\E{K_0}=H_0$, and ii) $Var\left[K_0(x,x')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$.\\
$\quad\bullet$ \textbf{Optimisation:}   We argue that  $H_0$ whitens with depth, and hence, increasing depth helps in training. Increasing depth further causes $K_0$ to deviate from $H_0$, and hence, degrades the spectrum of $K_0$, thereby affecting training performance.\\
$\quad\bullet$ \textbf{Generalisation:} By fixing the activation (i.e., $A_t=A_0,\forall t\geq 0$), we train DGNs with different fixed NPFs, and demonstrate that the NPFs are key for generalisation. Further, we show that the NPFs are learnt during training. This shows that NPFs can be regarded as the true hidden features in a deep network.\\
$3.$ \textbf{NPF Learning:} We present preliminary results connecting NPF learning and the trajectory method.
\end{comment}
\begin{comment}
Understanding optimisation and generalisation of deep neural networks (DNNs) trained using first-order method such as (stochastic) gradient descent is an important problem in machine learning. In this paper, we throw light on the following two questions:\hfill\\
\textbf{Question I (Optimisation):} \emph{What is the role of width and depth in training of DNNs? Why increasing depth till a point helps in training? Why increasing the depth beyond hurts training?}\\
\textbf{Question II (Generalisation):} \emph{What are the hidden features in a DNN? Are these features learned? and if so, how?} \hfill\\

\textbf {Background:} We consider a fully connected deep gated network (DGN) of depth $d$, and width $w$, weights $\Theta\in\R^{d_{net}}$, which, accepts an input $x\in \R^{d_{in}}$ and produces an output $\hat{y}_{\Theta_t}(x)\in \R$. In a DGN, the output of a hidden neuron is obtained as a product of its pre-activation input and a gating value. DNNs with ReLU activations are special cases DGNs. Some of the recent works to understand SGD in relation to the optimisation and generalisation in DNNs make use of the \emph{neural tangent features} (NTFs) and the \emph{trajectory} based analysis, which we describe in brief.\hfill\\
$1.$The NTF for an input $x\in \R^{d_{in}}$ is defined as $\psi_{x,t}=(\frac{\partial \hat{y}_{\Theta}}{\partial \theta}|_{\Theta=\Theta_t},\theta \in \Theta)\in \R^{d_{net}}$, i.e., collection of the gradients of the network output with respect to its weights. Since the NTF is a first-order term, it can be used to linearise the DNN output about the point $\Theta_t$. Associated with the NTF is a \emph{neural tangent kernel} (NTK), which is given by $K_t(x,x')=\psi^\top_{x,t}\psi_{x',t}$. 
$2.$ In the \emph{trajectory} based analysis, one looks at the dynamics of error $e_t$  (i.e., difference between predicted and true values) at time $t$. For a small step-size $\alpha_t>0$, the error dynamics follows a linear recursion given by: $e_{t+1}=e_t-\alpha_tK_te_t$, where $K_t$ is the NTK matrix obtained on the the dataset. Thus, the spectral properties of $K_t$ is key to achieve zero training error. \hfill\\
\textbf{Related Work and Gaps:} \hfill\\
\emph{Optimisation:} \cite{ntk} were the first to point out the role of NTK in DNNs. Using the trajectory based analysis, \cite{dudnn} show that in fully connected DNNs with $w=\Omega(poly(n)2^{O(d)})$, and in residual neural networks (ResNets) with $w=\Omega(poly(n,d))$ gradient descent converges to zero training loss.  However, Question I, i.e., `why depth helps in training?' is unresolved.\hfill\\%This shows that ResNets are better than fully connected DNNs, based on the fact that the dependence on the number of layers improves exponentially for ResNets. 
\emph{Generalisation:} \cite{arora2019exact,arora,cao2019generalization} use NTK to provide generalisation bounds as well as propose pure-kernel methods. However, couple of issues remain unresolved: firstly, if the DNNs are only linear learners with random NTFs, then it suggests that no feature learning happens in DNNs, and secondly, it was observed in prior experiments that the DNNs perform better than their corresponding NTK counterparts \cite{arora2019exact,lee2019wide}. \hfill\\

\textbf{Our Results:} We now highlight the contributions in this paper.\\
$1.$ \textbf{Neural Path Feature:} Central idea in this paper is the `path-view': to regard paths and gates as basic building blocks of a DGN.  A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Let $[P]=\{1,\ldots,P\}$ be an enumeration of all the paths, then the \emph{neural path feature} (NPF) is defined $\phi_{x,t}=(x(\I(p))A_t(x,p),p\in[P])\in\R^P$, wherein, for a path $p$, $\I(p)$ is the input node at which it starts, and  $A_t(x,p)$ is its activation level is equal to the product of the gating values in the path. Using the NPF, the output is expressed as $\hat{y}_t(x)=\phi^\top_{x,t}v_t$, where, $v_t=(v_t(p),p\in[P])\in \R^P$, with $v_t(p)$ is the value of a path $p$ and is equal to the product of its weights.\hfill\\
$2.$ \textbf{Neural Path Kernel (NPK)}  is given by $H_t(x,x')=\phi^\top_{x,t}\phi_{x',t}$, where $x,x'\in\R^{d_in}$ are the inputs to the DGN. We show that the NPK whitens as depth increases.\hfill\\
$3.$ \textbf{Optimisation:}  When weights are initialised at random (with variance $\sigma$) and statistically independent of the activations, we show that $\E{K_0}= CH_0$, (for some constant $C>0$) and  $Var\left[K_0(x,x')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$ (for $\sigma^2=O(\frac{1}w)$). Thus,\\
(i) For a fixed $d$, increasing $w$ ensures $K_0$ converges to $\E{K_0}$, and since NPK matrix whitens as depth increases, increasing depth till a point helps in training performance.\hfill\\
(ii) For a fixed $w$, increasing $d$ makes the entries of $K_0$ deviate from $\E{K_0}$, thus degrading the spectrum of $K_0$. Thus increasing depth beyond a point hurts training performance. \hfill\\
$4.$ \textbf{Feature Learning:} Since $\hat{y}_t(x)=\phi^\top_{x,t}v_t$, the gradient flow has two components: an $(\nabla v_t)^\top\phi_{x,t}$ term, which we call the \emph{value gradient}, learns the path values (which are akin to weight vector in a standard linear approximation) keeping the NPFs fixed, and a $(\nabla \phi_{x,t})^\top v_t $ term, which we call the \emph{feature gradient} learns the NPFs keeping the path values fixed. We present preliminary theory connecting the trajectory method and feature learning. \hfill\\
$5.$ \textbf{Generalisation:} We experiment with two gating regimes namely the static, wherein, $A_t=A_0,\forall t \geq 0$, and the dynamic, wherein $A_t$ changes during training. The experiments on MNIST and CIFAR show that, better generalisation happens when the activations $A_t$ change with time. In particular, for binary classification, we verify that the quantity $y^\top (\widehat{H_t})^{-1}y$\footnote{For a matrix $H$, $\hat{H}=\frac{1}{trace(H)}H$ .} reduces as the training progresses. This shows that the eigenspaces of the NPK matrix align themselves in accordance to the labelling function. These experiments supports the fact that NPF and the NPK are indeed learnt during training in a manner to improve the generalisation performance. \hfill\\
\end{comment}
\begin{comment}
\textbf{Paths:}  A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Let $[P]=\{1,\ldots,P\}$ be an enumeration of all the paths, the zeroth and first order quantities of a DNN can then be expressed as:
\begin{align}
\label{eq:zero}\text{(Zeroth Order)}\quad: \quad \hat{y}_t(x)&=\sum_{p\in [P]}x(\I(p))A_t(x,p)v_t(p)={\phi^\top_{x,t}} v_t\\
\text{(First Order)}\quad: \quad \partial \hat{y}_t(x)&= \sum_{p\in [P]}x(\I(p)) A_t(x,p) \partial((v_t(p))+\sum_{p\in [P]}x(\I(p)) \partial(A_t(x,p)) v_t(p)\nn\\
\label{eq:first}\psi_{x,t}=(\partial v_t)^\top\phi_{x,t}+ (\partial \phi_{x,t})^\top v_t,
\end{align}
where, for a path $p$, $\I(p)$ is the input node at which it starts, $v_t(p)$ is its value given by the product of its weights and for input $x\in\R^{d_{in}}$, $A_t(x,p)$ is its activation level given by the product of the gating values in the path. \hfill\\
\textbf{Independent Gates and Activations:} In order to understand the role $A_t$, we handle the gating values as independent variables\footnote{In DNN with ReLU, the gates are implicit: a gate is \emph{on} only if pre-activation input is positive.}. All the theoretical results are under the assumption that $A_0$ is statistically independent of $\Theta_0$. \hfill\\
\textbf{Neural Path Feature and Kernel:} The \emph{neural path feature} (NPF) is given by $\phi_{x,t}=(x(\I(p))A_t(x,p))\in \R^P$ (see \eqref{eq:zero}) , and an associated \emph{neural path kernel} (NPK) to $H_t(x,x')=\phi^\top_{x,t}\phi_{x',t}$. The NPK has special structure: $H_t(x,x')=(x^\top x')\lambda_t(x,x')$, where $\lambda_t(x,x')$ is a measure of similarity based on the path activation levels for inputs $x,x'\in\R^{d_{in}}$. \hfill\\
\textbf{NPK vs NTK: } Under symmetric Bernoulli initialisation, we have $\E{K_0} = C H_0$, ($C>0$ is a constant), where $K_0$ is the NTK matrix at $t=0$. \hfill\\
\textbf{Optimisation I:} We argue that $\frac{\lambda_0(x,x')}{\lambda_0(x,x)}$ decays at an exponential rate with depth, i.e., the Gram matrix $K_0$ whitens with depth. Thus increasing depth helps in training. \hfill\\
\textbf{Optimisation II:} We show that $Var\left[K_0(x,x')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$ (for $\sigma^2=O(\frac{1}w)$). For a fixed $d$, increasing $w$ ensures $K_0$ converges to $\E{K_0}$. However, for a fixed $w$, increasing $d$ makes the entries of $K_0$ deviate from $\E{K_0}$, thus degrading the spectrum of $K_0$. Thus increasing depth beyond a point hurts training performance. \hfill\\
\textbf{Generalisation:} We experiment with two gating regimes namely the static, wherein, $A_t=A_0,\forall t \geq 0$, and the dynamic, wherein $A_t$ changes during training. The experiments on MNIST and CIFAR show that, better generalisation happens when the activations $A_t$ change with time. In particular, for binary classification, we verify that the quantity $y^\top (\widehat{H_t})^{-1}y$\footnote{For a matrix $H$, $\hat{H}=\frac{1}{trace(H)}H$ .} reduces as the training progresses. This shows that the eigenspaces of the NPK matrix align themselves in accordance to the labelling function. These experiments supports the fact that NPF and the NPK are indeed learnt during training in a manner to improve the generalisation performance. \hfill\\
\end{comment}