\section{Fixed NPF regime}
\begin{comment}
\begin{wrapfigure}{h}{0.3\textwidth}
\includegraphics[scale=0.25]{figs/regime.png}
\caption{\label{fig:regime} Regimes.}
\end{wrapfigure}
\end{comment}
Based on the kernel expression in \eqref{eq:kerneldecomp} we write down the GD dynamics in three different regimes namely (see \Cref{tb:dynamics}) i) standard, ii) `large-width', and iii) fixed NPF. 

$\bullet$ Practical DNNs with ReLU activations and finite width operate in the standard regime. By replacing the ReLU by soft-ReLU we can write down the GD dynamics that incorporates NPF learning as shown in the leftmost column of \Cref{tb:dynamics}. In the standard regime, $K^v_{\Theta}(s,s')=\ip{\psi^v_{x_s,\Theta},\psi^v_{x_{s'},\Theta}}=\phi^\top_{x_s,\Theta}(\partial_{\theta}v_{\Theta})(\partial_{\theta}v_{\Theta})^\top\phi_{x_{s'},\Theta}$, because of which, $K^v_{\Theta_t}$ changes with time as the NPFs  $\phi_{x_s,\Theta_t},s\in[n]$ change with time. 

$\bullet$ In the `large-width' regime [\citenum{arora2019exact}], the NPFs and the NTFs do not deviate much from initialisation (see middle column in \Cref{tb:dynamics}), and GD dynamics is dictated by a deterministic matrix $K^{(d)}$ (an expression for $K^{(d)}$ is presented in \Cref{sec:prior}). 

$\bullet$ In this paper we consider a fixed NPF regime, wherein, we force $\psi^{\phi}_{\cdot,\Theta}=0$. The dynamics in this regime is given in the rightmost column in \Cref{tb:dynamics}.
\FloatBarrier
\begin{table}[h]\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{| l | l | l |}\hline
Finite $w$ with NPF learning (standard) & Large-Width & Fixed NPF\\\hline
$\dot{\Theta}_t=-\sum_{s=1}^n (\psi^v_{x_s,t}+\psi^{\phi}_{x_s,t})e_t(s)$ &$\dot{\Theta}_t\ra 0$ &$\dot{\Theta}_t=-\sum_{s=1}^n \psi^v_{x,t}e_t(s)$ \\\hline
$\dot{\phi}_{x_s,t}(p)=x(\I_0(p))\sum_{\theta\in\Theta}\partial_{\theta}A_t(x_s,p)\dot{\theta}_t$ &$\dot{\phi}_{x_s,t}(p)\ra 0$ &$\dot{\phi}_{x_s,t}(p)=0$\\\hline
$\dot{v}_t(p)=\sum_{\theta\in\Theta}\partial_{\theta}v_t(p)\dot{\theta}_t$ &$\dot{v}_t(p)\ra 0$ &$\dot{v}_t(p)=\sum_{\theta\in\Theta}\partial_{\theta}v_t(p)\dot{\theta}_t$\\\hline
$\dot{e}_t=-(K^v_t+K^{\phi}_t+K^{cross}_{t})e_t$ &$\dot{e}_t=-K^{(d)}e_t$ &$\dot{e}_t=-K^v_te_t$\\\hline
\end{tabular}
}
\caption{Dynamics in various regimes. Here $p\in[P], s\in[n]$.}
\label{tb:dynamics}
\end{table}


\textbf{Deep Gated Network (DGN):} The fixed NPF regime is obtained by using the framework of deep gated network (DGN), wherein, we hold the gates and hence the NPFs in a separate gating network parameterised by $\Tg\inrdnet$, and the NPVs in a different network called value network parameterised by $\Theta\inrdnet$ (see \Cref{fig:dgn}). Note that, the gating network uses $\chi^g$ as the activation function, which can be a standard ReLU activation, i.e., $\chi^g=\chi_r$. The pre-activations $q^g_{x,t}(l)$ of layer $l\in[d-1]$ from the gating network are used to derive the gating values $G_{x,t}(l)$ of layer $l\in[d-1]$. The gating values can be obtained from either a ReLU gate $\gamma_r$ or a soft-ReLU gate $\gamma_{sp}$. The process of separating the NPFs and the NPVs decouples the value and the feature gradients. In a DGN, the value gradient $\psi^v_{\cdot,\Theta}$ flows through the value network and the feature gradient $\psi^{\phi}_{\cdot,\Tg}$flows through the gating network. By letting the parameters of the gating network non-trainable, i.e., $\Tg_0=\Tg_t,\forall t\geq 0$ we can force $\psi^{\phi}_{\cdot,\Tg}$ to be $0$, and operate in the fixed NPF regime.
\begin{figure}[h] 
\begin{minipage}{0.70\columnwidth}
\begin{comment}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|l|l|}\hline
Layer& Gating Network (NPF)&Value Network (NPV)\\\hline 
Input & $z_{x,\Tg_t}(0)=x$ &$z_{x,\Theta_t}(0)=x$ \\\hline
Activation & $q_{x,\Tg_t}(l)={\Tg_t(l)}^\top z_{x,\Tg_t}(l-1)$& $q_{x,\Theta_t}(l)={\Theta_t(l)}^\top z_{x,\Theta_t}(l-1)$\\\hline
Hidden &$z_{x,\Tg_t}(l)=\chi^G\left(q_{x,\Tg_t}(l)\right)$& $z_{x,\Theta_t}(l)=q_{x,\Theta_t}(l)\odot G_{x,\Tg_t}(l)$ \\\hline
Output & $\hat{y}_{\Tg_t}(x)={\Tg_t(d)}^\top z_{x,\Tg_t}(d-1)$ &$\hat{y}_{\Tg_t,\Theta_t}(x)={\Theta_t(d)}^\top z_{x,\Theta_t}(d-1)$\\\hline
\multicolumn{3}{|l|}{Gating Values:\quad$ G_{x,\Tg_t}(l)= \gamma_{r}\left(q_{x,\Tg_t}(l)\right)\quad$or $G_{x,\Tg_t}(l)= \gamma_{sr}\left(q_{x,\Tg_t}(l)\right)$}\\\hline
\end{tabular}
}
\end{comment}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|l|l|}\hline
Layer& Gating Network (NPF)&Value Network (NPV)\\\hline 
Input & $z^g_{x,t}(0)=x$ &$z^v_{x,t}(0)=x$ \\\hline
Activation & $q^g_{x,t}(l)={\Tg_t(l)}^\top z^g_{x,t}(l-1)$& $q^v_{x,t}(l)={\Theta_t(l)}^\top z^v_{x,t}(l-1)$\\\hline
Hidden &$z^g_{x,t}(l)=\chi^g\left(q^g_{x,t}(l)\right)$& $z^v_{x,t}(l)=q^v_{x,t}(l)\odot G_{x,t}(l)$ \\\hline
Output & $\hat{y}^g_t(x)={\Tg_t(d)}^\top z^g_{x,t}(d-1)$ &$\hat{y}^v_t(x)={\Theta_t(d)}^\top z^v_{x,t}(d-1)$\\\hline
\multicolumn{3}{|l|}{Gating Values:\quad$ G_{x,t}(l)= \gamma_{r}\left(q^g_{x,t}(l)\right)\quad$or $G_{x,t}(l)= \gamma_{sr}\left(q^g_{x,t}(l)\right)$}\\\hline
\end{tabular}
}
\end{minipage}
\begin{minipage}{0.29\columnwidth}
\resizebox{\columnwidth}{!}{
\includegraphics[scale=0.5]{figs/nntwin-blck.png}
}
\end{minipage}
\caption{Deep gated network (DGN) setup.}
\label{fig:dgn}
\end{figure}

%\subsection{Main Result}
\begin{assumption}[Independent Initialisation]\label{assmp:main}
(i) $\Theta_0\inrdnet$ is statistically independent of NPFs, (ii) $\Theta_0$ are sampled i.i.d from a distribution such that for any $\theta_0\in\Theta_0$,  we have $\E{\theta_0}=0$, and  $\E{\theta^2_0}=\sigma^2$, and $\E{\theta^4_0}={\sigma'}^2$.
\end{assumption}
\textbf{Remark on\Cref{assmp:main} :} This is known as the \emph{independent initialisation} case, because, the NPVs $v_{\Theta_0}$ and NPFs $\phi_{x_s,\Tg_0},s\in[n]$ are statistically independent at initialisation. Note that, in a standard DNN with ReLU activations, at initialisation, NPFs and NPV are derived from the same $\Theta_0$, i.e., $\Tg_0=\Theta_0$; we call this case as the \emph{dependent initialisation} which does not satisfy \Cref{assmp:main}. In the case of \emph{dependent initialisation}, $K_{\Theta_0}\ra K^{(d)}$, where $K^{(d)}$ can be calculated by letting $\chi=\chi_{r}$ in \eqref{eq:ntkold}.%(note that in the fixed NPF regime, the feature gradient is $0$, and hence $\partial{\gamma}=0$).

\begin{theorem}[\textbf{Main Result}]\label{th:main} Let $H_{\Tg_0}=\Phi^\top_{\Tg_0}\Phi_{\Tg_0}$ be the neural path kernel (NPK) matrix , then under \Cref{assmp:main}, we have:\\
(i) $\E{K_{\Theta_0}}=d\sigma^{2(d-1)} H_{\Tg_0}.$

(ii) In addition, if ${4d}/{w^2}<1$, then $Var\left[K_0\right]\leq O\left(d^2_{in}\sigma^{4(d-1)}\max\{d^2w^{2(d-2)+1}, d^3w^{2(d-2)}\}\right)$.
\end{theorem}
\textbf{Remark on \Cref{th:main}:} This result says that in the fixed NPF regime, in the limit of infinite width, at random initialisation as per \Cref{assmp:main}, the NTK $K_{\Theta_0}$ is equal to the NPK $H_{\Tg_0}$ times a scaling constant. From pervious results, \cite{arora2019exact,cao2019generalization}, if follows that as $w\ra\infty$, the optimisation and generalisation properties of the fixed NPF learner can be tied down to $H_{\Tg_0}$. 
\begin{comment}
\Cref{th:main} can be applied in the following two interesting scenarios:\\
$1.$ Learning with random fixed NPFs. Here, we choose $\Tg_0$ according to \Cref{assmp:main}-(ii) and statistically independent of $\Theta_0$. The learning problem in this case is $\hat{y}_{\Theta_t}(x)=\ip{\phi_{x,\Tg_0},v_{\Theta_t}}$.\\
$2.$ Learning with fixed NPFs from pre-trained networks. Here, we copy the weights of a pre-trained network into $\Tg_0$, and then start with a random $\Theta_0$ as \Cref{assmp:main}. This is equivalent to retaining the NPFs and then resetting the NPVs to relearn them from scratch. \Cref{th:main} states that a network trained this way will generalise as well as its NPK.

Before we proceed to discuss the two scenarios, we will look at the structure of NPK matrix.
\end{comment}

\begin{comment}
We will now discuss about an interesting application of \Cref{th:main} to the problem of learning with fixed random NPFs.
\subsection{Optimisation with fixed random NPFs at random initialisation: Role of Depth}
Consider the case of independent initialisation, with $w\ra\infty$. Here, $K_{\Theta_0}\ra d\sigma^{(d-1)}H_{\Tg_0}=d(\Sigma)\odot(\sigma^{2(d-1)}\Lambda_{\Tg_0})$. Let us look at the diagonal and non-diagonal entries of $\sigma^{2(d-1)}\Lambda_0$, when $\Tg_0$ is initialised according to \Cref{assmp:main}-(ii) for $\sigma=\sqrt{\frac2w}$.\\
$\bullet$ \emph{Diagonal terms:} $\sqrt{\frac2w}\Lambda_{\Tg_0}(s,s)=2^{(d-1)}\Pi_{l=1}^{(d-1)}\frac{\tau_{\Tg_0}(s,s',l)}w$. Note that as $w\ra\infty$, $\frac{\tau_{\Tg_0}(s,s',l)}w\ra\frac12$, and hence $\sqrt{\frac2w}\Lambda_o(s,s)\ra 1$.\\
$\bullet$ \emph{Non-Diagonal terms:} As $w\ra\infty$, $\sqrt{\frac2w}\Lambda_{\Tg_0}(s,s')=2^{(d-1)}\Pi_{l=1}^{(d-1)}\frac{\tau_{\Tg_0}(s,s',l)}w\ra \Pi_{l=1}^{(d-1)}\mu_l(s,s')$, where $\mu_l(s,s')\in(0,1)$ is the fractional overlap between \emph{on} activation in layer $l$ for inputs $s,s'\in[n]$. Thus as depth increases the non-diagonal terms get suppressed by a factor $\Pi_{l}^{(d-1)}\mu_{l}(s,s')$.

When optimising with fixed NPFs, when the width is finite, increasing depth till a point helps in training due to the suppression of the non-diagonal terms of $H_{\Tg_0}$. Increasing depth beyond a point hurts, because, the variance term has a depth dependence, due to which, the entires of $K_{\Theta_0}$ deviate from their expected values.
\end{comment}
\begin{comment}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|}\hline
		&Gate&Activation&$\dot{\chi}$\\\hline
Prior&$G_R(q)=\mathbbm{1}_{\{q>0\}}$& $\begin{aligned}\chi_{R}(q)&=q\cdot\mathbbm{1}_{\{q>0\}}\\ \chi_{SP}(q)&=\frac{1}{\beta}\log(1+\exp(\beta q))\end{aligned}$&$\begin{aligned}\dot{\chi_{R}}(q)&: \text{Heaviside step function} \\ \dot{\chi}_{SP}(q)&=\frac{1}{1+\exp(-\beta q)}\end{aligned}$ \\\hline
Ours &$G_{SR}(q)=\frac{1}{1+\exp(-\beta q)}$ &$\chi_{SR}(q)=q\cdot G_{SR}(q)$& $\dot{\chi}_{SR}(q)=\dot{\chi}_{SP}(q)+\underbrace{\dot{G}_{SR}(q)}_{\text{Gate Dynamics}}$\\\hline
\end{tabular}
}
\caption{A comparison of prior work and our paper. Here subscripts $R$, $SR$ and $SP$ stand for ReLU, soft-ReLU and softplus respectively.}
\label{tb:compare}
\end{table}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|}\hline
		&Output&NTF&$NTK$ &$\dot{\Sigma}^{(l+1)}(s,s')$\\\hline
Prior&\shortstack{$\hat{y}_{\Theta}(x)$\\layer-by-layer} &$\psi_{x,\Theta}=\left(\partial_{\theta}\hat{y}_{\Theta}(x),\theta\in\Theta\right)$& $K_{\Theta}(x,x')=\ip{\psi_{x,\Theta},\psi_{x',\Theta}}$ & $\dot{\chi}_{SP}(q)\dot{\chi}_{SP}(q')$ \\\hline
Ours& $\hat{y}_{\Theta}(x)=\ip{\phi_{x,\Theta},v_{\Theta}}$ & $\begin{aligned}\psi_{x,\Theta}&=\psi^v_{x,\Theta}+\psi^{\phi}_{x,\Theta}\\\psi^v_{x,\Theta}&=\left(\ip{\phi_{x,\Theta}, \partial_{\theta} v_{\Theta}},\theta\in\Theta\right) \\ \psi^{\phi}_{x,\Theta}&=\left(\ip{\partial_{\theta}\phi_{x,\Theta}, v_{\Theta}},\theta\in\Theta\right)\end{aligned}$ &$\begin{aligned} K_{\Theta}(x,x')=\langle \psi^v_{x,\Theta}+\psi^{\phi}_{x,\Theta},\\ \psi^v_{x',\Theta}+\psi^{\phi}_{x',\Theta}\rangle\end{aligned}$ & $\begin{aligned}\left(\dot{\chi}_{SP}(q)+\dot{G}_{SR}(q)\right)\cdot\\\left(\dot{\chi}_{SP}(q')+\dot{G}_{SR}(q')\right)\end{aligned}$ \\\hline

\end{tabular}
}
\caption{A comparison of prior work and our paper. Here subscripts $R$, $SR$ and $SP$ stand for ReLU, soft-ReLU and softplus respectively.}
\label{tb:compare}
\end{table}
\end{comment}
%$\begin{aligned}&\text{Learning}\\ &\text{Regime} \end{aligned}$ 