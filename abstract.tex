\begin{abstract}
We study optimisation and generalisation in deep neural networks (DNNs) with ReLU activations. Central idea in this paper is to regard paths and gates as basic building blocks of a DNN. At time $t$, for an input $x\in\R^{d_{in}}$ to the DNN, we define a novel \emph{neural path feature} $\phi_{x,t}$ whose dimension is equal to total the number of paths, and whose co-ordinate corresponding to a path is the product of the signal at its input node  and the activity\footnote{A path is active if all the gates in the path are active.} of the path. We show that the associated \emph{neural path kernel} $H_t(x,x')=\phi^\top_{x,t}\phi_{x',t}$, plays an important role in optimisation and generalisation. In particular, $H_t(x,x')=(x^\top x')\lambda_t(x,x')$, where $\lambda_t(x,x')$ is a count of total number of paths that are active simultaneously for both inputs $x,x'\in \R^{d_{in}}$. \hfill\\
When the activations and as a consequence the neural path features are frozen (i.e. do not change over time), and under symmetric Bernoulli initialisation, we show (in theory and experiments) that i) increasing depth till a point helps in training and ii) increasing depth beyond hurts training. We verify via experiments that the NPFs and NPK are learnt during training by showing that the norm of the labelling function measured with respect to the inverse of the trace normalised NPK reduces with time.
\end{abstract}
