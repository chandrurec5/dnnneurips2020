\section{Introduction}
Understanding optimisation and generalisation of deep neural networks (DNNs) trained using first-order method such as (stochastic) gradient descent is an important problem in machine learning. In this paper, we consider fully connected networks of depth $d$, and width $w$, and we throw light on the following two questions:
\begin{comment}
Recent works based on the \emph{trajectory} analysis \cite{} have shown that, sufficiently over-parameterised DNNs can be optimised to achieve zero training error by a randomly initialised SGD procedure. The gist of the \emph{trajectory} based analysis is the following: consider the dataset given by $(x_s,y_s)_{i=1}^n\in \R^{d_{in}\times} \R$, and for an input $x_s\in \R^{d_{in}},i\in[n]$\footnote{We denote the set $\{1,\ldots, n\}$ by $[n]$.}, let $\hat{y}_{\Theta_t}(x_s)$ be predicted output of the DNN whose parameters/tunable weights at time $t$ is $\Theta_t\in \R^{d_{net}}$. Say one is interested in minimising the squared loss given by $L_{\Theta_t}=\frac{1}{2}\sum_{i=1}^n\left(\hat{y}_{\Theta_t}(x_s)-y_s\right)^2$ by a SGD procedure, then the idea behind the trajectory analysis is to look at the dynamics of the error term defined as $e_t(i)\stackrel{def}{=}\hat{y}_{\Theta_t}(x_s) -y_s$. Denoting $e_t=(e_t(i),i\in[n])\in \R^n$, one can study the following error recursion:
\begin{align}\label{eq:trajecbasic}
e_{t+1}=e_t-\alpha_t K_t e_t,
\end{align}
where $\alpha_t>0$ is a small stepsize, $K_t\in \R^{n\times n}$ is a Gram matrix. This gram matrix is in turn expressed as $K_t=\Psi_t^\top \Psi_t$, wherein, $\Psi_t$ is a $d_{net}\times n$ matrix known as the \emph{neural tangent feature} (NTF) matrix, which is the collection of the gradient of the network output with respect to the network parameters, and whose $s^{th}$ column is given by $\Psi_t(s)=(\frac{\partial \hat{y}_{\Theta_t}(x_s)}{\partial \theta},\theta \in \Theta)$.
\end{comment}

\textbf{Question I (Optimisation):} \emph{What is the role of depth in training of DNNs? Why increasing depth till a point helps in training? Why increasing the depth beyond hurts training?}\\
\begin{comment}We call the above  questions are the depth phenomena. \cite{dudnn} show that, when it comes to training, residual networks are better than simple FC-DNNs. However, the depth phenomena in the case of simple FC-DNNs is still unresolved.
\end{comment}
\textbf{Question II (Generalisation):} \emph{What are the hidden features in a DNN? Are these features learned? and if so, how?} \hfill\\
\begin{comment}The general consensus is that, the DNNs learn hidden representations progressively in each of the intermediate layers, and the final layer learns a linear model using features obtained in the penultimate layer.  This view, while conceptually simple, however, does not provide us any analytical insight regarding the above question.  A more analytically appealing candidate for the hidden representation (used in some of the recent works \cite{}) is the \emph{neural tangent random feature} (NTRF) which is the NTF evaluated at randomised initialisation of an infinitely wide DNN. \cite{} provides generalisation bounds in terms of the Rademacher complexity of the class of functions defined by the NTRF and also in terms of an associated neural tangent kernel (NTK). \cite{} uses NTK to set a significant new benchmark for pure-kernel based learning.  An issue with the NTRF/NTK approach is that the features do not change over the training of the DNN, thus implying no feature learning is happening, and yet experimental evidence (in \cite{} as well as \Cref{sec:generalisation-exp}) shows that DNNs perform significantly better than pure-kernel learning with NTK.
\end{comment}
The results in this paper fit within the the framework of the trajectory method, used in recent works \cite{dudnn} to show that SGD achieves zero training error in over-parameterised DNNs. In particular, the error trajectory can be given by $e_{t+1}=e_t-\alpha_tK_te_t$, where $e_t\in \R^n$ is the error in the DNN prediction (i.e., difference between predicted and true values), $\alpha_t$ is a small stepsize (of the SGD) and $K_t\in\R^{n\times n}$ is a Gram matrix. We now present the highlights of our contributions: \hfill\\
$\bullet$ \textbf{Paths:}  A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Let $[P]=\{1,\ldots,P\}$ be an enumeration of all the paths, the zeroth and first order quantities of a DNN can then be expressed as:
\begin{align*}
%\begin{split}
\text{(Zeroth Order)}\quad: \quad \hat{y}_t(x)&=\sum_{p\in [P]}x(\I(p))A_t(x,p)v_t(p)={\phi^\top_{x,t}} v_t\\
\text{(First Order)}\quad: \quad \partial \hat{y}_t(x)&= \sum_{p\in [P]}x(\I(p)) A_t(x,p) \partial((v_t(p))+\sum_{p\in [P]}x(\I(p)) \partial(A_t(x,p)) v_t(p)\\
\psi_{x,t}&=\phi^\top_{x,t} {\partial} v_t + {\partial} \phi^\top_{x,t} v_t,
%\end{split}
\end{align*}
where $\I(p)$ is the input node at which a path $p$ starts, $v_t(p)$ is its value given by the product of its weights and $A_t(x,p)$ is its activation level given by the product of the gating values in the path. \hfill\\
$\bullet$ \textbf{Gates and Activations:} In order to understand the role $A_t(\cdot,\cdot)$ in optimisation and generalisation, we handle the gating values as independent variables\footnote{In DNN with ReLU, the gates are implicit: a gate is \emph{on} only if pre-activation input is positive.}. This  enables us to compare the performances of DNNs with frozen gates (in which $A_t(\cdot,\cdot)=A_0(\cdot,\cdot),\forall t\geq 0$) and DNNs with adaptable gates (in which $A_t(\cdot,\cdot)$ changes with time). \hfill\\
$\bullet$ \textbf{Features and Kernels:}  We define a novel (zeroth-order) \emph{neural path feature} (NPF) $\phi_{x,t}\stackrel{def}=\left( x(\I(p))A_t(x,p),p\in[P]\right)\in \R^P$ , and an associated (zeroth-order) \emph{neural path kernel} (NPK) to $H_t(x,x')=\phi^\top_{x,t}\phi_{x',t}$. We show that the NPK has a special structure, in that, $H_t(x,x')=(x^\top x')\lambda_t(x,x')$, where $\lambda_t(x,x')$ is a measure of similarity based on the path activation levels for inputs $x,x'\in\R^{d_{in}}$. For network with frozen gates, we derive an explicit expression for  the (first-order) \emph{neural tangent kernel} (NTK) given by $K_t(x,x')=\psi^\top_{x,t}\psi^\top_{x',t}$, and with the further assumption that the
weights are initialised statistically independent of the gates (with symmetric Bernoulli distribution), we show that $\E{K_0}=C H_0$, for some constant $C>0$. \hfill\\
$\bullet$ \textbf{Optimisation:} Since $K_t$ dictates the convergence properties of the error recursion in the trajectory analysis, and from the fact that $\E{K_0(x,x')}=C H_0=C (x^\top x')\lambda_0(x,x')$, we have the following: \hfill\\
$1.$ We argue that the non-diagonal entries of $\lambda_0$ decay at an exponential rate (in comparison with the diagonal entries), i.e., the Gram matrix whitens with depth. Thus increasing depth helps in training. \hfill\\
$2.$ We show that $Var\left[K_0(x,x')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$ (for $\sigma^2=O(\frac{1}w)$). For a fixed $d$, increasing $w$ ensures $K_0$ converges to $\E{K_0}$. However, for a fixed $w$, increasing $d$ makes the entries of $K_0$ deviate from $\E{K_0}$, thus degrading the spectrum of $K_0$. Thus increasing depth beyond a point hurts training performance. \hfill\\
\begin{comment}
$\bullet$ \textbf{Feature Learning:} To the best of our knowledge, for the first time in the literature, we concretely identify that the gradient flow in a DNN has two components. The $\phi^\top_{x_s,t} {\partial} v_t $ term, which we call the \emph{value gradient}, learns the path values (which are akin to weight vector in a standard linear approximation) keeping the NPFs fixed. The $ {\partial} \phi^\top_{x_s,t} v_t$ term, which we call the \emph{feature gradient} learns the NPFs keeping the path values fixed. We further note that, in $\phi_{x,t}/H_t$, only $A_t/\lambda_t$ changes during training. 
\end{comment}
$\bullet$ \textbf{Feature Learning:} To the best of our knowledge, for the first time in the literature, we concretely identify that the gradient flow in a DNN has two components. The $\phi^\top_{x_s,t} {\partial} v_t $ term, which we call the \emph{value gradient}, learns the path values (which are akin to weight vector in a standard linear approximation) keeping the NPFs fixed. The $ {\partial} \phi^\top_{x_s,t} v_t$ term, which we call the \emph{feature gradient} learns the NPFs keeping the path values fixed. We further note that, in $\phi_{x,t}/H_t$, only $A_t/\lambda_t$ changes during training.  \hfill\\
$\bullet$ \textbf{Generalisation:} We show via experiments on standard datasets such as MNIST and CIFAR that better generalisation happens when the activations $A_t$ change with time. In particular, for MNIST restricted to two labels, we experimentally verify that the quantity $y^\top (\widehat{H_t})^{-1}y$\footnote{For a matrix $H$, $\hat{H}=\frac{1}{trace(H)}H$ .} reduces as the training progresses. This shows that the eigenspaces of the NPK matrix align themselves in accordance to the labelling function. These experiments supports the fact that NPF and the NPK are indeed learnt during training in a manner to improve the generalisation performance. \hfill\\
\textbf{Comparison with related Work:} \hfill\\
$1.$ \cite{dudnn} show that in fully connected DNNs with $w=\Omega(poly(n)2^{O(d)})$, and in residual neural networks (ResNets) with $w=\Omega(poly(n,d))$ gradient descent converges to zero training loss. Their result is restricted to only showing why ResNets are better than fully connected DNNs, based on the fact that the dependence on the number of layers improves exponentially for ResNets. Thus, Question I is unresolved, and we resolve the same in this paper. \hfill\\
$2.$ \cite{ntk} where the first to point out the role of NTK in DNNs. They showed that, in DNNs with randomised initialisation, the evolution of DNN during training can be described by the NTK.  Prior works \cite{arora2019exact,arora,cao2019generalization} suggest that, for propose  generalisation bounds as well as pure-kernel based using the NTK \cite{arora2019exact}. However, couple of issues remain unresolved: firstly, if the DNNs are only linear learners with random NTFs, then it suggests that no feature learning happens in DNNs, and secondly, it was observed in prior experiments that the DNNs perform better than their corresponding NTK counterparts \cite{arora2019exact,lee2017deep}. In this paper, we first identify the NPF and NPK, which are, zeroth-order feature and kernel respectively, and establish the connecting between the zeroth-order NPK and the first-order NTK. We also show that NPK is learnt during training, which resolved Question II, as well as the aforementioned issues with prior work based on NTFs and NTKs. \hfill\\
$3.$ In comparison to \cite{sss} who were the first to initiate the study on GaLU networks, we believe, our work has made significant progress. We show via experiments, that, gate adaptation is key in learning, thereby showing a clear separation between GaLU and ReLU networks. 
