\begin{comment}
$1$ A fully-trained wide neural net enjoys the same generalization ability as its corresponding NTK.
$2.$ A fully-trained sufficiently wide ReLU neural network is equivalent to the kernel regression predictor.
$3.$ In the limit, it can be shown that the matrix H(t) remains constant during training i.e., equal to H(0). Moreover, under a random initialization of parameters, the random matrix H (0) converges in probability to a certain deterministic kernel matrix H∗ as the width goes to infinity, which is the Neural Tangent Kernel ker(·,·).
$4.$ 
\end{comment}
\section{Our Contribution}
\textbf{Gate Dynamics and Feature Learning(Resolving Gap I ):}
A major step towards understanding feature learning in DNNs is to tie the \emph{neural path features} to the gates and as a result feature learning to the dynamics of the gates. Three key ingredients that separate this paper from the previous works are i) explicitly allocating separate variable for gating in our analysis, ii) the `path-view', and iii) the `soft-ReLU' gate. We now describe the significance of these three ingredients to the problem of understanding feature learning via gate dynamics.\\
$1.$ The expression for the output $\hat{y}_{\Theta}(x)=\ip{\phi_{x,\Theta},v_{\Theta}}$ reveals that there are two separate learning problems, one that of learning the NPFs and the other that of learning the NPVs. This viewpoint of two learning problems does not naturally emerge in the standard recursive `layer-by-layer' expression for the output of DNNs.\\
$2.$ The gradient (i.e., the \emph{neural tangent feature}) $\psi_{x,\Theta} $naturally splits into two namely the value gradient $\psi^v_{x,\Theta}$ and the feature gradient $\psi^{\phi}_{x,\Theta}$ (see row $5$ of \Cref{tb:compare}) which leads to the following  decomposition (obtained by expanding the expression in  row $6$ of \Cref{tb:compare}) of the NTK matrix:
\begin{align}
K_{\Theta}=K^v_{\Theta}+K^{\phi}_{\Theta}+K^{cross}_{\Theta},
\end{align}
where $K^v_{\Theta}=(\Psi^v_{\Theta})^\top \Psi^v_{\Theta}$ is the Gram matrix of the value gradient, $K^{\phi}_{\Theta}=(\Psi^{\phi}_{\Theta})^\top \Psi^{\phi}_{\Theta}$ is the Gram matrix due to the feature gradient, and $K^{cross}_{\Theta}=(\Psi^v_\Theta)^\top \Psi^{\phi}_{\Theta} +(\Psi^{\phi}_\Theta)^\top \Psi^{v}_{\Theta}$, is a symmetric matrix which is the cross-term obtained as an interaction of the value and the feature gradients.\\
$3.$ The kernel matrices $K^v_{\Theta}$ and $K^{\phi}_{\Theta}$ have different functions, with $K^v_{\Theta}$ captures the dynamics of learning the NPVs keeping for a given NPF and $K^{\phi}_{\Theta}$ capture the dynamics of feature learning.\\
$4.$ The soft-ReLU activation plays a key part: note that, in prior works, in the expression for the limit NTK $K^{(d)}$ in \eqref{eq:ntkold}, $\dot{\chi}$ is either a Heaviside step function or a logit function. Both these functions capture the \emph{on/off} behaviour of the gates, and hence the $K^{(d)}$ captures only the flow of value gradient through the active sub-networks. On the contrary, in the case of soft-ReLU (see row $3$ of \Cref{tb:compare}), $\dot{\chi}_{SR}$ contains both the \emph{on/off} information via the $\dot{\chi}_{SP}$ term as well as the information on the dynamics of sensitive gates via the $\dot{G}_{SR}$ term.
\FloatBarrier
\begin{figure*}[h]\centering
%\begin{minipage}{0.78\columnwidth}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccc}
\includegraphics[scale=0.4]{figs/act.png}
\includegraphics[scale=0.4]{figs/der-act.png}
\includegraphics[scale=0.4]{figs/gate.png}
\includegraphics[scale=0.4]{figs/der-gate.png}
%\includegraphics[scale=0.5]{figs/nntwin-blck.png}
%\includegraphics[scale=0.5]{figs/nn.png}
%\includegraphics[scale=0.5]{figs/nn.png}
\end{tabular}
}
%\end{minipage}
%\begin{minipage}{0.18\columnwidth}
%\resizebox{\columnwidth}{!}{
%\includegraphics[scale=0.5]{figs/nnconv.png}
%}
%\end{minipage}
\caption{Activations and Gates}
\label{fig:actgate}
\end{figure*}
\begin{comment}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|l|l|}\hline
Quantity		&Prior Work																							&Our Work							\\\hline
Gates		&$G_R(q)=\mathbbm{1}_{\{q>0\}}$																			&$G_{SR}(q)=\frac{1}{1+\exp(-\beta q)}$		\\\hline						
Activation		&$\begin{aligned}\chi_{R}(q)&=q\cdot\mathbbm{1}_{\{q>0\}}\\ \chi_{SP}(q)&=\frac{1}{\beta}\log(1+\exp(\beta q))\end{aligned}$			&$\chi_{SR}(q)=q\cdot G_{SR}(q)$					\\\hline				
%$\dot{\chi}$	&$\begin{aligned}\dot{\chi_{R}}(q)&: \text{Heaviside step function} \\ \dot{\chi}_{SP}(q)&=\frac{1}{1+\exp(-\beta q)}\end{aligned}$		&$\dot{\chi}_{SR}(q)=\dot{\chi}_{SP}(q)+\underbrace{\frac{\beta q}{(1+\exp(\beta q))(1+\exp(-\beta q))}}_{\text{Gate Dynamics}}$ \\\hline
$\dot{\chi}$	&$\begin{aligned}\dot{\chi_{R}}(q)&: \text{Heaviside step function} \\ \dot{\chi}_{SP}(q)&=\frac{1}{1+\exp(-\beta q)}\end{aligned}$		&$\dot{\chi}_{SR}(q)=\dot{\chi}_{SP}(q)+\underbrace{\dot{G}_{SR}(q)}_{\text{Gate Dynamics}}$ \\\hline
Output		&$\hat{y}_{\Theta}(x)$: recursive, nested, layer-by-layer 																						&$\hat{y}_{\Theta}(x)=\ip{\phi_{x,\Theta},v_{\Theta}}$\\\hline
%NPF, NPK		&Not Applicable																										&\shortstack[l]{NPF: $\phi_{x,\Theta}$ (see \eqref{eq:npfdef})\\ NPK: $H_{\Theta}(x,x')=\ip{\phi_{x,\Theta},\phi_{x',\Theta}}$}\\\hline
NTF			&$\psi_{x,\Theta}=\left(\partial_{\theta}\hat{y}_{\Theta}(x),\theta\in\Theta\right)\in\R^{d_{net}}$								
&$\begin{aligned}\psi_{x,\Theta}&=\psi^v_{x,\Theta}+\psi^{\phi}_{x,\Theta}\\\psi^v_{x,\Theta}&=\left(\ip{\phi_{x,\Theta}, \partial_{\theta} v_{\Theta}},\theta\in\Theta\right)\in\R^{d_{net}} \\ \psi^{\phi}_{x,\Theta}&=\left(\ip{\partial_{\theta}\phi_{x,\Theta}, v_{\Theta}},\theta\in\Theta\right)\in\R^{d_{net}}\end{aligned}$ \\\hline	
NTK			&$K_{\Theta}(x,x')=\ip{\psi_{x,\Theta},\psi_{x',\Theta}}$														&$K_{\Theta}(x,x')=\ip{\psi^v_{x,\Theta}+\psi^{\phi}_{x,\Theta},\psi^v_{x',\Theta}+\psi^{\phi}_{x',\Theta}}$	\\\hline		
$\dot{\Sigma}^{(l+1)}(s,s')$	&$\dot{\chi}_{SP}(q)\dot{\chi}_{SP}(q')$ &$\begin{aligned}\dot{\chi}_{SP}(q)\dot{\chi}_{SP}(q')&+qq'\dot{G}_{SR}(q)\dot{G}_{SR}(q')\\ +q\dot{G}_{SR}(q)\dot{\chi}_{SP}(q')&+q'\dot{G}_{SR}(q')\dot{\chi}_{SP}(q)\end{aligned}$\\\hline
%$\begin{aligned}\text{Learning}\\\text{Regime}\end{aligned}$			&$\begin{aligned}\text{Fixed NTF:}\quad\quad\quad\quad\quad \\ \hat{y}_{\Theta_0+\Delta \Theta}(x)=\hat{y}_{\Theta_0}(x)+\ip{\psi_{x,\Theta_0}, \Delta\Theta}\end{aligned}$								&$\hat{y}_{\Theta_0+\Delta \Theta}(x)=\hat{y}_{\Theta_0}(x)+\ip{\psi^v_{x,\Theta_0}, \Delta\Theta}$\\\hline
%\shortstack[l]{Idealised\\Regime\\ \quad}			&\shortstack[l]{{Fixed NTF/NTK (for large $w$):} \\ $\hat{y}_{\Theta_0+\Delta \Theta}(x)\approx\hat{y}_{\Theta_0}(x)+\ip{\psi_{x,\Theta_0}, \Delta\Theta}$\\ \quad\\ \quad \\ \quad\\ \quad}								&\shortstack[l]{\quad\\{Fixed NPF/NPK (for large $w$):}\\ $\hat{y}_{\Theta_0+\Delta \Theta}(x)\approx\hat{y}_{\Theta_0}(x)+\ip{\psi^v_{x,\Theta_0}, \Delta\Theta}$ \\ \Cref{sec:optimisation}-\Cref{th:main}}\\\hline


\end{tabular}
}
\caption{A comparison of prior work and our paper. Here subscripts $R$, $SR$ and $SP$ stand for ReLU, soft-ReLU and softplus respectively.}
\label{tb:compare}
\end{table}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|}\hline
		&Gate&Activation&$\dot{\chi}$\\\hline
Prior&$G_R(q)=\mathbbm{1}_{\{q>0\}}$& $\begin{aligned}\chi_{R}(q)&=q\cdot\mathbbm{1}_{\{q>0\}}\\ \chi_{SP}(q)&=\frac{1}{\beta}\log(1+\exp(\beta q))\end{aligned}$&$\begin{aligned}\dot{\chi_{R}}(q)&: \text{Heaviside step function} \\ \dot{\chi}_{SP}(q)&=\frac{1}{1+\exp(-\beta q)}\end{aligned}$ \\\hline
Ours &$G_{SR}(q)=\frac{1}{1+\exp(-\beta q)}$ &$\chi_{SR}(q)=q\cdot G_{SR}(q)$& $\dot{\chi}_{SR}(q)=\dot{\chi}_{SP}(q)+\underbrace{\dot{G}_{SR}(q)}_{\text{Gate Dynamics}}$\\\hline
\end{tabular}
}
\caption{A comparison of prior work and our paper. Here subscripts $R$, $SR$ and $SP$ stand for ReLU, soft-ReLU and softplus respectively.}
\label{tb:compare}
\end{table}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|}\hline
		&Output&NTF&$NTK$ &$\dot{\Sigma}^{(l+1)}(s,s')$\\\hline
Prior&\shortstack{$\hat{y}_{\Theta}(x)$\\layer-by-layer} &$\psi_{x,\Theta}=\left(\partial_{\theta}\hat{y}_{\Theta}(x),\theta\in\Theta\right)$& $K_{\Theta}(x,x')=\ip{\psi_{x,\Theta},\psi_{x',\Theta}}$ & $\dot{\chi}_{SP}(q)\dot{\chi}_{SP}(q')$ \\\hline
Ours& $\hat{y}_{\Theta}(x)=\ip{\phi_{x,\Theta},v_{\Theta}}$ & $\begin{aligned}\psi_{x,\Theta}&=\psi^v_{x,\Theta}+\psi^{\phi}_{x,\Theta}\\\psi^v_{x,\Theta}&=\left(\ip{\phi_{x,\Theta}, \partial_{\theta} v_{\Theta}},\theta\in\Theta\right) \\ \psi^{\phi}_{x,\Theta}&=\left(\ip{\partial_{\theta}\phi_{x,\Theta}, v_{\Theta}},\theta\in\Theta\right)\end{aligned}$ &$\begin{aligned} K_{\Theta}(x,x')=\langle \psi^v_{x,\Theta}+\psi^{\phi}_{x,\Theta},\\ \psi^v_{x',\Theta}+\psi^{\phi}_{x',\Theta}\rangle\end{aligned}$ & $\begin{aligned}\left(\dot{\chi}_{SP}(q)+\dot{G}_{SR}(q)\right)\cdot\\\left(\dot{\chi}_{SP}(q')+\dot{G}_{SR}(q')\right)\end{aligned}$ \\\hline

\end{tabular}
}
\caption{A comparison of prior work and our paper. Here subscripts $R$, $SR$ and $SP$ stand for ReLU, soft-ReLU and softplus respectively.}
\label{tb:compare}
\end{table}
\end{comment}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|}\hline
		&Gate&Activation&$\dot{\chi}$\\\hline
Prior&$G_R(q)=\mathbbm{1}_{\{q>0\}}$& $\begin{aligned}\chi_{R}(q)&=q\cdot\mathbbm{1}_{\{q>0\}}\\ \chi_{SP}(q)&=\frac{1}{\beta}\log(1+\exp(\beta q))\end{aligned}$&$\begin{aligned}\dot{\chi_{R}}(q)&: \text{Heaviside step function} \\ \dot{\chi}_{SP}(q)&=\frac{1}{1+\exp(-\beta q)}\end{aligned}$ \\\hline
Ours &$G_{SR}(q)=\frac{1}{1+\exp(-\beta q)}$ &$\chi_{SR}(q)=q\cdot G_{SR}(q)$& $\dot{\chi}_{SR}(q)=\dot{\chi}_{SP}(q)+\underbrace{\dot{G}_{SR}(q)}_{\text{Gate Dynamics}}$\\\hline\hline
		&NTF&$NTK$ &$\dot{\Sigma}^{(l+1)}(s,s')$\\\hline
Prior&$\psi_{x,\Theta}=\left(\partial_{\theta}\hat{y}_{\Theta}(x),\theta\in\Theta\right)$& $K_{\Theta}(x,x')=\ip{\psi_{x,\Theta},\psi_{x',\Theta}}$ & $\dot{\chi}_{SP}(q)\dot{\chi}_{SP}(q')$ \\\hline
Ours & $\begin{aligned}\psi_{x,\Theta}&=\psi^v_{x,\Theta}+\psi^{\phi}_{x,\Theta}\\\psi^v_{x,\Theta}&=\left(\ip{\phi_{x,\Theta}, \partial_{\theta} v_{\Theta}},\theta\in\Theta\right) \\ \psi^{\phi}_{x,\Theta}&=\left(\ip{\partial_{\theta}\phi_{x,\Theta}, v_{\Theta}},\theta\in\Theta\right)\end{aligned}$ &$\begin{aligned} K_{\Theta}(x,x')=\langle \psi^v_{x,\Theta}+\psi^{\phi}_{x,\Theta},\\ \psi^v_{x',\Theta}+\psi^{\phi}_{x',\Theta}\rangle\end{aligned}$ & $\begin{aligned}\left(\dot{\chi}_{SP}(q)+\dot{G}_{SR}(q)\right)\cdot\\\left(\dot{\chi}_{SP}(q')+\dot{G}_{SR}(q')\right)\end{aligned}$ \\\hline
\end{tabular}
}
\caption{A comparison of prior work and our paper. Here subscripts $R$, $SR$ and $SP$ stand for ReLU, soft-ReLU and softplus respectively.}
\label{tb:compare}
\end{table}
\begin{figure*}[t]
%\begin{minipage}{0.78\columnwidth}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccc}
\includegraphics[scale=0.5]{figs/gradflow.png}
%\includegraphics[scale=0.5]{figs/gradflow_feat.png}
%\includegraphics[scale=0.5]{figs/gradflow.png}
%\includegraphics[scale=0.5]{figs/nntwin-blck.png}
%\includegraphics[scale=0.5]{figs/nn.png}
%\includegraphics[scale=0.5]{figs/nn.png}
\end{tabular}
}
%\end{minipage}
%\begin{minipage}{0.18\columnwidth}
%\resizebox{\columnwidth}{!}{
%\includegraphics[scale=0.5]{figs/nnconv.png}
%}
%\end{minipage}
\caption{Gradient Flow in Prior work vs Our work}
\label{fig:gradflow}
\end{figure*}

$5.$ Plugging the soft-ReLU instead of softplus/ReLU in \eqref{eq:ntkold} will indeed bring in the additional terms related to gate dynamics (see last row of \Cref{tb:compare}) in $K^{(d)}$. However, note that $K^{(d)}$ is a single matrix, and only due to the `path-view', we can identify its components namely $K^v_{\Theta}$, and $K^{\phi}_{\Theta}$ which have completely different functions.
\FloatBarrier
\begin{table}[h]\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{| l | l | l |}\hline
Standard (finite $w$) &Fixed NTK ($w\ra\infty$) & Fixed NPK (finite $w$)\\\hline
$\dot{\Theta}_t=-\sum_{s=1}^n \psi_{x,t}e_t(s)$ &$\dot{\Theta}_t\ra 0$ &$\dot{\Theta}_t=-\sum_{s=1}^n \psi^v_{x,t}e_t(s)$ \\\hline
$\dot{\phi}_{x_s,t}(p)=x(\I_0(p))\sum_{\theta\in\Theta}\partial_{\theta}A_t(x_s,p)\dot{\theta}_t$ &$\dot{\phi}_{x_s,t}(p)\ra 0$ &$\dot{\phi}_{x_s,t}(p)=0$\\\hline
$\dot{v}_t(p)=\sum_{\theta\in\Theta}\partial_{\theta}v_t(p)\dot{\theta}_t$ &$\dot{v}_t(p)\ra 0$ &$\dot{v}_t(p)=\sum_{\theta\in\Theta}\partial_{\theta}v_t(p)\dot{\theta}_t$\\\hline
$\dot{e}_t=-(K^v_t+K^{\phi}_t+K^{cross}_{t})e_t$ &$\dot{e}_t=-K^{(d)}e_t$ &$\dot{e}_t=-K^v_te_t$\\\hline
\end{tabular}
}
\caption{Dynamics in various regimes. Here $p\in[P], s\in[n]$.}
\label{tb:dynamics}
\end{table}
\begin{comment}
\FloatBarrier
\begin{table}[h]\centering
\begin{tabular}{|l| lll |}\hline
\text{Parameter}&$\dot{\Theta}_t$&$=$&$-\sum_{s=1}^n \psi_{x,t}e_t(s)$\\\hline
\text{NPF}		&$\dot{\phi}_{x_s,t}(p)$&$=$&$x(\I_0(p))\sum_{\theta\in\Theta}\partial_{\theta}A_t(x_s,p)\dot{\theta}_t,\forall p\in[P], s\in[n]$\\\hline
\text{NPV}		&$\dot{v}_t(p)$&$=$&$\sum_{\theta\in\Theta}\partial_{\theta}v_t(p)\dot{\theta}_t,\forall p\in[P]$\\\hline
\text{Error}	 &$\dot{e}_t$&$=$&$-(K^v_t+K^{\phi}_t+(\Psi^v_{t})^\top \Psi^{\phi}_{t}+(\Psi^v_{t})^\top\Psi^{\phi}_{t})e_t$\\\hline
\end{tabular}
\caption{Dynamics in various regimes. Here $p\in[P], s\in[n]$.}
\label{tb:dynamics}
\end{table}
\end{comment}
\begin{wrapfigure}{h}{0.3\textwidth}
\includegraphics[scale=0.25]{figs/regime.png}
\caption{\label{fig:regime}Three Regimes.}
\end{wrapfigure}
\textbf{A New semi-idealised regime with fixed NPF(Resolving Gap II):} \Cref{tb:dynamics} shows the dynamics of GD for infinitesimal step-size (expressed in the form of coupled ordinary differential equations) in three regimes namely i) \emph{standard regime}, in  which practical DNNs operate, ii) \emph{fixed NTK regime}, an idealised regime which has been studied in recent works and iii) \emph{fixed NPF regime}, a new, semi-ideal regime which we introduce and study in this paper. In the standard regime, all the underlying quantities namely the weights, the NPFs, the NPVs and the error changes with time. In the fixed NTK regime, owing to the infinite width, while the weights and activations stay close to initialisation, and the cumulative effect drives the output to the required values, thereby reducing the training error. The fixed NPF regime, holds for any finite $w$, and here, we forcefully \emph{turn-off} feature learning, i.e. let the feature gradient $\psi^{\phi}_{x,\Theta}=0$. In order to do this, we hold the gates and hence the NPFs in a separate gating network and the NPVs in a different network called value network. By letting the parameters of the gating network non-trainable, we can force $\psi^{\phi}_{x,\Theta}$ to be $0$. Note that, in the limit of infinite width, the fixed NPF regime transitions to the fixed NTK regime. 
\begin{comment}
\begin{tabular}{ll}
\text{Parameter Update}&$\begin{aligned}\dot{\Theta}_t=-\sum_{s=1}^n \psi_{x,\Theta_t}e_t(s)\end{aligned}$\\
\text{NPF Update}		&$\begin{aligned}\dot{\phi}_{x_s,t}(p)=x(\I_0(p))\sum_{\theta\in\Theta}\partial_{\theta}A_t(x_s,p),\forall p\in[P], s\in[n]\end{aligned}$\\
\text{NPV Update}		&$\begin{aligned}\dot{v}_t(p)=\sum_{\theta\in\Theta}\partial_{\theta}v_t(p),\forall p\in[P]\end{aligned}$\\
\text{Error Trajectory}	 &$\begin{aligned}\dot{e}_t=-(K^v_t+K^{\phi}_t+(\Psi^v_{t})^\top \Psi^{\phi}_{t}+(\Psi^v_{t})^\top\Psi^{\phi}_{t})e_t\end{aligned}$
\end{tabular}
\end{comment}
\begin{comment}
\begin{align}
&\text{Parameter Update:}&\dot{\Theta}_t&=&\quad-\sum_{s=1}^n \psi_{x,\Theta_t}e_t(s)\\
&\text{NPF Update:}		&\dot{\phi}_{x_s,t}(p)&=&\quad x(\I_0(p))\sum_{\theta\in\Theta}\partial_{\theta}A_t(x_s,p),\forall p\in[P], s\in[n]\\
&\text{NPV Update:}		&\dot{v}_t(p)&=&\quad\sum_{\theta\in\Theta}\partial_{\theta}v_t(p),\forall p\in[P]\\
&\text{Error Trajectory:}	 &\dot{e}_t&=&\quad-(K^v_t+K^{\phi}_t+(\Psi^v_{t})^\top \Psi^{\phi}_{t}+(\Psi^v_{t})^\top\Psi^{\phi}_{t})e_t
\end{align}
\end{comment}
We first show that fixed NPFs can be trained and they also generalise \Cref{th:main}. We then

\begin{table}[h] 
\begin{minipage}{0.75\columnwidth}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|l|l|}\hline
Layer& Gating Network (NPF)&Value Network (NPV)\\\hline 
Input & $z_{x,\Tg_t}(0)=x$ &$z_{x,\Theta_t}(0)=x$ \\\hline
Activation & $q_{x,\Tg_t}(l)={\Tg_t(l)}^\top z_{x,\Tg_t}(l-1)$& $q_{x,\Theta_t}(l)={\Theta_t(l)}^\top z_{x,\Theta_t}(l-1)$\\\hline
Hidden &$z_{x,\Tg_t}(l)=q_{x,\Tg_t}(l)\odot G_{x,\Tg_t}(l)$& $z_{x,\Theta_t}(l)=q_{x,\Theta_t}(l)\odot G_{x,t}(l)$ \\\hline
Output & None &$\hat{y}_t(x)={\Theta_t(d)}^\top z_{x,\Theta_t}(d-1)$\\\hline
\multicolumn{3}{|l|}{Gating Values}\\\hline
\end{tabular}
}
\end{minipage}
\begin{minipage}{0.24\columnwidth}
\resizebox{\columnwidth}{!}{
\includegraphics[scale=0.5]{figs/nntwin-blck.png}
}
\end{minipage}
\caption{}
\label{tb:dgn}
\end{table}
\begin{comment}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|}\hline
		&Gate&Activation&$\dot{\chi}$\\\hline
Prior&$G_R(q)=\mathbbm{1}_{\{q>0\}}$& $\begin{aligned}\chi_{R}(q)&=q\cdot\mathbbm{1}_{\{q>0\}}\\ \chi_{SP}(q)&=\frac{1}{\beta}\log(1+\exp(\beta q))\end{aligned}$&$\begin{aligned}\dot{\chi_{R}}(q)&: \text{Heaviside step function} \\ \dot{\chi}_{SP}(q)&=\frac{1}{1+\exp(-\beta q)}\end{aligned}$ \\\hline
Ours &$G_{SR}(q)=\frac{1}{1+\exp(-\beta q)}$ &$\chi_{SR}(q)=q\cdot G_{SR}(q)$& $\dot{\chi}_{SR}(q)=\dot{\chi}_{SP}(q)+\underbrace{\dot{G}_{SR}(q)}_{\text{Gate Dynamics}}$\\\hline
\end{tabular}
}
\caption{A comparison of prior work and our paper. Here subscripts $R$, $SR$ and $SP$ stand for ReLU, soft-ReLU and softplus respectively.}
\label{tb:compare}
\end{table}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|}\hline
		&Output&NTF&$NTK$ &$\dot{\Sigma}^{(l+1)}(s,s')$\\\hline
Prior&\shortstack{$\hat{y}_{\Theta}(x)$\\layer-by-layer} &$\psi_{x,\Theta}=\left(\partial_{\theta}\hat{y}_{\Theta}(x),\theta\in\Theta\right)$& $K_{\Theta}(x,x')=\ip{\psi_{x,\Theta},\psi_{x',\Theta}}$ & $\dot{\chi}_{SP}(q)\dot{\chi}_{SP}(q')$ \\\hline
Ours& $\hat{y}_{\Theta}(x)=\ip{\phi_{x,\Theta},v_{\Theta}}$ & $\begin{aligned}\psi_{x,\Theta}&=\psi^v_{x,\Theta}+\psi^{\phi}_{x,\Theta}\\\psi^v_{x,\Theta}&=\left(\ip{\phi_{x,\Theta}, \partial_{\theta} v_{\Theta}},\theta\in\Theta\right) \\ \psi^{\phi}_{x,\Theta}&=\left(\ip{\partial_{\theta}\phi_{x,\Theta}, v_{\Theta}},\theta\in\Theta\right)\end{aligned}$ &$\begin{aligned} K_{\Theta}(x,x')=\langle \psi^v_{x,\Theta}+\psi^{\phi}_{x,\Theta},\\ \psi^v_{x',\Theta}+\psi^{\phi}_{x',\Theta}\rangle\end{aligned}$ & $\begin{aligned}\left(\dot{\chi}_{SP}(q)+\dot{G}_{SR}(q)\right)\cdot\\\left(\dot{\chi}_{SP}(q')+\dot{G}_{SR}(q')\right)\end{aligned}$ \\\hline

\end{tabular}
}
\caption{A comparison of prior work and our paper. Here subscripts $R$, $SR$ and $SP$ stand for ReLU, soft-ReLU and softplus respectively.}
\label{tb:compare}
\end{table}
\end{comment}
%$\begin{aligned}&\text{Learning}\\ &\text{Regime} \end{aligned}$ 