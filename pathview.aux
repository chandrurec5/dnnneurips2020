\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{dudnn,dudln}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\newlabel{sec:intro}{{1}{2}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][2][]2}}
\newlabel{eq:basictraj}{{1}{2}{Introduction}{equation.1.1}{}}
\newlabel{eq:basictraj@cref}{{[equation][1][]1}{[1][2][]2}}
\citation{sss}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces A deep gated network. Here $x_s\in \mathbb  {R}^{d_{in}},s\in [n]$ is the input, and $l\in [d-1]$ are the intermediate layers. $G_{x_s,t}(l)\in [0,1]^w$ and $q_{x,\Theta _t}(l)\in \mathbb  {R}^w$ are the gating and pre-activation input values respectively at time $t$.\relax }}{3}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tb:dgn}{{1}{3}{A deep gated network. Here $x_s\in \R ^{d_{in}},s\in [n]$ is the input, and $l\in [d-1]$ are the intermediate layers. $G_{x_s,t}(l)\in [0,1]^w$ and $q_{x,\Theta _t}(l)\in \R ^w$ are the gating and pre-activation input values respectively at time $t$.\relax }{table.caption.2}{}}
\newlabel{tb:dgn@cref}{{[table][1][]1}{[1][3][]3}}
\citation{ben}
\newlabel{eq:featstrength}{{2}{4}{Introduction}{equation.1.2}{}}
\newlabel{eq:featstrength@cref}{{[equation][2][]2}{[1][4][]4}}
\citation{lottery}
\citation{arora}
\@writefile{toc}{\contentsline {section}{\numberline {2}Deep Information Propagation in DGN}{6}{section.2}}
\newlabel{sec:optimisation}{{2}{6}{Deep Information Propagation in DGN}{section.2}{}}
\newlabel{sec:optimisation@cref}{{[section][2][]2}{[1][6][]6}}
\newlabel{assmp:main}{{1}{6}{}{assumption.1}{}}
\newlabel{assmp:main@cref}{{[assumption][1][]1}{[1][6][]6}}
\newlabel{th:opti}{{2.1}{6}{DIP}{theorem.2.1}{}}
\newlabel{th:opti@cref}{{[theorem][1][2]2.1}{[1][6][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Shows the plots for DGN-FRG with $\mu =\frac  {1}{2}$ and $\sigma =\sqrt  {\frac  {2}{w}}$. The first plot in the left shows the ideal cumulative eigenvalue (e.c.d.f) for various depths $d=2,4,6,8,12,16,20$. Note that the ideal plot converges to identity matrix as $d$ increases. The second plot from the left shows the cumulative eigenvalues (e.c.d.f) for $w=500$. \relax }}{7}{figure.caption.3}}
\newlabel{fig:dgn-frg-gram-ecdf}{{1}{7}{Shows the plots for DGN-FRG with $\mu =\frac {1}{2}$ and $\sigma =\sqrt {\frac {2}{w}}$. The first plot in the left shows the ideal cumulative eigenvalue (e.c.d.f) for various depths $d=2,4,6,8,12,16,20$. Note that the ideal plot converges to identity matrix as $d$ increases. The second plot from the left shows the cumulative eigenvalues (e.c.d.f) for $w=500$. \relax }{figure.caption.3}{}}
\newlabel{fig:dgn-frg-gram-ecdf@cref}{{[figure][1][]1}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Generalisation}{7}{section.3}}
\newlabel{sec:generalisation}{{3}{7}{Generalisation}{section.3}{}}
\newlabel{sec:generalisation@cref}{{[section][3][]3}{[1][7][]7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces First two plots from the left show optimisation and generalisation in ReLU and GaLU networks for standard MNIST. The right most plot shows $\nu _t=y^\top (\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle M$}\mathaccent "0362{M}_t)^{-1}y$, where $M_t=\Phi _t^\top \Phi _t$.\relax }}{8}{figure.caption.4}}
\newlabel{fig:galu-relu}{{2}{8}{First two plots from the left show optimisation and generalisation in ReLU and GaLU networks for standard MNIST. The right most plot shows $\nu _t=y^\top (\widehat {M}_t)^{-1}y$, where $M_t=\Phi _t^\top \Phi _t$.\relax }{figure.caption.4}{}}
\newlabel{fig:galu-relu@cref}{{[figure][2][]2}{[1][7][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Feature Learning: Preliminary analysis }{8}{subsection.3.1}}
\newlabel{eq:sensitivity}{{3}{8}{Feature Learning: Preliminary analysis}{equation.3.3}{}}
\newlabel{eq:sensitivity@cref}{{[equation][3][]3}{[1][8][]8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Shows a circular convolutional network with $d_{in}=3$ and kernel size $\mathaccentV {hat}05E{w}=2$. Note that there are only $8$ unique path strengths in this example (in the case of global average pooling).\relax }}{9}{figure.caption.5}}
\newlabel{fig:circconv}{{3}{9}{Shows a circular convolutional network with $d_{in}=3$ and kernel size $\hat {w}=2$. Note that there are only $8$ unique path strengths in this example (in the case of global average pooling).\relax }{figure.caption.5}{}}
\newlabel{fig:circconv@cref}{{[figure][3][]3}{[1][9][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Understanding the role of convolutions and pooling operations}{9}{section.4}}
\newlabel{sec:conv}{{4}{9}{Understanding the role of convolutions and pooling operations}{section.4}{}}
\newlabel{sec:conv@cref}{{[section][4][]4}{[1][9][]9}}
\citation{dudnn}
\citation{dudnn}
\citation{dudnn}
\citation{sss}
\citation{arora}
\citation{lottery}
\newlabel{lm:invariance}{{4.1}{10}{}{theorem.4.1}{}}
\newlabel{lm:invariance@cref}{{[theorem][1][4]4.1}{[1][10][]10}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Related Work}{10}{section.5}}
\bibstyle{plainnat}
\bibdata{refs}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion }{11}{section.6}}
