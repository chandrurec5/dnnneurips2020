\section{Introduction}
Understanding optimisation and generalisation of deep neural networks (DNNs) trained using first-order method such as (stochastic) gradient descent is an important problem in machine learning. In this paper, we throw light on the following two questions:\hfill\\
\textbf{Question I (Optimisation):} \emph{What is the role of width and depth in training of DNNs? Why increasing depth till a point helps in training? Why increasing the depth beyond hurts training?}\\
\textbf{Question II (Generalisation):} \emph{What are the hidden features in a DNN? Are these features learned? and if so, how?} \hfill\\
\textbf {Background:} We consider a fully connected DNN of depth $d$, and width $w$, weights $\Theta\in\R^{d_{net}}$, which, accepts an input $x\in \R^{d_{in}}$ and produces an output $\hat{y}_{\Theta_t}(x)\in \R$. Some of the recent works on optimisation and generalisation in DNNs have made use of the \emph{neural tangent features} (NTFs) and the \emph{trajectory} based analysis. The NTF for an input $x\in \R^{d_{in}}$ is defined as $\psi_{x,t}=(\frac{\partial \hat{y}_{\Theta}}{\partial \theta}|_{\Theta=\Theta_t},\theta \in \Theta)\in \R^{d_{net}}$, i.e., collection of the gradients of the network output with respect to its weights. Since the NTF is a first-order term, it can be used to linearise the DNN output about the point $\Theta_t$. Associated with the NTF is a \emph{neural tangent kernel} (NTK), which is given by $K_t(x,x')=\psi^\top_{x,t}\psi_{x',t}$. In the \emph{trajectory} based analysis, one looks at the dynamics of error $e_t$  (i.e., difference between predicted and true values) at time $t$. For a small step-size $\alpha_t>0$, the error dynamics follows a linear recursion given by: $e_{t+1}=e_t-\alpha_tK_te_t$, where $K_t$ is the NTK matrix obtained on the the dataset.\hfill\\
Central idea in this paper is the `path-view': to regard paths and gates as basic building blocks of a DNN. We now list the major highlights, and then present the results in brief.\hfill\\
$1.$ \textbf{Representation:} Using the `path-view', we are able to identity novel \emph{neural path feature} and \emph{neural path kernel}, which are \emph{zeroth-order} quantities. \hfill\\
$2.$ \textbf{Optimisation:}  At initialisation, the NPK is equal to the NTK (but for a constant scaling factor). The NPK has a special structure, that enables the whitening of the $K_0$ as depth increases.\hfill\\
$3.$ \textbf{Generalisation:} The NPF and NPK are learned during training and are key for generalisation.\hfill\\
\textbf{Paths:}  A \emph{path} starts from an input node, passes through exactly one weight and one activation in each layer, and finally ends at the output node. Let $[P]=\{1,\ldots,P\}$ be an enumeration of all the paths, the zeroth and first order quantities of a DNN can then be expressed as:
\begin{align}
\label{eq:zero}\text{(Zeroth Order)}\quad: \quad \hat{y}_t(x)&=\sum_{p\in [P]}x(\I(p))A_t(x,p)v_t(p)={\phi^\top_{x,t}} v_t\\
\text{(First Order)}\quad: \quad \partial \hat{y}_t(x)&= \sum_{p\in [P]}x(\I(p)) A_t(x,p) \partial((v_t(p))+\sum_{p\in [P]}x(\I(p)) \partial(A_t(x,p)) v_t(p)\nn\\
\label{eq:first}\psi_{x,t}&=\phi^\top_{x,t} {\partial} v_t + {\partial} \phi^\top_{x,t} v_t,
\end{align}
where, for a path $p$, $\I(p)$ is the input node at which it starts, $v_t(p)$ is its value given by the product of its weights and for input $x\in\R^{d_{in}}$, $A_t(x,p)$ is its activation level given by the product of the gating values in the path. In order to understand the role $A_t$, we handle the gating values as independent variables\footnote{In DNN with ReLU, the gates are implicit: a gate is \emph{on} only if pre-activation input is positive.}. \hfill\\
\textbf{Neural Path Feature and Kernel:} The \emph{neural path feature} (NPF) is given by $\phi_{x,t}=(x(\I(p))A_t(x,p))\in \R^P$ (see \eqref{eq:zero}) , and an associated \emph{neural path kernel} (NPK) to $H_t(x,x')=\phi^\top_{x,t}\phi_{x',t}$. The NPK has special structure: $H_t(x,x')=(x^\top x')\lambda_t(x,x')$, where $\lambda_t(x,x')$ is a measure of similarity based on the path activation levels for inputs $x,x'\in\R^{d_{in}}$. \hfill\\
\textbf{NPK vs NTK: }When the weights initialisation is random, zero mean, and independent of $A_0$, we show that $\E{K_0} = C H_0$, ($C>0$ is a constant), where $K_0$ is the NTK matrix at $t=0$. \hfill\\
\textbf{Optimisation I:} We argue that $\frac{\lambda_0(x,x')}{\lambda_0(x,x)}$ decays at an exponential rate with depth, i.e., the Gram matrix $K_0$ whitens with depth. Thus increasing depth helps in training. \hfill\\
\textbf{Optimisation II:} We show that $Var\left[K_0(x,x')\right]\leq O(\max\{\frac{d^2}{w}, \frac{d^3}{w^2}\})$ (for $\sigma^2=O(\frac{1}w)$). For a fixed $d$, increasing $w$ ensures $K_0$ converges to $\E{K_0}$. However, for a fixed $w$, increasing $d$ makes the entries of $K_0$ deviate from $\E{K_0}$, thus degrading the spectrum of $K_0$. Thus increasing depth beyond a point hurts training performance. \hfill\\
\textbf{Generalisation:} We experiment with two gating regimes namely the static, wherein, $A_t=A_0,\forall t \geq 0$, and the dynamic, wherein $A_t$ changes during training. The experiments on MNIST and CIFAR show that, better generalisation happens when the activations $A_t$ change with time. In particular, for binary classification, we verify that the quantity $y^\top (\widehat{H_t})^{-1}y$\footnote{For a matrix $H$, $\hat{H}=\frac{1}{trace(H)}H$ .} reduces as the training progresses. This shows that the eigenspaces of the NPK matrix align themselves in accordance to the labelling function. These experiments supports the fact that NPF and the NPK are indeed learnt during training in a manner to improve the generalisation performance. \hfill\\
\textbf{Feature Learning:} To the best of our knowledge, for the first time in the literature, we note (see \eqref{eq:first}) that the gradient flow has two components. The $\phi^\top_{x_s,t} {\partial} v_t $ term, which we call the \emph{value gradient}, learns the path values (which are akin to weight vector in a standard linear approximation) keeping the NPFs fixed. The $ {\partial} \phi^\top_{x_s,t} v_t$ term, which we call the \emph{feature gradient} learns the NPFs keeping the path values fixed. We present preliminary theory connecting the trajectory method and feature learning. \hfill\\
\textbf{Related Work and Gaps:} \hfill\\
\emph{Optimisation:} \cite{ntk} were the first to point out the role of NTK in DNNs. Using the trajectory based analysis, \cite{dudnn} show that in fully connected DNNs with $w=\Omega(poly(n)2^{O(d)})$, and in residual neural networks (ResNets) with $w=\Omega(poly(n,d))$ gradient descent converges to zero training loss.  However, Question I, i.e., `why depth helps in training?' is unresolved.\hfill\\%This shows that ResNets are better than fully connected DNNs, based on the fact that the dependence on the number of layers improves exponentially for ResNets. 
\emph{Generalisation:} \cite{arora2019exact,arora,cao2019generalization} use NTK to provide generalisation bounds as well as propose pure-kernel methods. However, couple of issues remain unresolved: firstly, if the DNNs are only linear learners with random NTFs, then it suggests that no feature learning happens in DNNs, and secondly, it was observed in prior experiments that the DNNs perform better than their corresponding NTK counterparts \cite{arora2019exact,lee2017deep}. \hfill\\
